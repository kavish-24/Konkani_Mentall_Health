{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavish-24/Konkani_Mentall_Health/blob/main/DataPreparationPrudentMedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrCp5PbZ-SYz",
        "outputId": "1050b147-0a79-43cc-c767-6d1ae4110205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting odfpy\n",
            "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/717.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/717.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from odfpy) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: odfpy\n",
            "  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for odfpy: filename=odfpy-1.4.1-py2.py3-none-any.whl size=160673 sha256=b7cfddab34573cc66ae5e2184355776e8038788af4edb902ddc1314d9388d1fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/5d/63/8243a7ee78fff0f944d638fd0e66d7278888f5e2285d7346b6\n",
            "Successfully built odfpy\n",
            "Installing collected packages: odfpy, evaluate\n",
            "Successfully installed evaluate-0.4.6 odfpy-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate odfpy pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from odf import text, teletype\n",
        "from odf.opendocument import OpenDocumentText\n",
        "from odf.style import Style, TextProperties\n",
        "from odf.text import P\n",
        "import os\n",
        "import re\n",
        "\n",
        "def extract_text_from_odt(odt_path, skip_bold=True, debug=False):\n",
        "    \"\"\"\n",
        "    Extract text from ODT file, skipping all bold text (including letters, spaces, and punctuation)\n",
        "    and removing all noise (punctuation, extra spaces) from non-bold text.\n",
        "\n",
        "    Args:\n",
        "        odt_path: Path to input ODT file\n",
        "        skip_bold: If True, skip all bold text\n",
        "        debug: If True, print debug information\n",
        "\n",
        "    Returns:\n",
        "        List of cleaned non-bold text paragraphs\n",
        "    \"\"\"\n",
        "    from odf.opendocument import load\n",
        "\n",
        "    if not os.path.exists(odt_path):\n",
        "        raise FileNotFoundError(f\"ODT file not found: {odt_path}\")\n",
        "\n",
        "    try:\n",
        "        doc = load(odt_path)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load ODT file: {e}\")\n",
        "\n",
        "    bold_styles = set()\n",
        "\n",
        "    # Check automatic styles for bold\n",
        "    for style in doc.automaticstyles.getElementsByType(Style):\n",
        "        style_name = style.getAttribute('name')\n",
        "        for prop in style.getElementsByType(TextProperties):\n",
        "            font_weight = prop.getAttribute('fontweight')\n",
        "            if font_weight and 'bold' in str(font_weight).lower():\n",
        "                bold_styles.add(style_name)\n",
        "                if debug:\n",
        "                    print(f\"   Found bold style: {style_name}\")\n",
        "\n",
        "    # Check named styles for bold\n",
        "    for style in doc.styles.getElementsByType(Style):\n",
        "        style_name = style.getAttribute('name')\n",
        "        for prop in style.getElementsByType(TextProperties):\n",
        "            font_weight = prop.getAttribute('fontweight')\n",
        "            if font_weight and 'bold' in str(font_weight).lower():\n",
        "                bold_styles.add(style_name)\n",
        "                if debug:\n",
        "                    print(f\"   Found bold style: {style_name}\")\n",
        "\n",
        "    print(f\"\\n🔍 Detected {len(bold_styles)} bold styles: {bold_styles}\")\n",
        "\n",
        "    extracted_parts = []\n",
        "    skipped_count = 0\n",
        "    kept_count = 0\n",
        "    skipped_text_samples = []\n",
        "    kept_text_samples = []\n",
        "\n",
        "    def get_style_name(node):\n",
        "        \"\"\"Get style name from a node using multiple methods\"\"\"\n",
        "        # Try different attribute names\n",
        "        for attr_name in ['stylename', 'style-name']:\n",
        "            try:\n",
        "                style = node.getAttribute(attr_name)\n",
        "                if style:\n",
        "                    return style\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Try namespace-aware retrieval\n",
        "        try:\n",
        "            style = node.getAttrNS(\n",
        "                \"urn:oasis:names:tc:opendocument:xmlns:text:1.0\",\n",
        "                \"style-name\"\n",
        "            )\n",
        "            if style:\n",
        "                return style\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def is_node_bold(node):\n",
        "        \"\"\"Check if a node has bold styling\"\"\"\n",
        "        style_name = get_style_name(node)\n",
        "        if style_name and style_name in bold_styles:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def process_node(node, parent_is_bold=False):\n",
        "        \"\"\"Recursively process a node and its children\"\"\"\n",
        "        nonlocal skipped_count, kept_count, skipped_text_samples, kept_text_samples\n",
        "\n",
        "        result_text = \"\"\n",
        "\n",
        "        # Check if current node is bold\n",
        "        current_is_bold = parent_is_bold or is_node_bold(node)\n",
        "\n",
        "        if node.nodeType == node.TEXT_NODE:\n",
        "            node_text = node.data\n",
        "            if node_text.strip():\n",
        "                if skip_bold and current_is_bold:\n",
        "                    skipped_count += 1\n",
        "                    if len(skipped_text_samples) < 5:\n",
        "                        skipped_text_samples.append(node_text.strip()[:50])\n",
        "                    if debug:\n",
        "                        print(f\"   [SKIP] TEXT_NODE (bold): {repr(node_text.strip()[:50])}\")\n",
        "                else:\n",
        "                    result_text += node_text\n",
        "                    kept_count += 1\n",
        "                    if len(kept_text_samples) < 5:\n",
        "                        kept_text_samples.append(node_text.strip()[:50])\n",
        "                    if debug:\n",
        "                        print(f\"   [KEEP] TEXT_NODE: {repr(node_text.strip()[:50])}\")\n",
        "\n",
        "        elif node.nodeType == node.ELEMENT_NODE:\n",
        "            # For span elements, check if they're bold\n",
        "            if node.tagName == \"text:span\":\n",
        "                span_is_bold = current_is_bold or is_node_bold(node)\n",
        "\n",
        "                # Get all text from this span (including nested elements)\n",
        "                span_full_text = teletype.extractText(node)\n",
        "\n",
        "                if skip_bold and span_is_bold:\n",
        "                    skipped_count += 1\n",
        "                    if span_full_text.strip() and len(skipped_text_samples) < 5:\n",
        "                        skipped_text_samples.append(span_full_text.strip()[:50])\n",
        "                    if debug:\n",
        "                        style_name = get_style_name(node)\n",
        "                        print(f\"   [SKIP] SPAN (bold - {style_name}): {repr(span_full_text[:50])}\")\n",
        "                    # Don't process children if parent span is bold\n",
        "                    return \"\"\n",
        "                else:\n",
        "                    if debug and span_full_text.strip():\n",
        "                        style_name = get_style_name(node)\n",
        "                        print(f\"   [KEEP] SPAN ({style_name}): {repr(span_full_text[:50])}\")\n",
        "                    # Process children with current bold status\n",
        "                    for child in node.childNodes:\n",
        "                        result_text += process_node(child, span_is_bold)\n",
        "            else:\n",
        "                # For other elements, process children\n",
        "                for child in node.childNodes:\n",
        "                    result_text += process_node(child, current_is_bold)\n",
        "\n",
        "        return result_text\n",
        "\n",
        "    for paragraph in doc.getElementsByType(text.P):\n",
        "        # Check if paragraph itself is bold\n",
        "        para_is_bold = is_node_bold(paragraph)\n",
        "\n",
        "        if debug and para_is_bold:\n",
        "            print(f\"\\n⚠️  Paragraph itself is BOLD - will skip all content\")\n",
        "\n",
        "        para_text = \"\"\n",
        "        for node in paragraph.childNodes:\n",
        "            para_text += process_node(node, para_is_bold)\n",
        "\n",
        "        if para_text.strip():\n",
        "            # Clean text: remove all punctuation and normalize spaces\n",
        "            para_text = re.sub(r'\\.{2,}', ' ', para_text)  # Replace 2+ dots with single space\n",
        "            para_text = re.sub(r'[!?,;:\"\\'()\\[\\]{}\\-—*]+', '', para_text)  # Remove other punctuation\n",
        "\n",
        "            para_text = re.sub(r'\\s+', ' ', para_text)  # Normalize spaces\n",
        "            para_text = para_text.strip()\n",
        "            if para_text:\n",
        "                extracted_parts.append(para_text)\n",
        "\n",
        "    print(f\"\\n📊 Extraction summary:\")\n",
        "    print(f\"   Kept: {kept_count} elements\")\n",
        "    print(f\"   Skipped (bold): {skipped_count} elements\")\n",
        "    print(f\"   Extracted paragraphs: {len(extracted_parts)}\")\n",
        "\n",
        "    if skipped_count > 0:\n",
        "        print(f\"\\n❌ Sample SKIPPED bold text:\")\n",
        "        for sample in skipped_text_samples:\n",
        "            print(f\"   • {repr(sample)}...\")\n",
        "\n",
        "    if kept_count > 0:\n",
        "        print(f\"\\n✅ Sample KEPT non-bold text:\")\n",
        "        for sample in kept_text_samples:\n",
        "            print(f\"   • {repr(sample)}...\")\n",
        "\n",
        "    if extracted_parts:\n",
        "        print(f\"\\n🧼 Sample CLEANED paragraphs:\")\n",
        "        for i, para in enumerate(extracted_parts[:3]):\n",
        "            print(f\"   • [{i}] {repr(para)[:100]}...\")\n",
        "\n",
        "    if skipped_count == 0 and skip_bold:\n",
        "        print(f\"\\n⚠️  WARNING: No bold text was found to skip! Check ODT styles.\")\n",
        "\n",
        "    return extracted_parts\n",
        "\n",
        "def create_new_odt(output_path, paragraphs):\n",
        "    \"\"\"\n",
        "    Create a new ODT file with the given paragraphs.\n",
        "\n",
        "    Args:\n",
        "        output_path: Path to save the new ODT file\n",
        "        paragraphs: List of text paragraphs to include\n",
        "    \"\"\"\n",
        "    doc = OpenDocumentText()\n",
        "\n",
        "    for para_text in paragraphs:\n",
        "        p = P(text=para_text)\n",
        "        doc.text.addElement(p)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        doc.save(output_path)\n",
        "        print(f\"\\n💾 New ODT file created: {output_path}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to save new ODT file: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    INPUT_ODT_PATH = os.getenv(\"TRANSCRIPT_FILE_PATH\", \"/content/drive/MyDrive/Anju Project (1)/Audio Prudent media (1)/August 2017 (1)/dataset/10  AUG PRIME.odt\")\n",
        "    OUTPUT_ODT_PATH = os.getenv(\"OUTPUT_ODT_PATH\", \"/content/drive/MyDrive/Anju Project (1)/Audio Prudent media (1)/August 2017 (1)/dataset/10 AUG PRIME_non_bold.odt\")\n",
        "    DEBUG = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"EXTRACTING NON-BOLD TEXT AND REMOVING ALL NOISE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        print(\"\\n📄 Extracting text from ODT file...\")\n",
        "        paragraphs = extract_text_from_odt(INPUT_ODT_PATH, skip_bold=True, debug=DEBUG)\n",
        "\n",
        "        print(f\"\\n✓ Extracted {len(paragraphs)} paragraphs\")\n",
        "        if paragraphs:\n",
        "            print(f\"   Sample cleaned paragraph: {repr(paragraphs[0])[:150]}...\")\n",
        "\n",
        "        print(\"\\n📝 Creating new ODT file...\")\n",
        "        create_new_odt(OUTPUT_ODT_PATH, paragraphs)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"✅ PROCESS COMPLETE!\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Process failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CaN9XmAZa5_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simplified Audio Segmentation Using Whisper-Only Approach\n",
        "=========================================================\n",
        "\n",
        "This script uses Whisper's built-in capabilities for accurate audio-text alignment\n",
        "without complex manual matching. Perfect for Google Colab.\n",
        "\n",
        "Approach:\n",
        "1. Use Silero VAD to detect speech segments\n",
        "2. Let Whisper transcribe each segment with word timestamps\n",
        "3. Validate and create training manifest\n",
        "\n",
        "No complex alignment needed - Whisper handles it!\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALLATION (Run this first in Colab)\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "!pip install -q faster-whisper\n",
        "!pip install -q torch torchaudio\n",
        "!pip install -q librosa soundfile\n",
        "!pip install -q odfpy\n",
        "!pip install -q tqdm\n",
        "!apt-get install -y ffmpeg\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For ODT reading\n",
        "from odf.opendocument import load\n",
        "from odf import text, teletype\n",
        "\n",
        "# Whisper\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"Configuration for audio segmentation.\"\"\"\n",
        "\n",
        "    # Paths (modify these for your Colab setup)\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/dataset/whisper_segments\"\n",
        "\n",
        "    # Whisper settings\n",
        "    WHISPER_MODEL = \"small\"  # Options: tiny, base, small, medium, large\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    COMPUTE_TYPE = \"float16\" if torch.cuda.is_available() else \"int8\"\n",
        "\n",
        "    # VAD settings\n",
        "    VAD_THRESHOLD = 0.5\n",
        "    MIN_SPEECH_DURATION = 0.5  # seconds\n",
        "    MIN_SILENCE_DURATION = 0.3  # seconds\n",
        "\n",
        "    # Segment settings\n",
        "    MIN_SEGMENT_DURATION = 1.0\n",
        "    MAX_SEGMENT_DURATION = 30.0\n",
        "    TARGET_SEGMENT_DURATION = 10.0  # Ideal segment length\n",
        "\n",
        "    # Language\n",
        "    LANGUAGE = \"mr\"  # Marathi (closest to Konkani in Whisper)\n",
        "\n",
        "    # Quality thresholds\n",
        "    MIN_CONFIDENCE = 0.3  # Minimum word probability\n",
        "    MIN_WORDS_PER_SEGMENT = 3\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SILERO VAD (Voice Activity Detection)\n",
        "# ============================================================================\n",
        "class SileroVAD:\n",
        "    \"\"\"Silero VAD for detecting speech segments.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Loading Silero VAD model...\")\n",
        "        try:\n",
        "            self.model, utils = torch.hub.load(\n",
        "                repo_or_dir='snakers4/silero-vad',\n",
        "                model='silero_vad',\n",
        "                force_reload=False,\n",
        "                onnx=False\n",
        "            )\n",
        "            self.get_speech_timestamps = utils[0]\n",
        "            print(\"✓ Silero VAD loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Could not load Silero VAD: {e}\")\n",
        "            print(\"Falling back to energy-based VAD\")\n",
        "            self.model = None\n",
        "\n",
        "    def detect_speech(self, audio_path, threshold=0.5, min_speech_ms=250, min_silence_ms=100):\n",
        "        \"\"\"Detect speech segments in audio file.\"\"\"\n",
        "\n",
        "        # Load audio at 16kHz (required by Silero)\n",
        "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "        if self.model is not None:\n",
        "            # Use Silero VAD\n",
        "            audio_tensor = torch.from_numpy(audio)\n",
        "\n",
        "            speech_timestamps = self.get_speech_timestamps(\n",
        "                audio_tensor,\n",
        "                self.model,\n",
        "                threshold=threshold,\n",
        "                min_speech_duration_ms=min_speech_ms,\n",
        "                min_silence_duration_ms=min_silence_ms,\n",
        "                sampling_rate=16000\n",
        "            )\n",
        "\n",
        "            segments = [\n",
        "                {\n",
        "                    'start': ts['start'] / 16000,\n",
        "                    'end': ts['end'] / 16000\n",
        "                }\n",
        "                for ts in speech_timestamps\n",
        "            ]\n",
        "        else:\n",
        "            # Fallback: Energy-based VAD\n",
        "            segments = self._energy_based_vad(audio, sr)\n",
        "\n",
        "        return segments\n",
        "\n",
        "    def _energy_based_vad(self, audio, sr, frame_length=2048, hop_length=512):\n",
        "        \"\"\"Fallback energy-based VAD.\"\"\"\n",
        "        # Calculate RMS energy\n",
        "        rms = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Threshold\n",
        "        threshold = np.mean(rms) * 1.5\n",
        "\n",
        "        # Find speech frames\n",
        "        speech_frames = rms > threshold\n",
        "\n",
        "        # Convert to time segments\n",
        "        times = librosa.frames_to_time(np.arange(len(speech_frames)), sr=sr, hop_length=hop_length)\n",
        "\n",
        "        segments = []\n",
        "        in_speech = False\n",
        "        start_time = 0\n",
        "\n",
        "        for i, is_speech in enumerate(speech_frames):\n",
        "            if is_speech and not in_speech:\n",
        "                start_time = times[i]\n",
        "                in_speech = True\n",
        "            elif not is_speech and in_speech:\n",
        "                segments.append({'start': start_time, 'end': times[i]})\n",
        "                in_speech = False\n",
        "\n",
        "        if in_speech:\n",
        "            segments.append({'start': start_time, 'end': times[-1]})\n",
        "\n",
        "        return segments\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# WHISPER ALIGNER\n",
        "# ============================================================================\n",
        "class WhisperAligner:\n",
        "    \"\"\"Simplified aligner using only Whisper.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config = None):\n",
        "        self.config = config or Config()\n",
        "\n",
        "        # Create output directories\n",
        "        self.output_dir = Path(self.config.OUTPUT_DIR)\n",
        "        self.audio_dir = self.output_dir / \"audio\"\n",
        "        self.text_dir = self.output_dir / \"text\"\n",
        "        self.metadata_dir = self.output_dir / \"metadata\"\n",
        "\n",
        "        for dir_path in [self.audio_dir, self.text_dir, self.metadata_dir]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Initialize VAD\n",
        "        self.vad = SileroVAD()\n",
        "\n",
        "        # Initialize Whisper\n",
        "        print(f\"Loading Whisper model: {self.config.WHISPER_MODEL}...\")\n",
        "        self.whisper = WhisperModel(\n",
        "            self.config.WHISPER_MODEL,\n",
        "            device=self.config.DEVICE,\n",
        "            compute_type=self.config.COMPUTE_TYPE\n",
        "        )\n",
        "        print(f\"✓ Whisper loaded on {self.config.DEVICE}\")\n",
        "\n",
        "    def load_transcript_from_odt(self, odt_path: str) -> str:\n",
        "        \"\"\"Load transcript from ODT file.\"\"\"\n",
        "        print(f\"Loading transcript: {Path(odt_path).name}\")\n",
        "\n",
        "        doc = load(str(odt_path))\n",
        "        paragraphs = []\n",
        "\n",
        "        for paragraph in doc.getElementsByType(text.P):\n",
        "            para_text = teletype.extractText(paragraph)\n",
        "            if para_text.strip():\n",
        "                paragraphs.append(para_text.strip())\n",
        "\n",
        "        full_text = \" \".join(paragraphs)\n",
        "        print(f\"✓ Loaded {len(paragraphs)} paragraphs, {len(full_text)} characters\")\n",
        "        return full_text\n",
        "\n",
        "    def process_audio_file(\n",
        "        self,\n",
        "        audio_path: str,\n",
        "        odt_path: str = None,\n",
        "        session_id: str = None\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process audio file using Whisper-only approach.\n",
        "\n",
        "        Args:\n",
        "            audio_path: Path to audio file\n",
        "            odt_path: Optional path to reference transcript (for validation only)\n",
        "            session_id: Session identifier\n",
        "\n",
        "        Returns:\n",
        "            List of segment metadata\n",
        "        \"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        if session_id is None:\n",
        "            session_id = audio_path.stem\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PROCESSING: {audio_path.name}\")\n",
        "        print(f\"Session: {session_id}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Load reference transcript if provided\n",
        "        reference_transcript = None\n",
        "        if odt_path:\n",
        "            reference_transcript = self.load_transcript_from_odt(odt_path)\n",
        "\n",
        "        # Step 1: Detect speech segments with VAD\n",
        "        print(\"Step 1: Detecting speech segments...\")\n",
        "        vad_segments = self.vad.detect_speech(\n",
        "            str(audio_path),\n",
        "            threshold=self.config.VAD_THRESHOLD,\n",
        "            min_speech_ms=int(self.config.MIN_SPEECH_DURATION * 1000),\n",
        "            min_silence_ms=int(self.config.MIN_SILENCE_DURATION * 1000)\n",
        "        )\n",
        "        print(f\"✓ Found {len(vad_segments)} speech segments\")\n",
        "\n",
        "        # Step 2: Merge short segments\n",
        "        merged_segments = self._merge_short_segments(vad_segments)\n",
        "        print(f\"✓ Merged to {len(merged_segments)} segments\")\n",
        "\n",
        "        # Step 3: Transcribe each segment with Whisper\n",
        "        print(\"\\nStep 2: Transcribing with Whisper...\")\n",
        "        all_segments = []\n",
        "\n",
        "        for idx, vad_seg in enumerate(tqdm(merged_segments, desc=\"Transcribing\")):\n",
        "            # Extract audio segment\n",
        "            segment_audio, sr = librosa.load(\n",
        "                str(audio_path),\n",
        "                sr=16000,\n",
        "                offset=vad_seg['start'],\n",
        "                duration=vad_seg['end'] - vad_seg['start']\n",
        "            )\n",
        "\n",
        "            # Save temporary audio file for Whisper\n",
        "            temp_audio = self.output_dir / f\"temp_{idx}.wav\"\n",
        "            sf.write(temp_audio, segment_audio, sr)\n",
        "\n",
        "            try:\n",
        "                # Transcribe with Whisper\n",
        "                segments, info = self.whisper.transcribe(\n",
        "                    str(temp_audio),\n",
        "                    language=self.config.LANGUAGE,\n",
        "                    word_timestamps=True,\n",
        "                    beam_size=5,\n",
        "                    best_of=5,\n",
        "                    temperature=0.0,\n",
        "                    vad_filter=False  # We already did VAD\n",
        "                )\n",
        "\n",
        "                # Process segments\n",
        "                for seg in segments:\n",
        "                    if not seg.text.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Adjust timestamps to original audio\n",
        "                    adjusted_start = vad_seg['start'] + seg.start\n",
        "                    adjusted_end = vad_seg['start'] + seg.end\n",
        "\n",
        "                    # Extract word-level info\n",
        "                    words = []\n",
        "                    if hasattr(seg, 'words') and seg.words:\n",
        "                        words = [\n",
        "                            {\n",
        "                                'word': w.word.strip(),\n",
        "                                'start': vad_seg['start'] + w.start,\n",
        "                                'end': vad_seg['start'] + w.end,\n",
        "                                'probability': w.probability\n",
        "                            }\n",
        "                            for w in seg.words\n",
        "                        ]\n",
        "\n",
        "                    all_segments.append({\n",
        "                        'start': adjusted_start,\n",
        "                        'end': adjusted_end,\n",
        "                        'duration': adjusted_end - adjusted_start,\n",
        "                        'text': seg.text.strip(),\n",
        "                        'words': words,\n",
        "                        'avg_logprob': seg.avg_logprob if hasattr(seg, 'avg_logprob') else 0.0\n",
        "                    })\n",
        "\n",
        "            finally:\n",
        "                # Clean up temp file\n",
        "                if temp_audio.exists():\n",
        "                    temp_audio.unlink()\n",
        "\n",
        "        print(f\"\\n✓ Transcribed {len(all_segments)} segments\")\n",
        "\n",
        "        # Step 3: Validate and filter segments\n",
        "        print(\"\\nStep 3: Validating segments...\")\n",
        "        valid_segments = self._validate_segments(all_segments)\n",
        "        print(f\"✓ {len(valid_segments)}/{len(all_segments)} segments passed validation\")\n",
        "\n",
        "        # Step 4: Save segments\n",
        "        print(\"\\nStep 4: Saving segments...\")\n",
        "        saved_segments = self._save_segments(\n",
        "            valid_segments,\n",
        "            audio_path,\n",
        "            session_id,\n",
        "            reference_transcript\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"✓ COMPLETE! Created {len(saved_segments)} segments\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return saved_segments\n",
        "\n",
        "    def _merge_short_segments(self, segments: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Merge segments that are too short.\"\"\"\n",
        "        if not segments:\n",
        "            return []\n",
        "\n",
        "        merged = []\n",
        "        current = segments[0].copy()\n",
        "\n",
        "        for next_seg in segments[1:]:\n",
        "            current_duration = current['end'] - current['start']\n",
        "            gap = next_seg['start'] - current['end']\n",
        "\n",
        "            # Merge if current is too short and gap is small\n",
        "            if current_duration < self.config.TARGET_SEGMENT_DURATION and gap < 1.0:\n",
        "                current['end'] = next_seg['end']\n",
        "            else:\n",
        "                merged.append(current)\n",
        "                current = next_seg.copy()\n",
        "\n",
        "        merged.append(current)\n",
        "        return merged\n",
        "\n",
        "    def _validate_segments(self, segments: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Validate and filter segments based on quality criteria.\"\"\"\n",
        "        valid = []\n",
        "\n",
        "        for seg in segments:\n",
        "            # Check duration\n",
        "            if seg['duration'] < self.config.MIN_SEGMENT_DURATION:\n",
        "                continue\n",
        "            if seg['duration'] > self.config.MAX_SEGMENT_DURATION:\n",
        "                continue\n",
        "\n",
        "            # Check text quality\n",
        "            text = seg['text'].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            words = text.split()\n",
        "            if len(words) < self.config.MIN_WORDS_PER_SEGMENT:\n",
        "                continue\n",
        "\n",
        "            # Check word-level confidence\n",
        "            if seg.get('words'):\n",
        "                avg_prob = np.mean([w['probability'] for w in seg['words']])\n",
        "                if avg_prob < self.config.MIN_CONFIDENCE:\n",
        "                    continue\n",
        "\n",
        "            valid.append(seg)\n",
        "\n",
        "        return valid\n",
        "\n",
        "    def _save_segments(\n",
        "        self,\n",
        "        segments: List[Dict],\n",
        "        audio_path: Path,\n",
        "        session_id: str,\n",
        "        reference_transcript: str = None\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"Save segments to disk and create manifest.\"\"\"\n",
        "        saved_segments = []\n",
        "\n",
        "        # Load full audio once\n",
        "        print(\"Loading audio for extraction...\")\n",
        "        audio, sr = librosa.load(str(audio_path), sr=16000, mono=True)\n",
        "\n",
        "        for idx, seg in enumerate(tqdm(segments, desc=\"Saving segments\")):\n",
        "            segment_id = f\"{session_id}_{idx:04d}\"\n",
        "\n",
        "            # Paths\n",
        "            audio_file = self.audio_dir / f\"{segment_id}.wav\"\n",
        "            text_file = self.text_dir / f\"{segment_id}.txt\"\n",
        "\n",
        "            try:\n",
        "                # Extract audio segment\n",
        "                start_sample = int(seg['start'] * sr)\n",
        "                end_sample = int(seg['end'] * sr)\n",
        "                segment_audio = audio[start_sample:end_sample]\n",
        "\n",
        "                # Save audio\n",
        "                sf.write(audio_file, segment_audio, sr)\n",
        "\n",
        "                # Save text\n",
        "                clean_text = self._clean_text(seg['text'])\n",
        "                with open(text_file, 'w', encoding='utf-8') as f:\n",
        "                    f.write(clean_text)\n",
        "\n",
        "                # Create metadata entry\n",
        "                metadata = {\n",
        "                    'segment_id': segment_id,\n",
        "                    'audio_filepath': f\"audio/{segment_id}.wav\",\n",
        "                    'text_filepath': f\"text/{segment_id}.txt\",\n",
        "                    'text': clean_text,\n",
        "                    'start_time': float(seg['start']),\n",
        "                    'end_time': float(seg['end']),\n",
        "                    'duration': float(seg['duration']),\n",
        "                    'word_count': len(clean_text.split()),\n",
        "                    'language': self.config.LANGUAGE,\n",
        "                    'avg_confidence': float(np.mean([w['probability'] for w in seg.get('words', [])])) if seg.get('words') else 0.0\n",
        "                }\n",
        "\n",
        "                saved_segments.append(metadata)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n⚠ Error saving segment {segment_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save manifest\n",
        "        manifest = {\n",
        "            'session_id': session_id,\n",
        "            'audio_file': str(audio_path.name),\n",
        "            'total_segments': len(saved_segments),\n",
        "            'total_duration': sum(s['duration'] for s in saved_segments),\n",
        "            'language': self.config.LANGUAGE,\n",
        "            'segments': saved_segments\n",
        "        }\n",
        "\n",
        "        manifest_file = self.metadata_dir / f\"{session_id}_manifest.json\"\n",
        "        with open(manifest_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Also save JSONL format for Whisper fine-tuning\n",
        "        jsonl_file = self.metadata_dir / f\"{session_id}_train.jsonl\"\n",
        "        with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
        "            for seg in saved_segments:\n",
        "                entry = {\n",
        "                    'audio': seg['audio_filepath'],\n",
        "                    'text': seg['text'],\n",
        "                    'duration': seg['duration']\n",
        "                }\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"\\n✓ Saved manifest to: {manifest_file}\")\n",
        "        print(f\"✓ Saved JSONL to: {jsonl_file}\")\n",
        "\n",
        "        return saved_segments\n",
        "\n",
        "    @staticmethod\n",
        "    def _clean_text(text: str) -> str:\n",
        "        \"\"\"Clean and normalize text.\"\"\"\n",
        "        # Normalize Unicode\n",
        "        text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Strip\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    config = Config()\n",
        "\n",
        "    # Initialize aligner\n",
        "    aligner = WhisperAligner(config)\n",
        "\n",
        "    # File list - MODIFY THESE PATHS FOR YOUR SETUP\n",
        "    files_to_process = [\n",
        "        {\n",
        "            'audio_path': '/content/drive/MyDrive/dataset/Konkani Prime News_100817.wav',  # Your audio file\n",
        "            'odt_path': '/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt',  # Optional: reference transcript\n",
        "            'session_id': 'session_001'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Process files\n",
        "    all_segments = []\n",
        "\n",
        "    for file_info in files_to_process:\n",
        "        try:\n",
        "            # Check if files exist\n",
        "            if not os.path.exists(file_info['audio_path']):\n",
        "                print(f\"❌ Audio file not found: {file_info['audio_path']}\")\n",
        "                continue\n",
        "\n",
        "            # Process\n",
        "            segments = aligner.process_audio_file(\n",
        "                audio_path=file_info['audio_path'],\n",
        "                odt_path=file_info.get('odt_path'),\n",
        "                session_id=file_info.get('session_id')\n",
        "            )\n",
        "\n",
        "            all_segments.extend(segments)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file_info['audio_path']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINAL SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total segments created: {len(all_segments)}\")\n",
        "    print(f\"Total duration: {sum(s['duration'] for s in all_segments):.2f}s\")\n",
        "    print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RUN\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5h5wS1yCo4O",
        "outputId": "cf8e4d87-aad0-4d32-8564-68cfb2367b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Silero VAD model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Silero VAD loaded\n",
            "Loading Whisper model: small...\n",
            "✓ Whisper loaded on cuda\n",
            "\n",
            "================================================================================\n",
            "PROCESSING: Konkani Prime News_100817.wav\n",
            "Session: session_001\n",
            "================================================================================\n",
            "\n",
            "Loading transcript: 10 AUG PRIME_non_bold (1).odt\n",
            "✓ Loaded 27 paragraphs, 6076 characters\n",
            "Step 1: Detecting speech segments...\n",
            "✓ Found 71 speech segments\n",
            "✓ Merged to 37 segments\n",
            "\n",
            "Step 2: Transcribing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 37/37 [01:00<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Transcribed 75 segments\n",
            "\n",
            "Step 3: Validating segments...\n",
            "✓ 68/75 segments passed validation\n",
            "\n",
            "Step 4: Saving segments...\n",
            "Loading audio for extraction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving segments: 100%|██████████| 68/68 [00:01<00:00, 38.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Saved manifest to: /content/drive/MyDrive/dataset/whisper_segments/metadata/session_001_manifest.json\n",
            "✓ Saved JSONL to: /content/drive/MyDrive/dataset/whisper_segments/metadata/session_001_train.jsonl\n",
            "\n",
            "================================================================================\n",
            "✓ COMPLETE! Created 68 segments\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY\n",
            "================================================================================\n",
            "Total segments created: 68\n",
            "Total duration: 336.08s\n",
            "Output directory: /content/drive/MyDrive/dataset/whisper_segments\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy librosa faster-whisper tqdm odfpy soundfile python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkzcJZzJDh9c",
        "outputId": "fa15c1a7-b452-4bae-ada0-3cb87fbf6e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: faster-whisper in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: odfpy in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.35.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (1.23.2)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (16.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from odfpy) (0.7.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.10)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2025.10.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from odf import text, teletype\n",
        "from odf.opendocument import load\n",
        "\n",
        "class PhoneticTranscriptMatcher:\n",
        "    def __init__(self, whisper_dir, odt_path, output_dir, noise_threshold=10):\n",
        "        self.whisper_dir = Path(whisper_dir)\n",
        "        self.odt_path = Path(odt_path)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.noise_threshold = noise_threshold  # Minimum word count\n",
        "        self.reference_paragraphs = []\n",
        "        self.used_paragraphs = set()  # Track which paragraphs have been used\n",
        "\n",
        "    def read_odt(self):\n",
        "        \"\"\"Extract text from ODT file\"\"\"\n",
        "        print(\"Reading ODT reference document...\")\n",
        "        try:\n",
        "            doc = load(self.odt_path)\n",
        "            all_paragraphs = doc.getElementsByType(text.P)\n",
        "\n",
        "            for para in all_paragraphs:\n",
        "                para_text = teletype.extractText(para)\n",
        "                if para_text.strip():\n",
        "                    self.reference_paragraphs.append(para_text.strip())\n",
        "\n",
        "            print(f\"Loaded {len(self.reference_paragraphs)} paragraphs from ODT\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading ODT: {e}\")\n",
        "            return False\n",
        "\n",
        "    def normalize_marathi_phonetic(self, char):\n",
        "        \"\"\"Normalize Marathi/Konkani characters to their phonetic equivalents\"\"\"\n",
        "        # Remove vowel marks (matras)\n",
        "        vowel_marks = ['ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', '़', 'ृ', 'ॅ']\n",
        "        if char in vowel_marks:\n",
        "            return ''\n",
        "\n",
        "        # Phonetic normalization mapping\n",
        "        phonetic_map = {\n",
        "            # थ / ट / ठ -> ट\n",
        "            'थ': 'ट', 'ठ': 'ट',\n",
        "            # ध / ड / ढ -> ड\n",
        "            'ध': 'ड', 'ढ': 'ड',\n",
        "            # फ / प -> प\n",
        "            'फ': 'प',\n",
        "            # भ / ब -> ब\n",
        "            'भ': 'ब',\n",
        "            # छ / च -> च\n",
        "            'छ': 'च',\n",
        "            # झ / ज -> ज\n",
        "            'झ': 'ज',\n",
        "            # ख / क / घ / ग -> क\n",
        "            'ख': 'क', 'घ': 'क', 'ग': 'क',\n",
        "            # ण / न -> न\n",
        "            'ण': 'न',\n",
        "            # ष / श / स -> स\n",
        "            'ष': 'स', 'श': 'स',\n",
        "            # ळ / ल -> ल\n",
        "            'ळ': 'ल',\n",
        "        }\n",
        "\n",
        "        return phonetic_map.get(char, char.lower())\n",
        "\n",
        "    def get_first_letters(self, text):\n",
        "        \"\"\"Extract first letters of each word for phonetic matching\"\"\"\n",
        "        words = re.findall(r'\\S+', text)\n",
        "        first_letters = []\n",
        "        for word in words:\n",
        "            # Remove punctuation from start\n",
        "            clean_word = re.sub(r'^[^\\w]+', '', word)\n",
        "            if clean_word:\n",
        "                first_char = clean_word[0]\n",
        "                normalized = self.normalize_marathi_phonetic(first_char)\n",
        "                if normalized:  # Only add if not empty (vowel marks return '')\n",
        "                    first_letters.append(normalized)\n",
        "        return first_letters\n",
        "\n",
        "    def is_noisy_transcript(self, content):\n",
        "        \"\"\"Detect if transcript is likely noise/useless\"\"\"\n",
        "        if not content:\n",
        "            return True\n",
        "\n",
        "        words = content.split()\n",
        "        if len(words) < self.noise_threshold:\n",
        "            return True\n",
        "\n",
        "        # Check for excessive repetition\n",
        "        unique_words = set(words)\n",
        "        repetition_ratio = len(unique_words) / len(words)\n",
        "        if repetition_ratio < 0.2:  # Too repetitive\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def match_by_first_letters(self, transcript_text):\n",
        "        \"\"\"Match transcript to reference paragraph based on first letter sounds\"\"\"\n",
        "        transcript_letters = self.get_first_letters(transcript_text)\n",
        "\n",
        "        if len(transcript_letters) < 3:\n",
        "            return None, 0, -1\n",
        "\n",
        "        # Compare first 5-10 letters\n",
        "        compare_length = min(10, len(transcript_letters))\n",
        "        transcript_signature = transcript_letters[:compare_length]\n",
        "\n",
        "        best_match = None\n",
        "        best_score = 0\n",
        "        best_idx = -1\n",
        "\n",
        "        for idx, ref_para in enumerate(self.reference_paragraphs):\n",
        "            # Skip if this paragraph has already been used\n",
        "            if idx in self.used_paragraphs:\n",
        "                continue\n",
        "\n",
        "            ref_letters = self.get_first_letters(ref_para)\n",
        "\n",
        "            if len(ref_letters) < 3:\n",
        "                continue\n",
        "\n",
        "            ref_signature = ref_letters[:compare_length]\n",
        "\n",
        "            # Calculate matching score\n",
        "            matches = sum(1 for i, letter in enumerate(transcript_signature)\n",
        "                         if i < len(ref_signature) and letter == ref_signature[i])\n",
        "\n",
        "            score = matches / compare_length\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = ref_para\n",
        "                best_idx = idx\n",
        "\n",
        "        return best_match, best_score, best_idx\n",
        "\n",
        "    def process_transcripts(self):\n",
        "        \"\"\"Process all transcript files\"\"\"\n",
        "        if not self.read_odt():\n",
        "            print(\"Failed to read ODT file. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Create output directory\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Check if directory exists\n",
        "        if not self.whisper_dir.exists():\n",
        "            print(f\"ERROR: Directory not found: {self.whisper_dir}\")\n",
        "            return\n",
        "\n",
        "        # Get all text files (try different patterns)\n",
        "        transcript_files = list(self.whisper_dir.glob('*.txt'))\n",
        "        if not transcript_files:\n",
        "            transcript_files = list(self.whisper_dir.glob('**/*.txt'))  # Search subdirectories\n",
        "\n",
        "        if not transcript_files:\n",
        "            print(f\"\\nNo .txt files found in: {self.whisper_dir}\")\n",
        "            print(\"Contents of directory:\")\n",
        "            try:\n",
        "                for item in self.whisper_dir.iterdir():\n",
        "                    print(f\"  - {item.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Cannot read directory: {e}\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nFound {len(transcript_files)} transcript files\")\n",
        "\n",
        "        processed = 0\n",
        "        skipped_noise = 0\n",
        "        no_match = 0\n",
        "\n",
        "        for txt_file in sorted(transcript_files):\n",
        "            print(f\"\\nProcessing: {txt_file.name}\")\n",
        "\n",
        "            # Read transcript\n",
        "            try:\n",
        "                with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "                    transcript_content = f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️  Error reading file: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Match based on first letter sounds (process all files, no noise filtering)\n",
        "            matched_text, score, idx = self.match_by_first_letters(transcript_content)\n",
        "\n",
        "            # Get first letters for display\n",
        "            trans_letters = ''.join(self.get_first_letters(transcript_content)[:15])\n",
        "\n",
        "            if score < 0.1:  # Low confidence match\n",
        "                print(f\"  ⚠️  No good match (score: {score:.2f}, letters: {trans_letters})\")\n",
        "                no_match += 1\n",
        "                # Skip this file - no good match found\n",
        "                continue\n",
        "            else:\n",
        "                ref_letters = ''.join(self.get_first_letters(matched_text)[:15]) if matched_text else ''\n",
        "                print(f\"  ✓ Match found (score: {score:.2f}, para: {idx})\")\n",
        "                print(f\"    Transcript letters: {trans_letters}\")\n",
        "                print(f\"    Reference letters:  {ref_letters}\")\n",
        "\n",
        "                # Mark this paragraph as used\n",
        "                self.used_paragraphs.add(idx)\n",
        "\n",
        "                # Output only the reference text from ODT\n",
        "                output_content = matched_text\n",
        "\n",
        "            # Save to output\n",
        "            output_file = self.output_dir / txt_file.name\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(output_content)\n",
        "\n",
        "            processed += 1\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PROCESSING SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total files found: {len(transcript_files)}\")\n",
        "        print(f\"Successfully processed: {processed}\")\n",
        "        print(f\"Skipped (noise): {skipped_noise}\")\n",
        "        print(f\"No good match: {no_match}\")\n",
        "        print(f\"Unique paragraphs used: {len(self.used_paragraphs)}/{len(self.reference_paragraphs)}\")\n",
        "        print(f\"\\nOutput saved to: {self.output_dir}\")\n",
        "\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure paths\n",
        "    WHISPER_DIR = \"/content/drive/MyDrive/dataset/whisper_segments/text\"\n",
        "    ODT_FILE = \"/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt\"\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/dataset/final_text\"\n",
        "\n",
        "    # Initialize and run\n",
        "    matcher = PhoneticTranscriptMatcher(\n",
        "        whisper_dir=WHISPER_DIR,\n",
        "        odt_path=ODT_FILE,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        noise_threshold=10  # Minimum words to not be considered noise\n",
        "    )\n",
        "\n",
        "    matcher.process_transcripts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiJxHNg1JoK_",
        "outputId": "84828e58-f870-4eef-a801-6665c9e551b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading ODT reference document...\n",
            "Loaded 27 paragraphs from ODT\n",
            "\n",
            "Found 68 transcript files\n",
            "\n",
            "Processing: session_001_0000.txt\n",
            "  ✓ Match found (score: 0.50, para: 0)\n",
            "    Transcript letters: नबपअ\n",
            "    Reference letters:  नपपक\n",
            "\n",
            "Processing: session_001_0001.txt\n",
            "  ✓ Match found (score: 0.10, para: 1)\n",
            "    Transcript letters: तवदकबवतआटडकसअउप\n",
            "    Reference letters:  टनडकवटटडकसउपउलस\n",
            "\n",
            "Processing: session_001_0002.txt\n",
            "  ✓ Match found (score: 0.20, para: 8)\n",
            "    Transcript letters: कसबआबकपवपजततपमब\n",
            "    Reference letters:  कवटआटडसऑकपउसआकन\n",
            "\n",
            "Processing: session_001_0003.txt\n",
            "  ✓ Match found (score: 0.30, para: 9)\n",
            "    Transcript letters: असकसकरबपककककललउ\n",
            "    Reference letters:  कपकककपनसकहकमबअस\n",
            "\n",
            "Processing: session_001_0004.txt\n",
            "  ✓ Match found (score: 0.30, para: 4)\n",
            "    Transcript letters: आजतममदलदचसवतहकत\n",
            "    Reference letters:  आजममदडकवहबदललएप\n",
            "\n",
            "Processing: session_001_0005.txt\n",
            "  ✓ Match found (score: 0.20, para: 7)\n",
            "    Transcript letters: चकबदवसउआमतहन\n",
            "    Reference letters:  बपयबदएआएमतकपडआ\n",
            "\n",
            "Processing: session_001_0006.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: ॐॐ)\n",
            "\n",
            "Processing: session_001_0007.txt\n",
            "  ✓ Match found (score: 0.10, para: 14)\n",
            "    Transcript letters: अबपकतबदआसअआआमतक\n",
            "    Reference letters:  समरसपबसकजनजकतदम\n",
            "\n",
            "Processing: session_001_0008.txt\n",
            "  ✓ Match found (score: 0.20, para: 10)\n",
            "    Transcript letters: अदसदरपवककतवटअसक\n",
            "    Reference letters:  रएकमपरकककनरललकह\n",
            "\n",
            "Processing: session_001_0009.txt\n",
            "  ✓ Match found (score: 0.50, para: 13)\n",
            "    Transcript letters: अदरद\n",
            "    Reference letters:  कआरदवनआसजसकमवतत\n",
            "\n",
            "Processing: session_001_0010.txt\n",
            "  ✓ Match found (score: 0.40, para: 17)\n",
            "    Transcript letters: मतवदन\n",
            "    Reference letters:  मवकदआआउपकववरनकक\n",
            "\n",
            "Processing: session_001_0011.txt\n",
            "  ✓ Match found (score: 0.25, para: 19)\n",
            "    Transcript letters: मपअतवसअल\n",
            "    Reference letters:  बपककमससपयनकमदवज\n",
            "\n",
            "Processing: session_001_0012.txt\n",
            "  ✓ Match found (score: 0.20, para: 5)\n",
            "    Transcript letters: आआआआआआआआआआआआआआआ\n",
            "    Reference letters:  मआमवबचसदवआमककप7\n",
            "\n",
            "Processing: session_001_0013.txt\n",
            "  ✓ Match found (score: 0.40, para: 23)\n",
            "    Transcript letters: कबपककपनसलकमबअसक\n",
            "    Reference letters:  ऑइपपआपडकलकहकआतए\n",
            "\n",
            "Processing: session_001_0014.txt\n",
            "  ✓ Match found (score: 0.20, para: 16)\n",
            "    Transcript letters: कबपजपजततलमककस\n",
            "    Reference letters:  करर1पचउहतहबपजजम\n",
            "\n",
            "Processing: session_001_0015.txt\n",
            "  ✓ Match found (score: 0.20, para: 21)\n",
            "    Transcript letters: मकजलअतनचमअकककल\n",
            "    Reference letters:  दकसअबहसतमबतचटनक\n",
            "\n",
            "Processing: session_001_0016.txt\n",
            "  ✓ Match found (score: 0.20, para: 2)\n",
            "    Transcript letters: कअवकलअकरकपदजयदत\n",
            "    Reference letters:  कपकपनपजमतपमबअकस\n",
            "\n",
            "Processing: session_001_0017.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रजअकब)\n",
            "\n",
            "Processing: session_001_0018.txt\n",
            "  ✓ Match found (score: 0.25, para: 3)\n",
            "    Transcript letters: मपकरकसकल\n",
            "    Reference letters:  एकतरपकककनलककरपप\n",
            "\n",
            "Processing: session_001_0019.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रललउक)\n",
            "\n",
            "Processing: session_001_0020.txt\n",
            "  ✓ Match found (score: 0.33, para: 12)\n",
            "    Transcript letters: हमस\n",
            "    Reference letters:  बमवरएआरजमडदमतकस\n",
            "\n",
            "Processing: session_001_0021.txt\n",
            "  ✓ Match found (score: 0.14, para: 26)\n",
            "    Transcript letters: अपलसबपम\n",
            "    Reference letters:  हबहसहकतपवटपआपवल\n",
            "\n",
            "Processing: session_001_0022.txt\n",
            "  ✓ Match found (score: 0.20, para: 24)\n",
            "    Transcript letters: मकउकसकमकनल\n",
            "    Reference letters:  मबआमनआपकवतब2कतस\n",
            "\n",
            "Processing: session_001_0023.txt\n",
            "  ✓ Match found (score: 0.10, para: 20)\n",
            "    Transcript letters: अआकअरतदलकप\n",
            "    Reference letters:  कसकववसटपचमकवपतउ\n",
            "\n",
            "Processing: session_001_0024.txt\n",
            "  ✓ Match found (score: 0.20, para: 11)\n",
            "    Transcript letters: कअममप\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0025.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बमवरअअरसजमददमकस)\n",
            "\n",
            "Processing: session_001_0026.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: वपदहकल)\n",
            "\n",
            "Processing: session_001_0027.txt\n",
            "  ✓ Match found (score: 0.10, para: 25)\n",
            "    Transcript letters: समरसपसबबसकरज\n",
            "    Reference letters:  सदतआयउकअदलआसलरल\n",
            "\n",
            "Processing: session_001_0028.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: नकदतदससकबल)\n",
            "\n",
            "Processing: session_001_0029.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: अअवआककरक)\n",
            "\n",
            "Processing: session_001_0030.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तसहपककद)\n",
            "\n",
            "Processing: session_001_0031.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इजकसजहलमज)\n",
            "\n",
            "Processing: session_001_0032.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तअमअकरउदप)\n",
            "\n",
            "Processing: session_001_0033.txt\n",
            "  ✓ Match found (score: 0.12, para: 15)\n",
            "    Transcript letters: कररपपचउह\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0034.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तहपजमतमबल)\n",
            "\n",
            "Processing: session_001_0035.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तइपपआसमकत)\n",
            "\n",
            "Processing: session_001_0036.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रकजमपबतरकद)\n",
            "\n",
            "Processing: session_001_0037.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मतपककबककवकबव)\n",
            "\n",
            "Processing: session_001_0038.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बपचककमससतपननकमद)\n",
            "\n",
            "Processing: session_001_0039.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: आसजकनकमसमपकजपज)\n",
            "\n",
            "Processing: session_001_0040.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इकसननकप)\n",
            "\n",
            "Processing: session_001_0041.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बबवजकपतबकलल)\n",
            "\n",
            "Processing: session_001_0042.txt\n",
            "  ✓ Match found (score: 0.10, para: 18)\n",
            "    Transcript letters: कसकववसतमपजआकवपत\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0043.txt\n",
            "  ✓ Match found (score: 0.11, para: 22)\n",
            "    Transcript letters: आपबजमकमपम\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0044.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: पमजरलकल)\n",
            "\n",
            "Processing: session_001_0045.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तबकससहसकचअबहसकच)\n",
            "\n",
            "Processing: session_001_0046.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: ससएकचरम)\n",
            "\n",
            "Processing: session_001_0047.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रआकहसइपनस)\n",
            "\n",
            "Processing: session_001_0048.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मससकबरतकनल)\n",
            "\n",
            "Processing: session_001_0049.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: cjavvapapek)\n",
            "\n",
            "Processing: session_001_0050.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: aiffअपडकलक)\n",
            "\n",
            "Processing: session_001_0051.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: यमआतदआपकआक)\n",
            "\n",
            "Processing: session_001_0052.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इकआपसaअaचअदवव)\n",
            "\n",
            "Processing: session_001_0053.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: कबइजल)\n",
            "\n",
            "Processing: session_001_0054.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: कबइजल)\n",
            "\n",
            "Processing: session_001_0055.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मसबसआमनआपकल)\n",
            "\n",
            "Processing: session_001_0056.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: वतपयअकतसबदचक)\n",
            "\n",
            "Processing: session_001_0057.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: वदककजव)\n",
            "\n",
            "Processing: session_001_0058.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मआसबकपपवसबवड)\n",
            "\n",
            "Processing: session_001_0059.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इअदमजबवसतकदअक)\n",
            "\n",
            "Processing: session_001_0060.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मएतइज)\n",
            "\n",
            "Processing: session_001_0061.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: सददतआपउकअदलकरसल)\n",
            "\n",
            "Processing: session_001_0062.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बहनदअदरअअक)\n",
            "\n",
            "Processing: session_001_0063.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: कअबकसललनअवल)\n",
            "\n",
            "Processing: session_001_0064.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: आबकहकरककल)\n",
            "\n",
            "Processing: session_001_0065.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इनमदनस)\n",
            "\n",
            "Processing: session_001_0066.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: यकपवपपनपवटलम)\n",
            "\n",
            "Processing: session_001_0067.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: पपतपपपतपपपतपपपत)\n",
            "\n",
            "============================================================\n",
            "PROCESSING SUMMARY\n",
            "============================================================\n",
            "Total files found: 68\n",
            "Successfully processed: 26\n",
            "Skipped (noise): 0\n",
            "No good match: 42\n",
            "Unique paragraphs used: 26/27\n",
            "\n",
            "Output saved to: /content/drive/MyDrive/dataset/final_text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiAZhHyMSzBs",
        "outputId": "18be58f5-894c-40e7-dcbf-c3d8f9e1da83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting odfpy\n",
            "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/717.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from odfpy) (0.7.1)\n",
            "Building wheels for collected packages: odfpy\n",
            "  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for odfpy: filename=odfpy-1.4.1-py2.py3-none-any.whl size=160673 sha256=4ac8bd269b5e770819bec1763e8a0525afdf4ee7d67515975dfb130d9ce8b277\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/5d/63/8243a7ee78fff0f944d638fd0e66d7278888f5e2285d7346b6\n",
            "Successfully built odfpy\n",
            "Installing collected packages: odfpy\n",
            "Successfully installed odfpy-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_KEY = \"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\"\n",
        "\n",
        "url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"HTTP-Referer\": \"http://localhost\",  # optional, but recommended\n",
        "    \"X-Title\": \"Gemini Test Script\",      # optional\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": \"google/gemini-2.5-flash-lite\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Hello! Test message. What is 2+2?\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "\n",
        "print(\"Status:\", response.status_code)\n",
        "print(\"Response:\")\n",
        "print(response.text)\n",
        "\n",
        "try:\n",
        "    print(\"\\nAssistant reply:\")\n",
        "    print(json.loads(response.text)[\"choices\"][0][\"message\"][\"content\"])\n",
        "except:\n",
        "    print(\"Could not parse response.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjtXBKiiAK5Q",
        "outputId": "1b204b09-aa99-435b-cbc5-bda113abf16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: 200\n",
            "Response:\n",
            "{\"id\":\"gen-1761842604-WsSfAkC7SjgZvLVTZIT5\",\"provider\":\"Google\",\"model\":\"google/gemini-2.5-flash-lite\",\"object\":\"chat.completion\",\"created\":1761842604,\"choices\":[{\"logprobs\":null,\"finish_reason\":\"stop\",\"native_finish_reason\":\"STOP\",\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello! Test message acknowledged.\\n\\n2 + 2 = **4**\",\"refusal\":null,\"reasoning\":null}}],\"usage\":{\"prompt_tokens\":12,\"completion_tokens\":15,\"total_tokens\":27,\"prompt_tokens_details\":{\"cached_tokens\":0},\"completion_tokens_details\":{\"reasoning_tokens\":0,\"image_tokens\":0}}}\n",
            "\n",
            "Assistant reply:\n",
            "Hello! Test message acknowledged.\n",
            "\n",
            "2 + 2 = **4**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import deque\n",
        "\n",
        "class SemanticWhisperAligner:\n",
        "    def __init__(self, gemini_api_key, whisper_folder, odt_path, output_folder):\n",
        "        \"\"\"\n",
        "        Semantic aligner that matches based on meaning, not word count.\n",
        "        Now includes validation to skip noise/null data.\n",
        "\n",
        "        Args:\n",
        "            gemini_api_key: API key for Gemini via OpenRouter\n",
        "            whisper_folder: Path to folder with Whisper segments\n",
        "            odt_path: Path to ODT file with correct transcript\n",
        "            output_folder: Path to save aligned correct transcripts\n",
        "        \"\"\"\n",
        "        self.gemini_api_key = gemini_api_key\n",
        "        self.whisper_folder = Path(whisper_folder)\n",
        "        self.odt_path = Path(odt_path)\n",
        "        self.output_folder = Path(output_folder)\n",
        "        self.url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "        # Validate API key\n",
        "        if not self.gemini_api_key:\n",
        "            raise ValueError(\"ERROR: GEMINI_API_KEY is not set!\")\n",
        "\n",
        "        print(f\"✓ API Key loaded: {self.gemini_api_key[:8]}...{self.gemini_api_key[-4:]}\")\n",
        "\n",
        "        # Create output folder\n",
        "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Progress tracking\n",
        "        self.progress_file = self.output_folder / \"alignment_progress.json\"\n",
        "        self.progress = self._load_progress()\n",
        "\n",
        "        # Read ODT as full text (not split into words yet)\n",
        "        self.odt_full_text = self._read_odt()\n",
        "        self.odt_words = self.odt_full_text.split()\n",
        "        self.current_position = self.progress.get(\"current_position\", 0)\n",
        "\n",
        "        print(f\"✓ Loaded {len(self.odt_words)} words from ODT reference\")\n",
        "\n",
        "    def _load_progress(self):\n",
        "        \"\"\"Load or create progress tracker\"\"\"\n",
        "        if self.progress_file.exists():\n",
        "            with open(self.progress_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                # Ensure all required keys exist\n",
        "                data.setdefault(\"processed_files\", [])\n",
        "                data.setdefault(\"skipped_files\", [])\n",
        "                data.setdefault(\"current_position\", 0)\n",
        "                data.setdefault(\"alignment_log\", [])\n",
        "                if \"statistics\" not in data:\n",
        "                    data[\"statistics\"] = {}\n",
        "                stats = data[\"statistics\"]\n",
        "                stats.setdefault(\"total_files\", 0)\n",
        "                stats.setdefault(\"exact_match\", 0)\n",
        "                stats.setdefault(\"semantic_match\", 0)\n",
        "                stats.setdefault(\"fallback\", 0)\n",
        "                stats.setdefault(\"skipped_noise\", 0)\n",
        "                stats.setdefault(\"failed_validation\", 0)\n",
        "                stats.setdefault(\"words_used\", 0)\n",
        "                stats.setdefault(\"words_remaining\", 0)\n",
        "                return data\n",
        "        return {\n",
        "            \"processed_files\": [],\n",
        "            \"skipped_files\": [],\n",
        "            \"current_position\": 0,\n",
        "            \"alignment_log\": [],\n",
        "            \"statistics\": {\n",
        "                \"total_files\": 0,\n",
        "                \"exact_match\": 0,\n",
        "                \"semantic_match\": 0,\n",
        "                \"fallback\": 0,\n",
        "                \"skipped_noise\": 0,\n",
        "                \"failed_validation\": 0,\n",
        "                \"words_used\": 0,\n",
        "                \"words_remaining\": 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _save_progress(self):\n",
        "        \"\"\"Save progress\"\"\"\n",
        "        self.progress[\"current_position\"] = self.current_position\n",
        "        self.progress[\"statistics\"][\"words_remaining\"] = len(self.odt_words) - self.current_position\n",
        "        self.progress[\"last_updated\"] = datetime.now().isoformat()\n",
        "\n",
        "        with open(self.progress_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.progress, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def _read_odt(self):\n",
        "        \"\"\"Read ODT and return as full text\"\"\"\n",
        "        try:\n",
        "            from odf import text, teletype\n",
        "            from odf.opendocument import load\n",
        "\n",
        "            doc = load(self.odt_path)\n",
        "            all_text = []\n",
        "\n",
        "            for paragraph in doc.getElementsByType(text.P):\n",
        "                para_text = teletype.extractText(paragraph)\n",
        "                if para_text.strip():\n",
        "                    all_text.append(para_text.strip())\n",
        "\n",
        "            full_text = \" \".join(all_text)\n",
        "            return full_text\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"ERROR: odfpy not installed. Run: pip install odfpy\")\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR reading ODT: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_odt_context(self, num_words=50):\n",
        "        \"\"\"Get next N words from current position as context\"\"\"\n",
        "        end_pos = min(self.current_position + num_words, len(self.odt_words))\n",
        "        return ' '.join(self.odt_words[self.current_position:end_pos])\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        \"\"\"Normalize text for comparison\"\"\"\n",
        "        normalized = re.sub(r'[।,;!?\\.\\-]', '', text)\n",
        "        normalized = ' '.join(normalized.split())\n",
        "        return normalized.strip().lower()\n",
        "\n",
        "    def validate_whisper_input(self, whisper_text):\n",
        "        \"\"\"\n",
        "        Validate if whisper input is meaningful Konkani/Marathi text or just noise.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_valid, confidence, reason)\n",
        "        \"\"\"\n",
        "        # Quick checks for obvious noise\n",
        "        if not whisper_text or len(whisper_text.strip()) < 5:\n",
        "            return False, 0.0, \"Empty or too short\"\n",
        "\n",
        "        # Check for excessive repetition\n",
        "        words = whisper_text.split()\n",
        "        if len(words) > 3 and len(set(words)) == 1:\n",
        "            return False, 0.0, \"Repetitive noise\"\n",
        "\n",
        "        # Use Gemini to validate\n",
        "        prompt = f\"\"\"Analyze if this text is meaningful Konkani or Marathi speech, or just noise/null data.\n",
        "\n",
        "**Text to analyze:**\n",
        "{whisper_text}\n",
        "\n",
        "**Instructions:**\n",
        "1. Check if this contains actual Konkani/Marathi words and phrases\n",
        "2. Identify if it's just noise, silence markers, or random characters\n",
        "3. Look for linguistic patterns that indicate real speech\n",
        "\n",
        "**Respond with ONLY ONE of these:**\n",
        "- VALID: Meaningful Konkani/Marathi text\n",
        "- NOISE: Background noise, silence, or random sounds\n",
        "- PARTIAL: Mix of valid words and noise\n",
        "\n",
        "**Response:**\"\"\"\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.gemini_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"google/gemini-2.0-flash-exp:free\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.url, headers=headers, json=data, timeout=30)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                validation = result['choices'][0]['message']['content'].strip().upper()\n",
        "\n",
        "                if \"VALID\" in validation and \"NOISE\" not in validation:\n",
        "                    return True, 0.9, \"Valid Konkani/Marathi text\"\n",
        "                elif \"PARTIAL\" in validation:\n",
        "                    return True, 0.6, \"Partial valid content\"\n",
        "                else:\n",
        "                    return False, 0.2, \"Detected as noise/invalid\"\n",
        "            else:\n",
        "                print(f\"  ⚠ Validation API Error: {response.status_code}\")\n",
        "                return True, 0.5, \"API error - proceeding with caution\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Validation Exception: {str(e)[:100]}\")\n",
        "            return True, 0.5, \"Validation failed - proceeding with caution\"\n",
        "\n",
        "    def validate_output_alignment(self, whisper_text, aligned_text):\n",
        "        \"\"\"\n",
        "        Validate that the aligned output actually matches the whisper input semantically.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_valid, confidence, reason)\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Compare these two texts and determine if they represent the same content in Konkani/Marathi.\n",
        "\n",
        "**Whisper Transcription (may have errors):**\n",
        "{whisper_text}\n",
        "\n",
        "**Aligned ODT Text (reference):**\n",
        "{aligned_text}\n",
        "\n",
        "**Task:**\n",
        "Determine if these texts convey the SAME meaning/content, accounting for:\n",
        "- Spelling variations and transcription errors\n",
        "- Word boundary differences\n",
        "- Minor phonetic variations\n",
        "\n",
        "**Respond with ONLY ONE of these:**\n",
        "- MATCH: Texts represent the same content\n",
        "- MISMATCH: Texts are completely different content\n",
        "- UNCERTAIN: Cannot determine clearly\n",
        "\n",
        "**Response:**\"\"\"\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.gemini_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"google/gemini-2.0-flash-exp:free\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.url, headers=headers, json=data, timeout=30)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                validation = result['choices'][0]['message']['content'].strip().upper()\n",
        "\n",
        "                if \"MATCH\" in validation and \"MISMATCH\" not in validation:\n",
        "                    return True, 0.9, \"Semantic match confirmed\"\n",
        "                elif \"UNCERTAIN\" in validation:\n",
        "                    return True, 0.6, \"Uncertain but proceeding\"\n",
        "                else:\n",
        "                    return False, 0.2, \"Semantic mismatch detected\"\n",
        "            else:\n",
        "                print(f\"  ⚠ Output Validation API Error: {response.status_code}\")\n",
        "                return True, 0.5, \"API error - cannot validate\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Output Validation Exception: {str(e)[:100]}\")\n",
        "            return True, 0.5, \"Validation failed - cannot verify\"\n",
        "\n",
        "    def semantic_align(self, whisper_text):\n",
        "        \"\"\"\n",
        "        Use Gemini to find the matching ODT text based on MEANING, not word count.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (matched_odt_text, word_count_used, confidence)\n",
        "        \"\"\"\n",
        "        # Get a large context window from ODT (next 100 words)\n",
        "        odt_context = self.get_odt_context(100)\n",
        "\n",
        "        prompt = f\"\"\"You are aligning a noisy Whisper transcription with clean reference text in Konkani.\n",
        "\n",
        "**Whisper Segment (noisy, may have errors):**\n",
        "{whisper_text}\n",
        "\n",
        "**Reference ODT Text (next 100 words from current position):**\n",
        "{odt_context}\n",
        "\n",
        "**Task:**\n",
        "1. Find the EXACT portion of the ODT text that corresponds to the Whisper segment\n",
        "2. The Whisper text has errors: wrong word boundaries, spelling mistakes, etc.\n",
        "3. Focus on MEANING and PHONETICS, not word count\n",
        "4. Extract and return ONLY the matching portion from ODT text\n",
        "5. Do NOT add or change anything - extract the exact matching text from ODT\n",
        "\n",
        "**Important:**\n",
        "- Whisper has {len(whisper_text.split())} words, but ODT match may have different word count\n",
        "- Return ONLY the matched ODT text, nothing else\n",
        "\n",
        "**Matched ODT Text:**\"\"\"\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.gemini_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"google/gemini-2.5-flash-lite\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.url, headers=headers, json=data, timeout=60)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                matched_text = result['choices'][0]['message']['content'].strip()\n",
        "\n",
        "                # Remove markdown formatting if present\n",
        "                matched_text = matched_text.replace('**', '').replace('*', '')\n",
        "                matched_text = matched_text.strip()\n",
        "\n",
        "                if not matched_text or len(matched_text) < 10:\n",
        "                    return None, 0, 0\n",
        "\n",
        "                # Calculate how many ODT words were matched\n",
        "                word_count = len(matched_text.split())\n",
        "\n",
        "                # Verify the match exists in ODT context\n",
        "                if self.normalize_text(matched_text) in self.normalize_text(odt_context):\n",
        "                    return matched_text, word_count, 0.9\n",
        "                else:\n",
        "                    # Try to find best overlap\n",
        "                    return matched_text, word_count, 0.7\n",
        "\n",
        "            else:\n",
        "                error_msg = response.text[:200] if response.text else \"No response\"\n",
        "                print(f\"  ⚠ API Error {response.status_code}: {error_msg}\")\n",
        "                return None, 0, 0\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"  ⚠ API Timeout\")\n",
        "            return None, 0, 0\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Exception: {str(e)[:100]}\")\n",
        "            return None, 0, 0\n",
        "\n",
        "    def align_segment(self, whisper_text, filename):\n",
        "        \"\"\"\n",
        "        Align a single whisper segment with ODT text.\n",
        "        Now includes input validation and output verification.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (aligned_text, method_used, confidence, words_consumed, skip_reason)\n",
        "        \"\"\"\n",
        "        whisper_words = whisper_text.split()\n",
        "        num_whisper_words = len(whisper_words)\n",
        "\n",
        "        print(f\"\\n  {'─'*66}\")\n",
        "        print(f\"  📄 Whisper Input:\")\n",
        "        print(f\"  Text: '{whisper_text[:100]}{'...' if len(whisper_text) > 100 else ''}'\")\n",
        "        print(f\"  Words: {num_whisper_words}\")\n",
        "        print(f\"  ODT Position: {self.current_position}/{len(self.odt_words)}\")\n",
        "\n",
        "        # VALIDATION STEP 1: Check if whisper input is valid\n",
        "        print(f\"\\n  🔍 Validating input...\")\n",
        "        is_valid, val_confidence, val_reason = self.validate_whisper_input(whisper_text)\n",
        "\n",
        "        if not is_valid:\n",
        "            print(f\"  ❌ SKIPPING: {val_reason}\")\n",
        "            print(f\"  Confidence: {val_confidence:.1%}\")\n",
        "            print(f\"  {'─'*66}\")\n",
        "            return None, \"skipped_noise\", 0, 0, val_reason\n",
        "\n",
        "        print(f\"  ✓ Input validated: {val_reason} (confidence: {val_confidence:.1%})\")\n",
        "\n",
        "        # Check if we have enough ODT words left\n",
        "        if self.current_position >= len(self.odt_words):\n",
        "            print(f\"  ⚠ No more ODT words available!\")\n",
        "            print(f\"  {'─'*66}\")\n",
        "            return None, \"no_words\", 0, 0, \"No ODT words remaining\"\n",
        "\n",
        "        # STEP 1: Try exact match with same word count\n",
        "        odt_same_count = ' '.join(self.odt_words[self.current_position:self.current_position + num_whisper_words])\n",
        "\n",
        "        if self.normalize_text(whisper_text) == self.normalize_text(odt_same_count):\n",
        "            aligned_text = odt_same_count\n",
        "            method = \"exact\"\n",
        "            confidence = 1.0\n",
        "            words_consumed = num_whisper_words\n",
        "        else:\n",
        "            # STEP 2: Use semantic alignment with Gemini\n",
        "            print(f\"  🤖 Using semantic alignment...\")\n",
        "            matched_text, word_count, confidence = self.semantic_align(whisper_text)\n",
        "\n",
        "            if matched_text and word_count > 0:\n",
        "                aligned_text = matched_text\n",
        "                method = \"semantic\"\n",
        "                words_consumed = word_count\n",
        "            else:\n",
        "                # STEP 3: Fallback - use same word count\n",
        "                aligned_text = odt_same_count\n",
        "                method = \"fallback\"\n",
        "                confidence = 0.3\n",
        "                words_consumed = num_whisper_words\n",
        "\n",
        "        # VALIDATION STEP 2: Verify output alignment\n",
        "        print(f\"\\n  🔍 Validating output alignment...\")\n",
        "        output_valid, out_confidence, out_reason = self.validate_output_alignment(whisper_text, aligned_text)\n",
        "\n",
        "        if not output_valid:\n",
        "            print(f\"  ❌ OUTPUT VALIDATION FAILED: {out_reason}\")\n",
        "            print(f\"  Confidence: {out_confidence:.1%}\")\n",
        "            print(f\"  {'─'*66}\")\n",
        "            return None, \"failed_validation\", 0, 0, out_reason\n",
        "\n",
        "        print(f\"  ✓ Output validated: {out_reason} (confidence: {out_confidence:.1%})\")\n",
        "\n",
        "        # Print alignment results\n",
        "        print(f\"\\n  ✓ Match Type: {method.upper()}\")\n",
        "        print(f\"  📝 ODT Output: '{aligned_text[:100]}{'...' if len(aligned_text) > 100 else ''}'\")\n",
        "\n",
        "        if method == \"semantic\":\n",
        "            print(f\"\\n  🔍 WORD COUNT ANALYSIS:\")\n",
        "            print(f\"  ┌──────────────────────────────────────────────┐\")\n",
        "            print(f\"  │ Whisper: {num_whisper_words:3d} words → ODT: {words_consumed:3d} words           │\")\n",
        "\n",
        "            if words_consumed < num_whisper_words:\n",
        "                diff = num_whisper_words - words_consumed\n",
        "                print(f\"  │ ⚠ Whisper split {diff} word(s) incorrectly    │\")\n",
        "            elif words_consumed > num_whisper_words:\n",
        "                diff = words_consumed - num_whisper_words\n",
        "                print(f\"  │ ⚠ Whisper merged {diff} word(s) incorrectly   │\")\n",
        "            else:\n",
        "                print(f\"  │ ✓ Word counts match                          │\")\n",
        "\n",
        "            print(f\"  │ ✓ Consuming {words_consumed} ODT words (CORRECT)       │\")\n",
        "            print(f\"  └──────────────────────────────────────────────┘\")\n",
        "\n",
        "        print(f\"  Words consumed: {words_consumed}\")\n",
        "        print(f\"  Confidence: {confidence:.2%}\")\n",
        "        print(f\"  {'─'*66}\")\n",
        "\n",
        "        return aligned_text, method, confidence, words_consumed, None\n",
        "\n",
        "    def get_whisper_files(self):\n",
        "        \"\"\"Get all whisper files sorted\"\"\"\n",
        "        files = list(self.whisper_folder.glob(\"*.txt\"))\n",
        "\n",
        "        def sort_key(filepath):\n",
        "            match = re.search(r'session_(\\d+)_(\\d+)', filepath.name)\n",
        "            if match:\n",
        "                return (int(match.group(1)), int(match.group(2)))\n",
        "            return (0, 0)\n",
        "\n",
        "        files.sort(key=sort_key)\n",
        "\n",
        "        # Print files found\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FILES FOUND IN INPUT DIRECTORY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Directory: {self.whisper_folder}\")\n",
        "        print(f\"Total files found: {len(files)}\\n\")\n",
        "\n",
        "        if files:\n",
        "            print(\"First 10 files:\")\n",
        "            for i, file in enumerate(files[:10], 1):\n",
        "                size = file.stat().st_size\n",
        "                print(f\"  {i:2d}. {file.name:<30s} ({size:,} bytes)\")\n",
        "\n",
        "            if len(files) > 10:\n",
        "                print(f\"  ... and {len(files) - 10} more files\")\n",
        "        else:\n",
        "            print(\"  ⚠ No .txt files found!\")\n",
        "\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        return files\n",
        "\n",
        "    def process_all_files(self, skip_processed=True):\n",
        "        \"\"\"Process all whisper segments with validation\"\"\"\n",
        "        whisper_files = self.get_whisper_files()\n",
        "        self.progress[\"statistics\"][\"total_files\"] = len(whisper_files)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"SEMANTIC WHISPER-ODT ALIGNMENT (with Validation)\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Total Whisper segments: {len(whisper_files)}\")\n",
        "        print(f\"Total ODT words: {len(self.odt_words)}\")\n",
        "        print(f\"Starting position: {self.current_position}\")\n",
        "        print(f\"Words remaining: {len(self.odt_words) - self.current_position}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        for i, whisper_file in enumerate(whisper_files):\n",
        "            filename = whisper_file.name\n",
        "\n",
        "            # Skip if processed\n",
        "            if skip_processed and filename in self.progress[\"processed_files\"]:\n",
        "                print(f\"[{i+1}/{len(whisper_files)}] ⊘ Skipping {filename} (already processed)\")\n",
        "                continue\n",
        "\n",
        "            # Skip if already marked as skipped\n",
        "            if skip_processed and filename in self.progress[\"skipped_files\"]:\n",
        "                print(f\"[{i+1}/{len(whisper_files)}] ⊘ Skipping {filename} (marked as noise/invalid)\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n{'█'*70}\")\n",
        "            print(f\"[{i+1}/{len(whisper_files)}] Processing: {filename}\")\n",
        "            print(f\"{'█'*70}\")\n",
        "\n",
        "            # Read whisper text\n",
        "            with open(whisper_file, 'r', encoding='utf-8') as f:\n",
        "                whisper_text = f.read().strip()\n",
        "\n",
        "            # Align segment (with validation)\n",
        "            aligned_text, method, confidence, words_consumed, skip_reason = self.align_segment(whisper_text, filename)\n",
        "\n",
        "            # Handle skipped files\n",
        "            if method == \"skipped_noise\":\n",
        "                print(f\"\\n  🚫 File SKIPPED - Reason: {skip_reason}\")\n",
        "                self.progress[\"skipped_files\"].append(filename)\n",
        "                self.progress[\"statistics\"][\"skipped_noise\"] += 1\n",
        "                self.progress[\"alignment_log\"].append({\n",
        "                    \"file\": filename,\n",
        "                    \"method\": method,\n",
        "                    \"reason\": skip_reason,\n",
        "                    \"status\": \"skipped\"\n",
        "                })\n",
        "                self._save_progress()\n",
        "                continue\n",
        "\n",
        "            # Handle failed validation\n",
        "            if method == \"failed_validation\":\n",
        "                print(f\"\\n  ❌ File FAILED VALIDATION - Reason: {skip_reason}\")\n",
        "                self.progress[\"skipped_files\"].append(filename)\n",
        "                self.progress[\"statistics\"][\"failed_validation\"] += 1\n",
        "                self.progress[\"alignment_log\"].append({\n",
        "                    \"file\": filename,\n",
        "                    \"method\": method,\n",
        "                    \"reason\": skip_reason,\n",
        "                    \"status\": \"failed_validation\"\n",
        "                })\n",
        "                self._save_progress()\n",
        "                continue\n",
        "\n",
        "            # Save successfully aligned text\n",
        "            if aligned_text and words_consumed > 0:\n",
        "                # Save aligned text\n",
        "                output_file = self.output_folder / filename\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    f.write(aligned_text)\n",
        "\n",
        "                print(f\"\\n  💾 Saved to: {output_file.name}\")\n",
        "\n",
        "                # Update position\n",
        "                self.current_position += words_consumed\n",
        "                self.progress[\"statistics\"][\"words_used\"] = self.current_position\n",
        "\n",
        "                print(f\"  📊 ODT Position: {self.current_position}/{len(self.odt_words)} ({len(self.odt_words) - self.current_position} remaining)\")\n",
        "\n",
        "                # Show comparison\n",
        "                print(f\"\\n  📋 COMPARISON:\")\n",
        "                print(f\"  {'─'*66}\")\n",
        "                print(f\"  Whisper words:     {len(whisper_text.split()):3d}\")\n",
        "                print(f\"  ODT words matched: {len(aligned_text.split()):3d}\")\n",
        "                print(f\"  ODT words consumed: {words_consumed:3d}\")\n",
        "                print(f\"  Method:            {method.upper()}\")\n",
        "                print(f\"  Confidence:        {confidence:.1%}\")\n",
        "                print(f\"  {'─'*66}\")\n",
        "\n",
        "                # Log alignment\n",
        "                self.progress[\"alignment_log\"].append({\n",
        "                    \"file\": filename,\n",
        "                    \"method\": method,\n",
        "                    \"confidence\": confidence,\n",
        "                    \"whisper_words\": len(whisper_text.split()),\n",
        "                    \"odt_words\": len(aligned_text.split()),\n",
        "                    \"odt_consumed\": words_consumed,\n",
        "                    \"position\": self.current_position,\n",
        "                    \"status\": \"success\"\n",
        "                })\n",
        "\n",
        "                # Update statistics\n",
        "                self.progress[\"processed_files\"].append(filename)\n",
        "                if method == \"exact\":\n",
        "                    self.progress[\"statistics\"][\"exact_match\"] += 1\n",
        "                elif method == \"semantic\":\n",
        "                    self.progress[\"statistics\"][\"semantic_match\"] += 1\n",
        "                else:\n",
        "                    self.progress[\"statistics\"][\"fallback\"] += 1\n",
        "\n",
        "                self._save_progress()\n",
        "\n",
        "            # Warn if running low\n",
        "            remaining = len(self.odt_words) - self.current_position\n",
        "            if remaining < 100:\n",
        "                print(f\"\\n{'⚠'*35}\")\n",
        "                print(f\"⚠ WARNING: Only {remaining} ODT words remaining!\")\n",
        "                print(f\"{'⚠'*35}\")\n",
        "\n",
        "        self._print_statistics()\n",
        "        self._print_detailed_log()\n",
        "\n",
        "    def _print_statistics(self):\n",
        "        \"\"\"Print statistics\"\"\"\n",
        "        stats = self.progress[\"statistics\"]\n",
        "        total_processed = stats['exact_match'] + stats['semantic_match'] + stats['fallback']\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ALIGNMENT STATISTICS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Total Files:        {stats['total_files']}\")\n",
        "        print(f\"\\n✓ Successfully Processed: {total_processed}\")\n",
        "        print(f\"  Exact Matches:      {stats['exact_match']:3d} ({stats['exact_match']/max(total_processed,1)*100:5.1f}%)\")\n",
        "        print(f\"  Semantic Matches:   {stats['semantic_match']:3d} ({stats['semantic_match']/max(total_processed,1)*100:5.1f}%)\")\n",
        "        print(f\"  Fallback:           {stats['fallback']:3d} ({stats['fallback']/max(total_processed,1)*100:5.1f}%)\")\n",
        "        print(f\"\\n❌ Skipped/Failed:\")\n",
        "        print(f\"  Noise/Invalid:      {stats['skipped_noise']:3d}\")\n",
        "        print(f\"  Failed Validation:  {stats['failed_validation']:3d}\")\n",
        "        print(f\"\\n📊 ODT Progress:\")\n",
        "        print(f\"  Words Used:         {stats['words_used']:,}\")\n",
        "        print(f\"  Words Remaining:    {stats['words_remaining']:,}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "    def _print_detailed_log(self):\n",
        "        \"\"\"Print detailed log\"\"\"\n",
        "        if not self.progress[\"alignment_log\"]:\n",
        "            return\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"DETAILED ALIGNMENT LOG (Last 10 files)\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        recent_logs = self.progress[\"alignment_log\"][-10:]\n",
        "\n",
        "        for i, log in enumerate(recent_logs, 1):\n",
        "            status_icon = \"✓\" if log.get(\"status\") == \"success\" else \"❌\"\n",
        "            print(f\"{status_icon} {i}. {log['file']}\")\n",
        "            print(f\"   Method: {log['method'].upper():<12s}\", end=\"\")\n",
        "\n",
        "            if log.get(\"status\") == \"success\":\n",
        "                print(f\" Confidence: {log.get('confidence', 0):.1%}\")\n",
        "                print(f\"   Whisper: {log.get('whisper_words')} words → ODT: {log.get('odt_words')} words (consumed {log.get('odt_consumed')})\")\n",
        "                print(f\"   Position: {log.get('position')}\")\n",
        "            else:\n",
        "                print(f\" Reason: {log.get('reason', 'Unknown')}\")\n",
        "            print()\n",
        "\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Install: !pip install odfpy\n",
        "\n",
        "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "    WHISPER_FOLDER = \"/content/drive/MyDrive/dataset/whisper_segments/text\"\n",
        "    ODT_PATH = \"/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt\"\n",
        "    OUTPUT_FOLDER = \"/content/drive/MyDrive/dataset/corrected_segments\"\n",
        "\n",
        "    aligner = SemanticWhisperAligner(\n",
        "        gemini_api_key=GEMINI_API_KEY,\n",
        "        whisper_folder=WHISPER_FOLDER,\n",
        "        odt_path=ODT_PATH,\n",
        "        output_folder=OUTPUT_FOLDER\n",
        "    )\n",
        "\n",
        "    aligner.process_all_files(skip_processed=True)\n",
        "\n",
        "    print(\"\\n✓ Alignment complete!\")\n",
        "    print(f\"  Input (Whisper): {WHISPER_FOLDER}\")\n",
        "    print(f\"  Output (Aligned): {OUTPUT_FOLDER}\")\n",
        "    print(f\"\\n📊 Check 'alignment_progress.json' for detailed logs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NQavxtdVEJxg",
        "outputId": "53bd9d2b-80d5-47e1-8d26-2422b0bbde18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ API Key loaded: sk-or-v1...e4cf\n",
            "✓ Loaded 832 words from ODT reference\n",
            "\n",
            "======================================================================\n",
            "FILES FOUND IN INPUT DIRECTORY\n",
            "======================================================================\n",
            "Directory: /content/drive/MyDrive/dataset/whisper_segments/text\n",
            "Total files found: 68\n",
            "\n",
            "First 10 files:\n",
            "   1. session_001_0000.txt           (75 bytes)\n",
            "   2. session_001_0001.txt           (509 bytes)\n",
            "   3. session_001_0002.txt           (475 bytes)\n",
            "   4. session_001_0003.txt           (431 bytes)\n",
            "   5. session_001_0004.txt           (243 bytes)\n",
            "   6. session_001_0005.txt           (145 bytes)\n",
            "   7. session_001_0006.txt           (295 bytes)\n",
            "   8. session_001_0007.txt           (252 bytes)\n",
            "   9. session_001_0008.txt           (208 bytes)\n",
            "  10. session_001_0009.txt           (99 bytes)\n",
            "  ... and 58 more files\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SEMANTIC WHISPER-ODT ALIGNMENT (with Validation)\n",
            "======================================================================\n",
            "Total Whisper segments: 68\n",
            "Total ODT words: 832\n",
            "Starting position: 0\n",
            "Words remaining: 832\n",
            "======================================================================\n",
            "\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "[1/68] Processing: session_001_0000.txt\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  📄 Whisper Input:\n",
            "  Text: 'नमस्कार बोई प्रुद अंखोब्रों'\n",
            "  Words: 4\n",
            "  ODT Position: 0/832\n",
            "\n",
            "  🔍 Validating input...\n",
            "  ⚠ Validation API Error: 429\n",
            "  ✓ Input validated: API error - proceeding with caution (confidence: 50.0%)\n",
            "  🤖 Using semantic alignment...\n",
            "\n",
            "  🔍 Validating output alignment...\n",
            "  ⚠ Output Validation API Error: 429\n",
            "  ✓ Output validated: API error - cannot validate (confidence: 50.0%)\n",
            "\n",
            "  ✓ Match Type: SEMANTIC\n",
            "  📝 ODT Output: 'नमस्कार पळोवया प्रुडंट'\n",
            "\n",
            "  🔍 WORD COUNT ANALYSIS:\n",
            "  ┌──────────────────────────────────────────────┐\n",
            "  │ Whisper:   4 words → ODT:   3 words           │\n",
            "  │ ⚠ Whisper split 1 word(s) incorrectly    │\n",
            "  │ ✓ Consuming 3 ODT words (CORRECT)       │\n",
            "  └──────────────────────────────────────────────┘\n",
            "  Words consumed: 3\n",
            "  Confidence: 90.00%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "  💾 Saved to: session_001_0000.txt\n",
            "  📊 ODT Position: 3/832 (829 remaining)\n",
            "\n",
            "  📋 COMPARISON:\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  Whisper words:       4\n",
            "  ODT words matched:   3\n",
            "  ODT words consumed:   3\n",
            "  Method:            SEMANTIC\n",
            "  Confidence:        90.0%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "[2/68] Processing: session_001_0001.txt\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  📄 Whisper Input:\n",
            "  Text: 'तेंकर वाल्यक्ना दर्बान, गवाई भर्चा वाटर तेंकर आचा ट्रास्पोट धिपार्ट्मन करतोला सिझ्टी अडिट, उद्का पर ...'\n",
            "  Words: 30\n",
            "  ODT Position: 3/832\n",
            "\n",
            "  🔍 Validating input...\n",
            "  ⚠ Validation API Error: 429\n",
            "  ✓ Input validated: API error - proceeding with caution (confidence: 50.0%)\n",
            "  🤖 Using semantic alignment...\n",
            "\n",
            "  🔍 Validating output alignment...\n",
            "  ⚠ Output Validation API Error: 429\n",
            "  ✓ Output validated: API error - cannot validate (confidence: 50.0%)\n",
            "\n",
            "  ✓ Match Type: SEMANTIC\n",
            "  📝 ODT Output: 'तेंकरवाल्यांक ना धरबांद गोंयभरच्या वॉटर टॅंकरांचें ट्रान्स्पोर्ट डिपोर्टमेन्ट करतलो सेफ्टीऑडिट उदका ...'\n",
            "\n",
            "  🔍 WORD COUNT ANALYSIS:\n",
            "  ┌──────────────────────────────────────────────┐\n",
            "  │ Whisper:  30 words → ODT:  23 words           │\n",
            "  │ ⚠ Whisper split 7 word(s) incorrectly    │\n",
            "  │ ✓ Consuming 23 ODT words (CORRECT)       │\n",
            "  └──────────────────────────────────────────────┘\n",
            "  Words consumed: 23\n",
            "  Confidence: 70.00%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "  💾 Saved to: session_001_0001.txt\n",
            "  📊 ODT Position: 26/832 (806 remaining)\n",
            "\n",
            "  📋 COMPARISON:\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  Whisper words:      30\n",
            "  ODT words matched:  23\n",
            "  ODT words consumed:  23\n",
            "  Method:            SEMANTIC\n",
            "  Confidence:        70.0%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "[3/68] Processing: session_001_0002.txt\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  📄 Whisper Input:\n",
            "  Text: 'कर सा बन्दूर आप्रकल बाख केंद्राजी पर वान्गिना, प्रोज्यक्त जालम ते ती फुट, माधे बचा अभियनान करनाटकाक ...'\n",
            "  Words: 28\n",
            "  ODT Position: 26/832\n",
            "\n",
            "  🔍 Validating input...\n",
            "  ⚠ Validation API Error: 429\n",
            "  ✓ Input validated: API error - proceeding with caution (confidence: 50.0%)\n",
            "  🤖 Using semantic alignment...\n",
            "\n",
            "  🔍 Validating output alignment...\n",
            "  ⚠ Output Validation API Error: 429\n",
            "  ✓ Output validated: API error - cannot validate (confidence: 50.0%)\n",
            "\n",
            "  ✓ Match Type: SEMANTIC\n",
            "  📝 ODT Output: 'कळसाभंडुरा प्रकल्पाक केंद्राची परवानगी ना प्रॉजॅक्ट जाला म्हण्टा ती फट म्हादय बचाव अभियानान कर्नाटका...'\n",
            "\n",
            "  🔍 WORD COUNT ANALYSIS:\n",
            "  ┌──────────────────────────────────────────────┐\n",
            "  │ Whisper:  28 words → ODT:  23 words           │\n",
            "  │ ⚠ Whisper split 5 word(s) incorrectly    │\n",
            "  │ ✓ Consuming 23 ODT words (CORRECT)       │\n",
            "  └──────────────────────────────────────────────┘\n",
            "  Words consumed: 23\n",
            "  Confidence: 90.00%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "  💾 Saved to: session_001_0002.txt\n",
            "  📊 ODT Position: 49/832 (783 remaining)\n",
            "\n",
            "  📋 COMPARISON:\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  Whisper words:      28\n",
            "  ODT words matched:  23\n",
            "  ODT words consumed:  23\n",
            "  Method:            SEMANTIC\n",
            "  Confidence:        90.0%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "[4/68] Processing: session_001_0003.txt\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  📄 Whisper Input:\n",
            "  Text: 'अक्तिविस्टान सो कोंगरे सिक्ते को राई बन्रा पर्रे करान काईईच कामा के लिना लाई उद्काचो कद्कती गिरिशान ...'\n",
            "  Words: 27\n",
            "  ODT Position: 49/832\n",
            "\n",
            "  🔍 Validating input...\n",
            "  ⚠ Validation API Error: 429\n",
            "  ✓ Input validated: API error - proceeding with caution (confidence: 50.0%)\n",
            "  🤖 Using semantic alignment...\n",
            "\n",
            "  🔍 Validating output alignment...\n",
            "  ⚠ Output Validation API Error: 429\n",
            "  ✓ Output validated: API error - cannot validate (confidence: 50.0%)\n",
            "\n",
            "  ✓ Match Type: SEMANTIC\n",
            "  📝 ODT Output: 'एक्टिव्हिस्टांचो कॉंग्रेसीक तेंको रायबंदरा पर्रीकारान कांयच कामां केल्लीं ना लायटउदकाच्यो कटकटी गिरी...'\n",
            "\n",
            "  🔍 WORD COUNT ANALYSIS:\n",
            "  ┌──────────────────────────────────────────────┐\n",
            "  │ Whisper:  27 words → ODT:  19 words           │\n",
            "  │ ⚠ Whisper split 8 word(s) incorrectly    │\n",
            "  │ ✓ Consuming 19 ODT words (CORRECT)       │\n",
            "  └──────────────────────────────────────────────┘\n",
            "  Words consumed: 19\n",
            "  Confidence: 90.00%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "  💾 Saved to: session_001_0003.txt\n",
            "  📊 ODT Position: 68/832 (764 remaining)\n",
            "\n",
            "  📋 COMPARISON:\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  Whisper words:      27\n",
            "  ODT words matched:  19\n",
            "  ODT words consumed:  19\n",
            "  Method:            SEMANTIC\n",
            "  Confidence:        90.0%\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "[5/68] Processing: session_001_0004.txt\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n",
            "  ──────────────────────────────────────────────────────────────────\n",
            "  📄 Whisper Input:\n",
            "  Text: 'आईरीशाग जी तो मार्टा मुन दिल ले दमके चके शींज विष्वाजी तान है को ताक बल लग देड लाख लिगल एड फुंड'\n",
            "  Words: 22\n",
            "  ODT Position: 68/832\n",
            "\n",
            "  🔍 Validating input...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3628951156.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    651\u001b[0m     )\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0maligner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_all_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_processed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✓ Alignment complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3628951156.py\u001b[0m in \u001b[0;36mprocess_all_files\u001b[0;34m(self, skip_processed)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# Align segment (with validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0maligned_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_consumed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhisper_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Handle skipped files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3628951156.py\u001b[0m in \u001b[0;36malign_segment\u001b[0;34m(self, whisper_text, filename)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# VALIDATION STEP 1: Check if whisper input is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n  🔍 Validating input...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mis_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_confidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_whisper_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhisper_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3628951156.py\u001b[0m in \u001b[0;36mvalidate_whisper_input\u001b[0;34m(self, whisper_text)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your API key (after revoking the old one!)\n",
        "%env GEMINI_API_KEY=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\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQzln2DGYrO",
        "outputId": "ea962a0a-fbbf-42c8-d449-5ccaf2b35ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GEMINI_API_KEY=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from odf.opendocument import load\n",
        "from odf import text, teletype\n",
        "import re\n",
        "\n",
        "def count_konkani_words_from_odt(file_path):\n",
        "    # Load ODT file\n",
        "    doc = load(file_path)\n",
        "    content = []\n",
        "\n",
        "    # Extract text paragraphs\n",
        "    for p in doc.getElementsByType(text.P):\n",
        "        para = teletype.extractText(p).strip()\n",
        "        if para:\n",
        "            content.append(para)\n",
        "\n",
        "    full_text = \" \".join(content)\n",
        "\n",
        "    # Regex for Konkani (Devanagari) + English/Numbers\n",
        "    pattern = r'[\\u0900-\\u097F]+|[a-zA-Z0-9]+'\n",
        "\n",
        "    words = re.findall(pattern, full_text)\n",
        "    return len(words), words\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt\"\n",
        "count, words = count_konkani_words_from_odt(file_path)\n",
        "\n",
        "print(f\"Total Konkani Words: {count}\")\n",
        "print(\"Sample:\", words[:844])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muRWq23iLzsO",
        "outputId": "bd59244c-2d98-4370-b583-49ce617e1dc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Konkani Words: 844\n",
            "Sample: ['नमस्कार', 'पळोवया', 'प्रुडंट', 'खबरो', 'टॅंकरवाल्यांक', 'ना', 'धरबांद', 'गोंयभरच्या', 'वॉटर', 'टॅंकरांचें', 'ट्रान्स्पोर्ट', 'डिपोर्टमेन्ट', 'करतलो', 'सेफ्टीऑडिट', 'उदका', 'पुरवणे', 'उपरांत', 'लोकांकडच्यान', 'सय', 'घेवपाची', 'सिस्टम', 'प्रुडंटाचे', 'ऑपा', 'वॉट', 'टॅंकर', 'एक्स्पोजेचो', 'इम्पॅक्ट', 'कळसाभंडुरा', 'प्रकल्पाक', 'केंद्राची', 'परवानगी', 'ना', 'प्रॉजॅक्ट', 'जाला', 'म्हण्टा', 'ती', 'फट', 'म्हादय', 'बचाव', 'अभियानान', 'कर्नाटकाक', 'सुप्रिम', 'कोर्टांत', 'केलें', 'एक्स्पोज', 'कोर्टान', 'मागलें', 'केंद्र', 'कर्नाटकाकडच्यान', 'एफिडॅव्हिट', 'एक्टिव्हिस्टांचो', 'कॉंग्रेसीक', 'तेंको', 'रायबंदरा', 'पर्रीकारान', 'कांयच', 'कामां', 'केल्लीं', 'ना', 'लायटउदकाच्यो', 'कटकटी', 'गिरीशाचो', 'रायबंदरा', 'प्रचार', 'पर्रीकार', 'खंय', 'सगळ्यात', 'भ्रष्ट', 'मुख्यमंत्री', 'आयरीशाक', 'जीतो', 'मारता', 'म्हूण', 'दिल्ले', 'धमकेचे', 'केशींत', 'विश्वजीतान', 'हायकोर्टांत', 'भल्लो', 'देड', 'लाख', 'लीगल', 'एड', 'फंड', 'चार्जशीट', 'रद्द', 'केस', 'बंद', '10', 'वर्सां', 'उपरांत', 'आयरिश', 'म्हण्टा', 'हरकत', 'ना', 'मुरगांवचो', 'आमदार', 'मिलींदाची', 'वास्कोसावन', 'बोरयेशिरोडा', 'चलत', 'साईबाबाचे', 'दर्शनाक', 'वचपाची', 'आंगवण', 'मिलिंदाची', 'कार्यकर्त्यांक', 'घेवन', 'फांतोडेरसावनबोरयेमेरेन', '7', 'वरांची', 'वारी', 'आनी', 'भारतांत', 'फुटसालाक', 'येतले', 'बरे', 'दिस', 'ए', 'एफ', 'सी', 'आनी', 'ए', 'आय', 'एफ', 'एफ', 'मेळून', 'तयार', 'करता', 'फुटसाल', 'डॅव्हलापमॅन्टाचो', 'आराखडो', 'गोंयभरच्या', 'वॉटर', 'टॅंकरांचें', 'आतां', 'ट्रान्स्पोर्ट', 'डिपोर्टमेन्ट', 'सेफ्टी', 'ऑडिट', 'करतलो', 'ते', 'पिवपाचें', 'उदकाक', 'सेफ', 'आसात', 'काय', 'ना', 'आनी', 'ट्रिपी', 'कॉंत्रादाप्रमाण', 'लोकांमेरेन', 'पावतात', 'काय', 'ना', 'ताचीय', 'खातरजमा', 'जातली', 'एखाद्या', 'सुवातीर', 'उदका', 'पुरवण', 'केली', 'काय', 'टॅंकरवाल्यान', 'थंयचे', 'लोकांची', 'सय', 'घेवपाची', 'आसता', 'आनी', 'डिपार्टमॅन्टाक', 'ट्रिपींचो', 'रिपोर्ट', 'दिवपाचो', 'आसता', 'मात', 'टॅंकरवाल्यांकधरबांद', 'ना', 'म्हूण', 'प्रुडंटान', 'ओपाची', 'टॅंकर', 'वॉटर', 'सप्लाय', 'एक्स्पोजे', 'केल्लो', 'एक्स्पोजेचो', 'इम्पॅक्ट', 'कळसाभंडुरा', 'प्रकल्पाचे', 'काम', 'कर्नाटकान', 'केंद्राची', 'परवानगी', 'नासतना', 'सुरु', 'केलां', 'ही', 'गजाल', 'म्हादय', 'बचाव', 'अभियानान', 'सुप्रिम', 'कोर्टांत', 'एक्पोज', 'केल्या', 'कळसा', 'बंडुरा', 'प्रकल्पाचे', 'काम', 'जवळ', 'जवळ', 'पुराय', 'जायत', 'आयलां', 'म्हूण', 'कर्नाटकान', 'कोर्टाक', 'सांगलां', 'मात', 'काम', 'जाल्लें', 'ना', 'आनी', 'तें', 'नेटान', 'चल्लां', 'म्हूण', 'अभियानान', 'कोर्टाक', 'क्लियर', 'केलें', 'कर्नाटकान', 'असो', 'अर्ज', 'केला', 'आनी', 'केंद्रान', 'तांकां', 'परवानगी', 'दिल्या', 'जाल्यार', 'दोगांनीय', 'ती', 'कोर्टाक', 'एफिडॅव्हिटाचेर', 'दाखोवची', 'पडटली', 'रायबंदराचे', 'एक्टिव्हिस्ट', 'कॉंग्रेसीफाटल्यान', 'मुख्यमंत्री', 'पर्रीकारान', 'रायबंदरा', 'कांयच', 'कामां', 'केल्लीं', 'ना', 'रायबंदरा', 'लोकांक', 'लायटउदकाच्यो', 'कटकटी', 'हिंदुक', 'मसुंडी', 'सारकी', 'ना', 'आपल्या', 'प्रचाराक', 'लोकांचो', 'बरो', 'प्रदिसात', 'मेळटा', 'म्हूण', 'काँग्रेसीचो', 'उमेदवार', 'गिरीश', 'चोडणकार', 'मिडियाकडेन', 'उलयलो', 'एक्टिव्हिस्ट', 'आयरीश', 'गिरीश', 'आनी', 'रॉयाक', 'तेंको', 'डिक्लॅर', 'करपाक', 'काँग्रेस', 'हावजांत', 'मिडियामुखार', 'पावलो', 'घेवया', 'एक', 'ब्रेक', 'भलायकी', 'मंत्री', 'विश्वजीत', 'राणेन', 'एक्टिविस्ट', 'आयरिश', 'रॉड्रीग्साक', 'जीतो', 'मारपाची', 'धमकी', 'दिल्ली', 'म्हण्टा', 'ती', 'केस', 'सॅटल', 'ताचे', 'आड', 'चार्जशीट', 'रद्द', 'लीगल', 'एड', 'फंडाक', 'विश्वजीताकडच्यान', 'देड', 'लाख', 'फंड', 'वसूल', 'करून', 'हायकोर्टान', 'केशीक', 'पूर्णविराम', 'दिलो', 'विश्वजीता', 'फंड', 'डिपॉजिटूय', 'केलो', 'वर्स', '2007', 'त', 'विश्वजितान', 'आयरीशाक', 'जितो', 'मारपाची', 'धमकी', 'दिली', 'म्हूण', 'हें', 'प्रकरण', 'गाजिल्लें', 'आयरिशाक', 'खंय', 'ही', 'केस', 'क्लोज', 'करपाक', 'हरकत', 'ना', 'गोंय', 'आनी', 'रत्नागीरी', 'दर्यादेग', 'वाठारांत', 'नुस्तें', 'आनी', 'सागरी', 'जिवावळीचें', 'संवर्धन', 'करुन', 'मत्स्योत्पादन', 'वाडचें', 'तशें', 'तंत्रज्ञानाचे', 'आदाराचेर', 'मत्स्योत्पादनाचेर', 'अन्नप्रक्रियेचे', 'वेगवेगळे', 'प्रयोग', 'आनी', 'उपक्रम', 'चालीक', 'लावपाचे', 'हेतान', 'गोंय', 'आनी', 'रत्नागिरीच्यो', '6', 'मुखेल', 'नुस्तेंकार', 'सोसायटी', 'मत्स्योत्पादक', 'आनी', 'खावडीच्या', 'उत्पादकांची', 'एकठांय', 'कबलात', 'जाल्या', 'खासगी', 'उद्देगाक', 'बरोबर', 'घेवन', 'हो', 'फिशरीज', 'इम्प्रुव्हमेंट', 'प्रॉजेक्ट', 'शाळांच्या', 'मॅनेजमॅन्टांनी', 'रिटायर', 'स्टाफाच्या', 'पॅन्शनचो', 'बेगोबेग', 'सॅटलमेंट', 'करुंक', 'जाय', 'ना', 'जाल्यार', 'काद्यान', 'तांका', 'दंड', 'म्हूण', 'शिक्षण', 'संचालक', 'गजाजन', 'भट', 'उलयला', 'एडमीशनावेळार', 'भुरग्यांचे', 'आर्धारकार्ड', 'करुन', 'घेयात', 'तशें', 'सर्कुलरुय', 'हे', 'पयली', 'खंय', 'खात्यान', 'काडलां', 'इलेक्शनाचोकोड', 'सुरु', 'जावचें', 'पयली', 'रिक्रुटमेंट', 'जाला', 'तांच्योच', 'अपॉयंटमेंट', 'ऑर्डरी', 'खंय', 'रिझल्टा', 'उपरांत', 'दिवपाच्यो', 'घेवया', 'एक', 'ब्रेक', 'कोंबा', 'रिंग', 'रोडावेल्या', '15', 'फामिलींक', 'चवथी', 'उपरांत', 'हालोवचे', 'तांचें', 'हावसींग', 'बोर्डांत', 'पुनर्वसन', 'जावंक', 'जाय', 'म्हूण', 'टिसीपी', 'मंत्री', 'विजय', 'उलयला', 'थंयचे', 'फामीलींची', 'फातोड्डेच्या', 'आमदारासयत', 'मडगांव', 'कलॅक्टरेटांत', 'बसका', 'जाली', 'रस्त्याचे', 'काम', 'जावंक', 'जाय', 'म्हूण', 'पंदरशीभीतर', 'तांकां', 'हालोवपाचो', 'कोर्टान', 'आदेश', 'दिला', 'मात', 'चवथ', 'आनी', 'फामिलींचो', 'विचार', 'करून', 'कांय', 'बदलांखातीर', 'कोर्टाक', 'विनवणी', 'करपाचो', 'विजयाचो', 'विचार', 'मुरगांव', 'वास्को', 'कुठ्ठाळे', 'दाबोळी', 'आनी', 'आगशीक', 'उदकाची', 'पुरवण', 'करपी', 'वेर्णाचे', 'वॉटर', 'रिजरवॉयराचेर', 'नजर', 'घालपाक', 'कोण', 'ना', 'रिजरवॉयराचे', 'एअर', 'वॅन्ट', 'कळमेल्यात', 'रिजरवॉयर', 'वयर', 'तसोच', 'उक्तो', 'दवरिल्ल्यान', 'उदकांत', 'किदें', 'पडटा', 'ताचो', 'नेम', 'ना', 'टाकी', 'दोन', 'तीन', 'वर्सांत', 'नितळ', 'करप', 'आसता', 'मात', 'थंय', 'टाकी', 'नितळ', 'केल्ल्याची', 'तारीख', 'माल्ल्या', '2012', 'खासा', 'रिपोर्ट', 'घेवया', 'एक', 'ब्रेक', 'बार्देस', 'पेडणेंच्या', 'गोवा', 'कूळ', 'मुंडकार', 'संघर्श', 'समितीन', 'पणजे', 'येवन', 'नवें', 'कूळ', 'मुंडकार', 'दुरुस्ती', 'विधेयक', 'जाळपाचो', 'लान', 'केलो', 'खरो', 'मात', 'इलॅक्शन', 'कोडांत', 'परमिशन', 'सारकें', 'नाशिल्ल्यान', 'प्लान', 'फिसकटलो', 'आशिल्ले', 'सात', 'आठ', 'जाणूच', 'करंजाळे', 'निदर्शनां', 'करता', 'म्हूण', 'सांगून', 'म्हापालिका', 'पोलीस', 'कलॅक्टराचें', 'परमिशन', 'घेतलें', 'इलॅक्शन', 'कमिशनाक', 'सांगलेंच', 'ना', 'निदर्शनां', 'केलीं', 'पणजे', 'बीजेपीवाल्यांचे', 'कंप्लेनीचेर', 'पुलिसांनी', 'तांकां', 'बिडकी', 'कवळूंक', 'लायली', 'गोंयात', 'सायबर', 'क्रायम', 'वाडाटा', 'वेबसायटींचेर', 'सॅक्स', 'टुरिजमाचें', 'प्रोमोशन', 'चल्लां', 'म्हूण', 'गोवा', 'वुमन्स', 'फोरमान', 'तिडक', 'उक्तायली', 'शिंदळकेखातीर', 'चलयां', 'आनी', 'बायलांची', 'पुरवण', 'करपी', 'वॅबसाटीं', 'चलतात', 'असले', 'प्रकार', 'बंद', 'जावंक', 'जाय', 'म्हूण', 'गोवा', 'वूमन्स', 'फोरम', 'मागता', 'फोरमान', 'मडगावां', 'जागृती', 'रॅली', 'काडली', 'दाबोळे', 'केशव', 'स्मृतीच्या', 'अकरावेच्या', 'भुरग्यांक', 'हायर', 'सेकंडरीचे', 'तीन', 'मजली', 'बिल्डिंगेचे', 'तेर्रासाचेर', 'चडोवन', 'टायल्स', 'नितळ', 'करूंक', 'लायिल्ले', 'प्रकरणाची', 'शिक्षण', 'खात्यान', 'दखल', 'घेतल्या', 'शिक्षण', 'संचालकान', 'एडीईआय', 'कडच्यान', 'रिपोर्ट', 'मागला', 'रिपोर्ट', 'आयलो', 'काय', 'हायर', 'सेकंडरीच्या', 'प्रिन्सिपलाक', 'नोटीस', 'वतली', 'म्हूण', 'शिक्षण', 'संचालक', 'गजानन', 'भट', 'प्रुडंटाकडेन', 'उलयलो', 'सिटिझन', 'जर्नलिस्टाचे', 'व्हिडिओवेल्यान', 'प्रुडंटान', 'हो', 'प्रकार', 'एक्स्पोज', 'केल्लो', 'घेवया', 'एक', 'ब्रेक', 'ऑल', 'इंडिया', 'फुटबॉल', 'फॅडरेशन', 'आतां', 'फुटसाल', 'डेव्हलाप', 'करपाचेर', 'लक्ष', 'घालतलें', 'ह्या', 'कामांत', 'आतां', 'तांकां', 'एशीयन', 'फुटबॉल', 'कन्फॅडरेशन', 'आदार', 'करतलें', 'इन्फ्रास्ट्रक्चर', 'कशें', 'आसा', 'कांय', 'पळोवंक', 'सद्या', 'एएफसी', 'आनी', 'एआयएफएफाचें', 'शिश्टमंडळ', 'देशभर', 'भोंवतले', 'गोंयात', 'ब्रेस्तारा', 'इन्स्पेक्शन', 'जालें', 'गोंयच्यो', 'साधन', 'सुविदा', 'तांकां', 'मानवल्यात', 'मुरगांवचो', 'बीजेपी', 'आमदार', 'मिलींद', 'नायकान', 'आंगवण', 'पुराय', 'केली', 'वास्को', 'ते', 'बोरये', '28', 'किलोमिटर', 'तो', 'साई', 'बाबाचें', 'दर्शनाक', 'चलत', 'गेलो', 'बरोबर', 'धायेक', 'कार्यकर्त्यांचीय', 'वारी', 'मिलींदान', 'सगल्यांक', 'बरोबर', 'घेवन', 'फांतोडेर', 'पाचंक', 'वास्कोसावन', 'बोरयेची', 'वाट', 'धल्ली', 'इलॅक्शनाकडेन', 'मिलिंद', 'जिकल्यार', 'बोरये', 'वारीची', 'साईबाबाक', 'ताचे', 'कार्य़कर्त्यान', 'आंगवण', 'केल्ली', 'मिलिंद', '143', 'मतांनी', 'जिकलो', 'स्वतंत्रताय', 'दीस', 'तेंकलो', 'आयकपाक', 'येना', 'उलोवंक', 'कळना', 'अश्या', 'दिव्यांग', 'लोकांखातीर', 'आता', 'सायन', 'लॅग्वेजीन', 'राष्ट्रगीत', 'लाँच', 'जालां', 'ब्रदरहुड', 'न्यु', 'दिल्ली', 'आनी', 'डिसॅबीलीटी', 'रायट्स', 'असोसिएशन', 'ऑफ', 'गोवा', 'कला', 'अकादमीक', 'बरोबर', 'घेवन', 'सायन', 'लॅग्वीजींतलो', 'नॅशनल', 'एन्थम', 'व्हिडियो', 'लाँच', 'केलो', 'अमिताभ', 'बच्चनाक', 'घेवन', 'हें', 'खास', 'राष्ट्रगीत', 'केलां', 'हें', 'बुलॅटिन', 'हांगांच', 'सोंपता', 'ह्यो', 'खबरो', 'तुमकां', 'प्रुडंट', 'वॉट्सॅप', 'ट्विटर', 'फेसबूक', 'आनी', 'प्रुडंट', 'व्हॅबसायटीचेर', 'लायव्ह', 'मेळटात', 'प्रुडंटाक', 'फॉलो', 'करात', 'नमस्कार']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# path to your folder containing txt files\n",
        "folder_path = \"/content/drive/MyDrive/dataset/whisper_segments/text\"\n",
        "\n",
        "# loop through the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".txt\"):  # only .txt files\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # open file with utf-8 encoding\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            print(f\"--- {file_name} ---\")\n",
        "            content = file.read()\n",
        "            print(content)\n",
        "            print(\"\\n\")  # spacing between files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ92aCH2akt3",
        "outputId": "548bb595-7d85-4361-f9f5-521a54978c01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- session_001_0000.txt ---\n",
            "नमस्कार बोई प्रुद अंखोब्रों\n",
            "\n",
            "\n",
            "--- session_001_0001.txt ---\n",
            "तेंकर वाल्यक्ना दर्बान, गवाई भर्चा वाटर तेंकर आचा ट्रास्पोट धिपार्ट्मन करतोला सिझ्टी अडिट, उद्का पर वने अप्रान्त लगा कर च्यान से ग्योपाची सिस्टम प्रुडन्दाचा अपा वाटर तेंकर अप्स्पोजेच अ इंपैक.\n",
            "\n",
            "\n",
            "--- session_001_0002.txt ---\n",
            "कर सा बन्दूर आप्रकल बाख केंद्राजी पर वान्गिना, प्रोज्यक्त जालम ते ती फुट, माधे बचा अभियनान करनाटकाक सुप्रिम कोटान, के लें अप्स्पोस कोटान मागलें केंद्र करनाटकाक गड्जाने अप्टीडेवेई.\n",
            "\n",
            "\n",
            "--- session_001_0003.txt ---\n",
            "अक्तिविस्टान सो कोंगरे सिक्ते को राई बन्रा पर्रे करान काईईच कामा के लिना लाई उद्काचो कद्कती गिरिशान सो राई बन्रा प्राचार पर्रे कर कही सग्यान्त ब्रष्ट मुक्यमंद्री\n",
            "\n",
            "\n",
            "--- session_001_0004.txt ---\n",
            "आईरीशाग जी तो मार्टा मुन दिल ले दमके चके शींज विष्वाजी तान है को ताक बल लग देड लाख लिगल एड फुंड\n",
            "\n",
            "\n",
            "--- session_001_0005.txt ---\n",
            "चार्षिट्रद, केस बंद, दा वर सा उपरानत आईरीश मन ता हरकोत ना\n",
            "\n",
            "\n",
            "--- session_001_0006.txt ---\n",
            "ॐ ॐ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ\n",
            "\n",
            "\n",
            "--- session_001_0007.txt ---\n",
            "अनी भार्दान फुट्साला के तले बारे दिस आप सी अनी आप आप में तैर करता फुट्साल ड़वलाप मेंटाजा अराख़वो\n",
            "\n",
            "\n",
            "--- session_001_0008.txt ---\n",
            "अगा दे सुवा दे रूद्का पूर वन केली काई तेंकर वालें थुईजल अखाची सुई गेव पाची अस्था\n",
            "\n",
            "\n",
            "--- session_001_0009.txt ---\n",
            "अनी दिपार्ट्मेंटाख रिपोड द्वाचास्था\n",
            "\n",
            "\n",
            "--- session_001_0010.txt ---\n",
            "मद तेंकर वाल्यांग द्वर्बान ना\n",
            "\n",
            "\n",
            "--- session_001_0011.txt ---\n",
            "मुन प्रुदन्टान अपाची तेंकर वाटर सप्लाय अख्स्पोजिकल लोग\n",
            "\n",
            "\n",
            "--- session_001_0012.txt ---\n",
            "आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आ�\n",
            "\n",
            "\n",
            "--- session_001_0013.txt ---\n",
            "कर्सा बन्दूरा प्रकल्बाचकाम करनाटकान केन्राची पर्वान्गी नास्तन सुरूके लाही गजाल मादेई बचाव अवियानान सुप्रिम कोर्टान्त अग्स्पोच के लेए.\n",
            "\n",
            "\n",
            "--- session_001_0014.txt ---\n",
            "कर्सा बन्दूरा प्रकल्पाचकाम जोवर-जोवर पूराए जाये ताये ताये ला, मुन करनाटकान कोर्टाक संगिलने.\n",
            "\n",
            "\n",
            "--- session_001_0015.txt ---\n",
            "माद काम जल लेना, अनी ते नेटान चलना, मुन अभियनान कोर्टा क्लीर के लें\n",
            "\n",
            "\n",
            "--- session_001_0016.txt ---\n",
            "करनाटगान असा वर्ज के ला, अनी केन रान्दां का परवान्गी दिल्या जल यार दोगानी ती कोर्टाक अपी देवीटा चेर दाखोची, मुन सुप्रिम कोर्टान सांगला\n",
            "\n",
            "\n",
            "--- session_001_0017.txt ---\n",
            "राएबन्रा जा अक्टिविस्ट कोंगरेशी भातलें\n",
            "\n",
            "\n",
            "--- session_001_0018.txt ---\n",
            "मुख्यमंद्री पुर्रे करान राएबन्रा काईच सकाम के लिना\n",
            "\n",
            "\n",
            "--- session_001_0019.txt ---\n",
            "राएबन्रा लोकांक लाएट उद्काजो कद्कती\n",
            "\n",
            "\n",
            "--- session_001_0020.txt ---\n",
            "हिन्दूक मोसुन्दी सरकिना\n",
            "\n",
            "\n",
            "--- session_001_0021.txt ---\n",
            "अपले प्रचाराक लगान सो भरो प्रुदिसाद मेंता\n",
            "\n",
            "\n",
            "--- session_001_0022.txt ---\n",
            "मुन खोंग्रेश़ उमेदवार गीरीष शोड़ान कार मेड्या गरे नुले लो\n",
            "\n",
            "\n",
            "--- session_001_0023.txt ---\n",
            "अक्टिविस्ट आयरीष गीरीष अनी रोयाक तेको दिक लेएर कर पा\n",
            "\n",
            "\n",
            "--- session_001_0024.txt ---\n",
            "खोंग्रेश अजात मेड्या मुखार पालो\n",
            "\n",
            "\n",
            "--- session_001_0025.txt ---\n",
            "बलाएकी मुन्त्री विशुजित रानें अक्टिविस्ट अएदिश रोट्रिक साग जितो मरबाची द्हुमकि दिल्ली मंत्त्तिक केस सेटृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृृ\n",
            "\n",
            "\n",
            "--- session_001_0026.txt ---\n",
            "विश्वेजिदाग फन्द दिपोजिट ही क्या लोग?\n",
            "\n",
            "\n",
            "--- session_001_0027.txt ---\n",
            "शानाचा मेनेज्मेंदानी रेटार स्तापाचा पेंषना सो भिगो भिग सेटलमें को रुब जाए.\n",
            "\n",
            "\n",
            "--- session_001_0028.txt ---\n",
            "नाजालर काई दियान तंक दंदमुन शिक्षन संचालक गजानन भधो लेला.\n",
            "\n",
            "\n",
            "--- session_001_0029.txt ---\n",
            "अदमीशन अवर वर्गेंचा आदार काड को रुन गयात.\n",
            "\n",
            "\n",
            "--- session_001_0030.txt ---\n",
            "तश्या सर्कूलरी हे पहिलिं कहें खात्यां दाल्ला\n",
            "\n",
            "\n",
            "--- session_001_0031.txt ---\n",
            "इलेक्षाना जो कोड सुरुजा। जब हैं लेक्रूट में जाला\n",
            "\n",
            "\n",
            "--- session_001_0032.txt ---\n",
            "तान्चोच अपाइन में अडरी कहें रिजाल्टा उप्रान दिव पाजो\n",
            "\n",
            "\n",
            "--- session_001_0033.txt ---\n",
            "कोंबार रिंग रोडावेले पन्द्रा फामिलिक चुथि उप्रान्द हलोचे\n",
            "\n",
            "\n",
            "--- session_001_0034.txt ---\n",
            "तान्चा हाँजिंबोडाद पुनर्वासन जाँजाय मुन तिस्पी मुन्द्री भीजयो लेला\n",
            "\n",
            "\n",
            "--- session_001_0035.txt ---\n",
            "तो इजा पामिलिची पातोदेचा आम्दरा सुइत मद्गाँ कलेक्टरी तान्बूसकजाली\n",
            "\n",
            "\n",
            "--- session_001_0036.txt ---\n",
            "रस्तेच काम जाूंजाय, मुन पंद्रषी भितर तांक रालोपाज़ कोर्डाना देष्दिला\n",
            "\n",
            "\n",
            "--- session_001_0037.txt ---\n",
            "मच्ष्वो तानी पामिलिजार करून, काई बदलां कहतिर कोर्डाक विनूनी करपाज़ भिजायाज़ विचार\n",
            "\n",
            "\n",
            "--- session_001_0038.txt ---\n",
            "बार्देश पेटने चा गवा कुड मुनकार संगर्ष समी तिन पंजे न नवे कुड मुनकार दूर्स्ती विदेः जाएपाचा प्लान क्यालोख हरो मात इलेक्षन कोणाग परमीशन सरके ना शिल्यन प्लान फिस्कोट लोग.\n",
            "\n",
            "\n",
            "--- session_001_0039.txt ---\n",
            "आशिल्ले साथाद जानुच, करन्जारे निदर्षान करता मुन साँन महापालिका पूलिस कलेक्टरेटा जे परमिशन जेतले,\n",
            "\n",
            "\n",
            "--- session_001_0040.txt ---\n",
            "इलेक्ष्यन कमिशनाक साँलेत ना, निदर्षान केलिए पूनीजे.\n",
            "\n",
            "\n",
            "--- session_001_0041.txt ---\n",
            "भीज़े भी वालाई जा कंपलेनिजर पूलिसानी तंकः भीडकी को लाई ली\n",
            "\n",
            "\n",
            "--- session_001_0042.txt ---\n",
            "गवायात साइबर क्राम वाट्ता, वेप्साइटिचर सेक्स तूरीजा माचा प्रमोशन जल्ला, आमड गवा वूमेंस प्रमान तिडग उक्ताए लिए, शिनर के कहतिर चलिया नि भायलांची पुर्वोन कर पी वेप्साइटी जल्तात.\n",
            "\n",
            "\n",
            "--- session_001_0043.txt ---\n",
            "आश्ले प्रकार बन्द जाूँँजाय मन गोवा मुमन्स पोराम माक्ता\n",
            "\n",
            "\n",
            "--- session_001_0044.txt ---\n",
            "फोरामान मद्गामा जागरोती रहे लिए का लि\n",
            "\n",
            "\n",
            "--- session_001_0045.txt ---\n",
            "ता बगे के शाव स्म्रती हाईर से गन्री चा अख्रावेचा बुर्गयाग हाईर से गन्री चा तीन मुजली बिल्निंगे चा तेर्रासा चिर्चोडवून तायल्स नितार करुक लाईले परकरानाची शिक्षन खातें दुख्ल गेत ले.\n",
            "\n",
            "\n",
            "--- session_001_0046.txt ---\n",
            "शिक्ष्यान संचालकान एद्याय कर चान रिपोड मागला\n",
            "\n",
            "\n",
            "--- session_001_0047.txt ---\n",
            "रिपोड आएलो काई हाईर सकंडर इच प्रिन्सिपलाक नोटी सुत्ली\n",
            "\n",
            "\n",
            "--- session_001_0048.txt ---\n",
            "मुन शिक्ष्यान संचालक गजानन भध रूड़ान ता करे नूले लो\n",
            "\n",
            "\n",
            "--- session_001_0049.txt ---\n",
            "CITIZON JOURNALIST ACHE VIDEO VELLY AND PRUDENT ANHOV PRAKARA EXPOSE KALLO\n",
            "\n",
            "\n",
            "--- session_001_0050.txt ---\n",
            "All India Football Federation, अत फुट्साल ड़ूलाप करपाच्याल लक्षा गालतले\n",
            "\n",
            "\n",
            "--- session_001_0051.txt ---\n",
            "यका मात आता ता दंग आश्यन फुट्बाल कन्फेडरिष्यन आदार करता\n",
            "\n",
            "\n",
            "--- session_001_0052.txt ---\n",
            "इन्फ्रस्ट्रक्चर कश्या आसकाय पनवक सद्या AFC अनी AI-FFI च्सिस्टम अंडर देश वर वावतले\n",
            "\n",
            "\n",
            "--- session_001_0053.txt ---\n",
            "गवायात ब्रस्टारा इंस्पेक्षन जा लें\n",
            "\n",
            "\n",
            "--- session_001_0054.txt ---\n",
            "गवायात ब्रस्टारा इंस्पेक्षन जा लें\n",
            "\n",
            "\n",
            "--- session_001_0055.txt ---\n",
            "मुर्गाँ सो भीजेवी सो आम्दार मिलिन नाएकान आँववन पुराए के लिए\n",
            "\n",
            "\n",
            "--- session_001_0056.txt ---\n",
            "वास्को ते पर ये अथाविस क्लोमिटर तो साई भाबाचे दर्षनाक चलत गलो\n",
            "\n",
            "\n",
            "--- session_001_0057.txt ---\n",
            "वर्वर दाए कर्ये करतें जी वारी\n",
            "\n",
            "\n",
            "--- session_001_0058.txt ---\n",
            "मिलिंद आं सगल्यांक भूर्वार ग्यून, फाथोदेर पाचाग वास्को सावन भूर्येची वाद डल्ली\n",
            "\n",
            "\n",
            "--- session_001_0059.txt ---\n",
            "इलेक्षन अगर देन मिलिंद जिकल्यार भूर्ये वारीची साईईभाबाक ताचा खार्यागर देन अंवोन किल्ली\n",
            "\n",
            "\n",
            "--- session_001_0060.txt ---\n",
            "मिलिन्द एक्षे त्रेजा इस्मतानी जिकिल्लों\n",
            "\n",
            "\n",
            "--- session_001_0061.txt ---\n",
            "स्वतन्द्र दाय दिस तेक्लो, आएक पाक्यना, उलोक करना, अशे दिव्यांग लोका खाति राता सायन लेंवेजिन राश्टर गित लोईच जाला.\n",
            "\n",
            "\n",
            "--- session_001_0062.txt ---\n",
            "ब्रदर हुड न्यू दिल्ली अनी दिजआबिलिटी राट्स असुस्विश्यान अप गोवा\n",
            "\n",
            "\n",
            "--- session_001_0063.txt ---\n",
            "गला अकादमीद बरोवर गेवन सायन लेंवेजिन्त लों नेशनल अंठम विडियो लाँईज्गलों\n",
            "\n",
            "\n",
            "--- session_001_0064.txt ---\n",
            "आमिता बच्चना गेवन हे खास रष्टर गीट के लाँं\n",
            "\n",
            "\n",
            "--- session_001_0065.txt ---\n",
            "इतले ने मुले दी नागत सवता\n",
            "\n",
            "\n",
            "--- session_001_0066.txt ---\n",
            "यो खुब्रोदुंको प्रुडन वाज्चाप प्रिटर, फिस्बूक निप प्रुडन वेप्साइ टीचरी लाईव मेटात\n",
            "\n",
            "\n",
            "--- session_001_0067.txt ---\n",
            "प्रुदन्ताक फलुगरी त्रावाद, प्रुदन्ताक प्रुदन्ताक फलुगरी त्रावाद, प्रुदन्ताक प्रुदन्ताक फलुगरी त्रावाद, प्रुदन्ताक प्रुदन्ताक फलुगरी त्रावाद, प्रुदन्ताक प्रुदन्ताक फलुगरी त्रावाद, प्रु\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transcript Segment Matcher\n",
        "Matches segmented transcript files to their corresponding lines in the full transcript\n",
        "using fuzzy string matching to handle OCR errors and text variations.\n",
        "\"\"\"\n",
        "\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "import json\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text by removing extra whitespace and special characters\n",
        "    while preserving the core content for matching.\n",
        "    \"\"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def fuzzy_match_score(str1: str, str2: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculate fuzzy matching score between two strings.\n",
        "    Returns a value between 0 and 1, where 1 is perfect match.\n",
        "    \"\"\"\n",
        "    return SequenceMatcher(None, str1, str2).ratio()\n",
        "\n",
        "\n",
        "def find_best_match(segment: str, transcript_lines: List[str],\n",
        "                    min_threshold: float = 0.6) -> Tuple[int, float, str]:\n",
        "    \"\"\"\n",
        "    Find the best matching line in the transcript for a given segment.\n",
        "\n",
        "    Args:\n",
        "        segment: The segment text to match\n",
        "        transcript_lines: List of lines from the full transcript\n",
        "        min_threshold: Minimum similarity score to consider a match\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (line_index, similarity_score, matched_line)\n",
        "    \"\"\"\n",
        "    segment_norm = normalize_text(segment)\n",
        "    best_score = 0\n",
        "    best_index = -1\n",
        "    best_line = \"\"\n",
        "\n",
        "    for idx, line in enumerate(transcript_lines):\n",
        "        line_norm = normalize_text(line)\n",
        "\n",
        "        # Skip empty lines\n",
        "        if not line_norm:\n",
        "            continue\n",
        "\n",
        "        # Calculate similarity score\n",
        "        score = fuzzy_match_score(segment_norm, line_norm)\n",
        "\n",
        "        # Also check if segment is contained within the line (for partial matches)\n",
        "        if segment_norm in line_norm:\n",
        "            score = max(score, 0.85)  # Boost score for containment\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_index = idx\n",
        "            best_line = line\n",
        "\n",
        "    if best_score < min_threshold:\n",
        "        return -1, best_score, \"\"\n",
        "\n",
        "    return best_index, best_score, best_line\n",
        "\n",
        "\n",
        "def match_segments_to_transcript(segments: Dict[str, str],\n",
        "                                 full_transcript: str,\n",
        "                                 min_threshold: float = 0.5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Match all segments to their corresponding lines in the full transcript.\n",
        "\n",
        "    Args:\n",
        "        segments: Dictionary mapping segment filenames to segment text\n",
        "        full_transcript: The complete transcript text\n",
        "        min_threshold: Minimum similarity threshold for matching\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing match results\n",
        "    \"\"\"\n",
        "    # Split transcript into lines\n",
        "    transcript_lines = [line.strip() for line in full_transcript.split('\\n')\n",
        "                       if line.strip()]\n",
        "\n",
        "    results = []\n",
        "    used_lines = set()  # Track which lines have been matched to avoid duplicates\n",
        "\n",
        "    # Sort segments by filename to process in order\n",
        "    sorted_segments = sorted(segments.items())\n",
        "\n",
        "    for segment_file, segment_text in sorted_segments:\n",
        "        # Skip if segment is mostly special characters or empty\n",
        "        if not normalize_text(segment_text) or len(normalize_text(segment_text)) < 3:\n",
        "            results.append({\n",
        "                'segment_file': segment_file,\n",
        "                'segment_text': segment_text,\n",
        "                'matched_line_index': -1,\n",
        "                'matched_line': '',\n",
        "                'similarity_score': 0.0,\n",
        "                'status': 'SKIPPED_EMPTY'\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Find best match in transcript\n",
        "        line_idx, score, matched_line = find_best_match(\n",
        "            segment_text, transcript_lines, min_threshold\n",
        "        )\n",
        "\n",
        "        # Check if this line was already matched\n",
        "        if line_idx in used_lines and line_idx != -1:\n",
        "            # Find next best match\n",
        "            temp_lines = transcript_lines.copy()\n",
        "            temp_lines[line_idx] = \"\"  # Temporarily remove used line\n",
        "            line_idx, score, matched_line = find_best_match(\n",
        "                segment_text, temp_lines, min_threshold\n",
        "            )\n",
        "\n",
        "        if line_idx != -1:\n",
        "            used_lines.add(line_idx)\n",
        "            status = 'MATCHED'\n",
        "        else:\n",
        "            status = 'NO_MATCH'\n",
        "\n",
        "        results.append({\n",
        "            'segment_file': segment_file,\n",
        "            'segment_text': segment_text[:100] + '...' if len(segment_text) > 100 else segment_text,\n",
        "            'matched_line_index': line_idx,\n",
        "            'matched_line': matched_line[:100] + '...' if len(matched_line) > 100 else matched_line,\n",
        "            'similarity_score': round(score, 3),\n",
        "            'status': status\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def display_results(results: List[Dict]):\n",
        "    \"\"\"Display matching results in a readable format.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"SEGMENT MATCHING RESULTS\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    matched_count = sum(1 for r in results if r['status'] == 'MATCHED')\n",
        "    total_count = len([r for r in results if r['status'] != 'SKIPPED_EMPTY'])\n",
        "\n",
        "    print(f\"\\nSummary: {matched_count}/{total_count} segments matched successfully\")\n",
        "    print(f\"Match rate: {matched_count/total_count*100:.1f}%\\n\")\n",
        "\n",
        "    for result in results:\n",
        "        print(f\"\\nSegment: {result['segment_file']}\")\n",
        "        print(f\"Status: {result['status']}\")\n",
        "        print(f\"Similarity Score: {result['similarity_score']}\")\n",
        "        if result['matched_line_index'] != -1:\n",
        "            print(f\"Matched Line Index: {result['matched_line_index']}\")\n",
        "        print(f\"Segment Text: {result['segment_text']}\")\n",
        "        if result['matched_line']:\n",
        "            print(f\"Matched Line: {result['matched_line']}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "\n",
        "def load_segments_from_folder(folder_path: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Load all segment files from the specified folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path: Path to the folder containing segment .txt files\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping filenames to their content\n",
        "    \"\"\"\n",
        "    segments = {}\n",
        "\n",
        "    # Check if folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder '{folder_path}' not found!\")\n",
        "        return segments\n",
        "\n",
        "    # Loop through all files in the folder\n",
        "    for file_name in sorted(os.listdir(folder_path)):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            try:\n",
        "                # Open and read file with utf-8 encoding\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read().strip()\n",
        "                    segments[file_name] = content\n",
        "                    print(f\"✓ Loaded: {file_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error loading {file_name}: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal segments loaded: {len(segments)}\\n\")\n",
        "    return segments\n",
        "\n",
        "\n",
        "def load_odt_file(odt_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Load and extract text from an ODT (OpenDocument Text) file.\n",
        "\n",
        "    Args:\n",
        "        odt_path: Path to the .odt file\n",
        "\n",
        "    Returns:\n",
        "        Extracted text content as a string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ODT files are zip archives containing XML\n",
        "        with ZipFile(odt_path, 'r') as odt_zip:\n",
        "            # Extract content.xml which contains the text\n",
        "            content_xml = odt_zip.read('content.xml')\n",
        "\n",
        "        # Parse XML\n",
        "        root = ET.fromstring(content_xml)\n",
        "\n",
        "        # Define namespace for ODT\n",
        "        namespaces = {\n",
        "            'text': 'urn:oasis:names:tc:opendocument:xmlns:text:1.0',\n",
        "            'office': 'urn:oasis:names:tc:opendocument:xmlns:office:1.0'\n",
        "        }\n",
        "\n",
        "        # Extract all text elements\n",
        "        text_elements = []\n",
        "\n",
        "        # Find all paragraph and heading elements\n",
        "        for elem in root.iter():\n",
        "            # Check if it's a text paragraph or heading\n",
        "            if elem.tag.endswith('}p') or elem.tag.endswith('}h'):\n",
        "                # Get all text content from this element and its children\n",
        "                text_content = ''.join(elem.itertext()).strip()\n",
        "                if text_content:\n",
        "                    text_elements.append(text_content)\n",
        "\n",
        "        # Join all text with newlines\n",
        "        full_text = '\\n'.join(text_elements)\n",
        "\n",
        "        print(f\"✓ Successfully loaded ODT file\")\n",
        "        print(f\"  Total characters: {len(full_text)}\")\n",
        "        print(f\"  Total lines: {len(text_elements)}\\n\")\n",
        "\n",
        "        return full_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading ODT file: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to your folder containing txt files\n",
        "    folder_path = \"/content/drive/MyDrive/dataset/whisper_segments/text\"\n",
        "\n",
        "    # Load all segments from folder\n",
        "    print(\"Loading segments from folder...\")\n",
        "    print(\"=\"*100)\n",
        "    segments = load_segments_from_folder(folder_path)\n",
        "\n",
        "    if not segments:\n",
        "        print(\"No segments loaded. Please check the folder path.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Path to your ODT transcript file\n",
        "    odt_file_path = \"/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt\"\n",
        "\n",
        "    # Load transcript from ODT file\n",
        "    print(\"Loading transcript from ODT file...\")\n",
        "    print(\"=\"*100)\n",
        "    full_transcript = load_odt_file(odt_file_path)\n",
        "\n",
        "    if not full_transcript:\n",
        "        print(\"Error: Could not load transcript from ODT file.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Perform matching with adjustable threshold\n",
        "    print(\"Starting matching process...\")\n",
        "    print(\"=\"*100)\n",
        "    results = match_segments_to_transcript(segments, full_transcript, min_threshold=0.5)\n",
        "\n",
        "    # Display results\n",
        "    display_results(results)\n",
        "\n",
        "    # Save results to JSON\n",
        "    output_file = 'matching_results.json'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Results saved to '{output_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFw5iQbeeT3v",
        "outputId": "dc63bbd3-6234-4744-8a9d-31cdd8874756"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading segments from folder...\n",
            "====================================================================================================\n",
            "✓ Loaded: session_001_0000.txt\n",
            "✓ Loaded: session_001_0001.txt\n",
            "✓ Loaded: session_001_0002.txt\n",
            "✓ Loaded: session_001_0003.txt\n",
            "✓ Loaded: session_001_0004.txt\n",
            "✓ Loaded: session_001_0005.txt\n",
            "✓ Loaded: session_001_0006.txt\n",
            "✓ Loaded: session_001_0007.txt\n",
            "✓ Loaded: session_001_0008.txt\n",
            "✓ Loaded: session_001_0009.txt\n",
            "✓ Loaded: session_001_0010.txt\n",
            "✓ Loaded: session_001_0011.txt\n",
            "✓ Loaded: session_001_0012.txt\n",
            "✓ Loaded: session_001_0013.txt\n",
            "✓ Loaded: session_001_0014.txt\n",
            "✓ Loaded: session_001_0015.txt\n",
            "✓ Loaded: session_001_0016.txt\n",
            "✓ Loaded: session_001_0017.txt\n",
            "✓ Loaded: session_001_0018.txt\n",
            "✓ Loaded: session_001_0019.txt\n",
            "✓ Loaded: session_001_0020.txt\n",
            "✓ Loaded: session_001_0021.txt\n",
            "✓ Loaded: session_001_0022.txt\n",
            "✓ Loaded: session_001_0023.txt\n",
            "✓ Loaded: session_001_0024.txt\n",
            "✓ Loaded: session_001_0025.txt\n",
            "✓ Loaded: session_001_0026.txt\n",
            "✓ Loaded: session_001_0027.txt\n",
            "✓ Loaded: session_001_0028.txt\n",
            "✓ Loaded: session_001_0029.txt\n",
            "✓ Loaded: session_001_0030.txt\n",
            "✓ Loaded: session_001_0031.txt\n",
            "✓ Loaded: session_001_0032.txt\n",
            "✓ Loaded: session_001_0033.txt\n",
            "✓ Loaded: session_001_0034.txt\n",
            "✓ Loaded: session_001_0035.txt\n",
            "✓ Loaded: session_001_0036.txt\n",
            "✓ Loaded: session_001_0037.txt\n",
            "✓ Loaded: session_001_0038.txt\n",
            "✓ Loaded: session_001_0039.txt\n",
            "✓ Loaded: session_001_0040.txt\n",
            "✓ Loaded: session_001_0041.txt\n",
            "✓ Loaded: session_001_0042.txt\n",
            "✓ Loaded: session_001_0043.txt\n",
            "✓ Loaded: session_001_0044.txt\n",
            "✓ Loaded: session_001_0045.txt\n",
            "✓ Loaded: session_001_0046.txt\n",
            "✓ Loaded: session_001_0047.txt\n",
            "✓ Loaded: session_001_0048.txt\n",
            "✓ Loaded: session_001_0049.txt\n",
            "✓ Loaded: session_001_0050.txt\n",
            "✓ Loaded: session_001_0051.txt\n",
            "✓ Loaded: session_001_0052.txt\n",
            "✓ Loaded: session_001_0053.txt\n",
            "✓ Loaded: session_001_0054.txt\n",
            "✓ Loaded: session_001_0055.txt\n",
            "✓ Loaded: session_001_0056.txt\n",
            "✓ Loaded: session_001_0057.txt\n",
            "✓ Loaded: session_001_0058.txt\n",
            "✓ Loaded: session_001_0059.txt\n",
            "✓ Loaded: session_001_0060.txt\n",
            "✓ Loaded: session_001_0061.txt\n",
            "✓ Loaded: session_001_0062.txt\n",
            "✓ Loaded: session_001_0063.txt\n",
            "✓ Loaded: session_001_0064.txt\n",
            "✓ Loaded: session_001_0065.txt\n",
            "✓ Loaded: session_001_0066.txt\n",
            "✓ Loaded: session_001_0067.txt\n",
            "\n",
            "Total segments loaded: 68\n",
            "\n",
            "Loading transcript from ODT file...\n",
            "====================================================================================================\n",
            "✓ Successfully loaded ODT file\n",
            "  Total characters: 6076\n",
            "  Total lines: 27\n",
            "\n",
            "Starting matching process...\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "SEGMENT MATCHING RESULTS\n",
            "====================================================================================================\n",
            "\n",
            "Summary: 7/68 segments matched successfully\n",
            "Match rate: 10.3%\n",
            "\n",
            "\n",
            "Segment: session_001_0000.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.704\n",
            "Matched Line Index: 0\n",
            "Segment Text: नमस्कार बोई प्रुद अंखोब्रों\n",
            "Matched Line: नमस्कार पळोवया प्रुडंट खबरो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0001.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.67\n",
            "Matched Line Index: 1\n",
            "Segment Text: तेंकर वाल्यक्ना दर्बान, गवाई भर्चा वाटर तेंकर आचा ट्रास्पोट धिपार्ट्मन करतोला सिझ्टी अडिट, उद्का पर ...\n",
            "Matched Line: टॅंकरवाल्यांक ना धरबांद गोंयभरच्या वॉटर टॅंकरांचें ट्रान्स्पोर्ट डिपोर्टमेन्ट करतलो सेफ्टीऑडिट उदका ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0002.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.729\n",
            "Matched Line Index: 2\n",
            "Segment Text: कर सा बन्दूर आप्रकल बाख केंद्राजी पर वान्गिना, प्रोज्यक्त जालम ते ती फुट, माधे बचा अभियनान करनाटकाक ...\n",
            "Matched Line: कळसाभंडुरा प्रकल्पाक केंद्राची परवानगी ना प्रॉजॅक्ट जाला म्हण्टा ती फट म्हादय बचाव अभियानान कर्नाटका...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0003.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.717\n",
            "Matched Line Index: 3\n",
            "Segment Text: अक्तिविस्टान सो कोंगरे सिक्ते को राई बन्रा पर्रे करान काईईच कामा के लिना लाई उद्काचो कद्कती गिरिशान ...\n",
            "Matched Line: एक्टिव्हिस्टांचो कॉंग्रेसीक तेंको रायबंदरा पर्रीकारान कांयच कामां केल्लीं ना लायटउदकाच्यो कटकटी गिरी...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0004.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.55\n",
            "Matched Line Index: 4\n",
            "Segment Text: आईरीशाग जी तो मार्टा मुन दिल ले दमके चके शींज विष्वाजी तान है को ताक बल लग देड लाख लिगल एड फुंड\n",
            "Matched Line: आयरीशाक जीतो मारता म्हूण दिल्ले धमकेचे केशींत विश्वजीतान हायकोर्टांत भल्लो देड लाख लीगल एड फंड चार्ज...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0005.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.394\n",
            "Segment Text: चार्षिट्रद, केस बंद, दा वर सा उपरानत आईरीश मन ता हरकोत ना\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0006.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.165\n",
            "Segment Text: ॐ ॐ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ौ ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0007.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.66\n",
            "Matched Line Index: 7\n",
            "Segment Text: अनी भार्दान फुट्साला के तले बारे दिस आप सी अनी आप आप में तैर करता फुट्साल ड़वलाप मेंटाजा अराख़वो\n",
            "Matched Line: भारतांत फुटसालाक येतले बरे दिस ए.एफ.सी आनी ए.आय.एफ.एफ मेळून तयार करता फुटसाल डॅव्हलापमॅन्टाचो आराखडो...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0008.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.221\n",
            "Segment Text: अगा दे सुवा दे रूद्का पूर वन केली काई तेंकर वालें थुईजल अखाची सुई गेव पाची अस्था\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0009.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.29\n",
            "Segment Text: अनी दिपार्ट्मेंटाख रिपोड द्वाचास्था\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0010.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.357\n",
            "Segment Text: मद तेंकर वाल्यांग द्वर्बान ना\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0011.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.247\n",
            "Segment Text: मुन प्रुदन्टान अपाची तेंकर वाटर सप्लाय अख्स्पोजिकल लोग\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0012.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.095\n",
            "Segment Text: आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आप आ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0013.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.261\n",
            "Segment Text: कर्सा बन्दूरा प्रकल्बाचकाम करनाटकान केन्राची पर्वान्गी नास्तन सुरूके लाही गजाल मादेई बचाव अवियानान स...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0014.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.358\n",
            "Segment Text: कर्सा बन्दूरा प्रकल्पाचकाम जोवर-जोवर पूराए जाये ताये ताये ला, मुन करनाटकान कोर्टाक संगिलने.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0015.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.269\n",
            "Segment Text: माद काम जल लेना, अनी ते नेटान चलना, मुन अभियनान कोर्टा क्लीर के लें\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0016.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.36\n",
            "Segment Text: करनाटगान असा वर्ज के ला, अनी केन रान्दां का परवान्गी दिल्या जल यार दोगानी ती कोर्टाक अपी देवीटा चेर ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0017.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.231\n",
            "Segment Text: राएबन्रा जा अक्टिविस्ट कोंगरेशी भातलें\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0018.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.233\n",
            "Segment Text: मुख्यमंद्री पुर्रे करान राएबन्रा काईच सकाम के लिना\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0019.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.267\n",
            "Segment Text: राएबन्रा लोकांक लाएट उद्काजो कद्कती\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0020.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.32\n",
            "Segment Text: हिन्दूक मोसुन्दी सरकिना\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0021.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.275\n",
            "Segment Text: अपले प्रचाराक लगान सो भरो प्रुदिसाद मेंता\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0022.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.286\n",
            "Segment Text: मुन खोंग्रेश़ उमेदवार गीरीष शोड़ान कार मेड्या गरे नुले लो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0023.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.274\n",
            "Segment Text: अक्टिविस्ट आयरीष गीरीष अनी रोयाक तेको दिक लेएर कर पा\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0024.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.241\n",
            "Segment Text: खोंग्रेश अजात मेड्या मुखार पालो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0025.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.23\n",
            "Segment Text: बलाएकी मुन्त्री विशुजित रानें अक्टिविस्ट अएदिश रोट्रिक साग जितो मरबाची द्हुमकि दिल्ली मंत्त्तिक केस ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0026.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.207\n",
            "Segment Text: विश्वेजिदाग फन्द दिपोजिट ही क्या लोग?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0027.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.277\n",
            "Segment Text: शानाचा मेनेज्मेंदानी रेटार स्तापाचा पेंषना सो भिगो भिग सेटलमें को रुब जाए.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0028.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.241\n",
            "Segment Text: नाजालर काई दियान तंक दंदमुन शिक्षन संचालक गजानन भधो लेला.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0029.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.294\n",
            "Segment Text: अदमीशन अवर वर्गेंचा आदार काड को रुन गयात.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0030.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.258\n",
            "Segment Text: तश्या सर्कूलरी हे पहिलिं कहें खात्यां दाल्ला\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0031.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.24\n",
            "Segment Text: इलेक्षाना जो कोड सुरुजा। जब हैं लेक्रूट में जाला\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0032.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.275\n",
            "Segment Text: तान्चोच अपाइन में अडरी कहें रिजाल्टा उप्रान दिव पाजो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0033.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.313\n",
            "Segment Text: कोंबार रिंग रोडावेले पन्द्रा फामिलिक चुथि उप्रान्द हलोचे\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0034.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.228\n",
            "Segment Text: तान्चा हाँजिंबोडाद पुनर्वासन जाँजाय मुन तिस्पी मुन्द्री भीजयो लेला\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0035.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.25\n",
            "Segment Text: तो इजा पामिलिची पातोदेचा आम्दरा सुइत मद्गाँ कलेक्टरी तान्बूसकजाली\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0036.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.232\n",
            "Segment Text: रस्तेच काम जाूंजाय, मुन पंद्रषी भितर तांक रालोपाज़ कोर्डाना देष्दिला\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0037.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.268\n",
            "Segment Text: मच्ष्वो तानी पामिलिजार करून, काई बदलां कहतिर कोर्डाक विनूनी करपाज़ भिजायाज़ विचार\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0038.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.186\n",
            "Segment Text: बार्देश पेटने चा गवा कुड मुनकार संगर्ष समी तिन पंजे न नवे कुड मुनकार दूर्स्ती विदेः जाएपाचा प्लान क्...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0039.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.264\n",
            "Segment Text: आशिल्ले साथाद जानुच, करन्जारे निदर्षान करता मुन साँन महापालिका पूलिस कलेक्टरेटा जे परमिशन जेतले,\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0040.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.205\n",
            "Segment Text: इलेक्ष्यन कमिशनाक साँलेत ना, निदर्षान केलिए पूनीजे.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0041.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.266\n",
            "Segment Text: भीज़े भी वालाई जा कंपलेनिजर पूलिसानी तंकः भीडकी को लाई ली\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0042.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.263\n",
            "Segment Text: गवायात साइबर क्राम वाट्ता, वेप्साइटिचर सेक्स तूरीजा माचा प्रमोशन जल्ला, आमड गवा वूमेंस प्रमान तिडग उ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0043.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.253\n",
            "Segment Text: आश्ले प्रकार बन्द जाूँँजाय मन गोवा मुमन्स पोराम माक्ता\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0044.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.196\n",
            "Segment Text: फोरामान मद्गामा जागरोती रहे लिए का लि\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0045.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.258\n",
            "Segment Text: ता बगे के शाव स्म्रती हाईर से गन्री चा अख्रावेचा बुर्गयाग हाईर से गन्री चा तीन मुजली बिल्निंगे चा ते...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0046.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.23\n",
            "Segment Text: शिक्ष्यान संचालकान एद्याय कर चान रिपोड मागला\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0047.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.25\n",
            "Segment Text: रिपोड आएलो काई हाईर सकंडर इच प्रिन्सिपलाक नोटी सुत्ली\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0048.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.276\n",
            "Segment Text: मुन शिक्ष्यान संचालक गजानन भध रूड़ान ता करे नूले लो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0049.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.114\n",
            "Segment Text: CITIZON JOURNALIST ACHE VIDEO VELLY AND PRUDENT ANHOV PRAKARA EXPOSE KALLO\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0050.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.198\n",
            "Segment Text: All India Football Federation, अत फुट्साल ड़ूलाप करपाच्याल लक्षा गालतले\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0051.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.295\n",
            "Segment Text: यका मात आता ता दंग आश्यन फुट्बाल कन्फेडरिष्यन आदार करता\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0052.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.25\n",
            "Segment Text: इन्फ्रस्ट्रक्चर कश्या आसकाय पनवक सद्या AFC अनी AI-FFI च्सिस्टम अंडर देश वर वावतले\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0053.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.375\n",
            "Segment Text: गवायात ब्रस्टारा इंस्पेक्षन जा लें\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0054.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.375\n",
            "Segment Text: गवायात ब्रस्टारा इंस्पेक्षन जा लें\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0055.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.268\n",
            "Segment Text: मुर्गाँ सो भीजेवी सो आम्दार मिलिन नाएकान आँववन पुराए के लिए\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0056.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.332\n",
            "Segment Text: वास्को ते पर ये अथाविस क्लोमिटर तो साई भाबाचे दर्षनाक चलत गलो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0057.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.286\n",
            "Segment Text: वर्वर दाए कर्ये करतें जी वारी\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0058.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.254\n",
            "Segment Text: मिलिंद आं सगल्यांक भूर्वार ग्यून, फाथोदेर पाचाग वास्को सावन भूर्येची वाद डल्ली\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0059.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.246\n",
            "Segment Text: इलेक्षन अगर देन मिलिंद जिकल्यार भूर्ये वारीची साईईभाबाक ताचा खार्यागर देन अंवोन किल्ली\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0060.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.302\n",
            "Segment Text: मिलिन्द एक्षे त्रेजा इस्मतानी जिकिल्लों\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0061.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.252\n",
            "Segment Text: स्वतन्द्र दाय दिस तेक्लो, आएक पाक्यना, उलोक करना, अशे दिव्यांग लोका खाति राता सायन लेंवेजिन राश्टर ग...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0062.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.251\n",
            "Segment Text: ब्रदर हुड न्यू दिल्ली अनी दिजआबिलिटी राट्स असुस्विश्यान अप गोवा\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0063.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.221\n",
            "Segment Text: गला अकादमीद बरोवर गेवन सायन लेंवेजिन्त लों नेशनल अंठम विडियो लाँईज्गलों\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0064.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.242\n",
            "Segment Text: आमिता बच्चना गेवन हे खास रष्टर गीट के लाँं\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0065.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.222\n",
            "Segment Text: इतले ने मुले दी नागत सवता\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0066.txt\n",
            "Status: MATCHED\n",
            "Similarity Score: 0.514\n",
            "Matched Line Index: 26\n",
            "Segment Text: यो खुब्रोदुंको प्रुडन वाज्चाप प्रिटर, फिस्बूक निप प्रुडन वेप्साइ टीचरी लाईव मेटात\n",
            "Matched Line: हें बुलॅटिन हांगांच सोंपता ह्यो खबरो तुमकां प्रुडंट वॉट्सॅप ट्विटर फेसबूक आनी प्रुडंट व्हॅबसायटीचेर ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Segment: session_001_0067.txt\n",
            "Status: NO_MATCH\n",
            "Similarity Score: 0.222\n",
            "Segment Text: प्रुदन्ताक फलुगरी त्रावाद, प्रुदन्ताक प्रुदन्ताक फलुगरी त्रावाद, प्रुदन्ताक प्रुदन्ताक फलुगरी त्रावा...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "✓ Results saved to 'matching_results.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directory where files will be created\n",
        "output_dir = \"/content/drive/MyDrive/dataset/matched_segments\"\n",
        "\n",
        "# Create folder if not exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Mapping of file_name → matched_text\n",
        "segments = {\n",
        "    \"session_001_0000.txt\": \"नमस्कार पळोवया प्रुडंट खबरो\",\n",
        "    \"session_001_0001.txt\": \"टॅंकरवाल्यांक ना धरबांद गोंयभरच्या वॉटर टॅंकरांचें ट्रान्स्पोर्ट डिपोर्टमेन्ट करतलो सेफ्टीऑडिट उदका पुरवणे उपरांत लोकांकडच्यान सय घेवपाची सिस्टम प्रुडंटाचे ऑपा वॉट टॅंकर एक्स्पोजेचो इम्पॅक्ट.\",\n",
        "    \"session_001_0002.txt\": \"कळसाभंडुरा प्रकल्पाक केंद्राची परवानगी ना प्रॉजॅक्ट जाला म्हण्टा ती फट म्हादय बचाव अभियानान कर्नाटकाक सुप्रिम कोर्टांत केलें एक्स्पोज कोर्टान मागलें केंद्र कर्नाटकाकडच्यान एफिडॅव्हिट.\",\n",
        "    \"session_001_0003.txt\": \"एक्टिव्हिस्टांचो कॉंग्रेसीक तेंको रायबंदरा पर्रीकारान कांयच कामां केल्लीं ना लायटउदकाच्यो कटकटी गिरीशाचो रायबंदरा प्रचार पर्रीकार खंय सगळ्यात भ्रष्ट मुख्यमंत्री\",\n",
        "    \"session_001_0004.txt\": \"आयरीशाक जीतो मारता म्हूण दिल्ले धमकेचे केशींत विश्वजीतान हायकोर्टांत भल्लो देड लाख लीगल एड फंड\",\n",
        "    \"session_001_0005.txt\": \"चार्जशीट रद्द केस बंद 10 वर्सां उपरांत आयरिश म्हण्टा हरकत ना.\",\n",
        "    \"session_001_0007.txt\": \"आनी भारतांत फुटसालाक येतले बरे दिस ए.एफ.सी आनी ए.आय.एफ.एफ मेळून तयार करता फुटसाल डॅव्हलापमॅन्टाचो आराखडो.\",\n",
        "    \"session_001_0008.txt\": \"एखाद्या सुवातीर उदका पुरवण केली काय टॅंकरवाल्यान थंयचे लोकांची सय घेवपाची आसता\",\n",
        "    \"session_001_0009.txt\": \"आनी डिपार्टमेंटाक ट्रिपींचो रिपोर्ट दिवपाचो आसता.\",\n",
        "    \"session_001_0010.txt\": \"मात टॅंकरवाल्यांकधरबांद ना म्हूण\",\n",
        "    \"session_001_0011.txt\": \"प्रुडंटान ओपाची टॅंकर वॉटर सप्लाय एक्स्पोजे केल्लो.\",\n",
        "    \"session_001_0013.txt\": \"कळसाभंडुरा प्रकल्पाचे काम कर्नाटकान केंद्राची परवानगी नासतना सुरु केलां ही गजाल म्हादय बचाव अभियानान सुप्रिम कोर्टांत एक्पोज केल्या.\",\n",
        "    \"session_001_0014.txt\": \"कळसा बंडुरा प्रकल्पाचे काम जवळ जवळ पुराय जायत आयलां म्हूण कर्नाटकान कोर्टाक सांगलां.\",\n",
        "    \"session_001_0015.txt\": \"मात काम जाल्लें ना आनी तें नेटान चल्लां म्हूण अभियानान कोर्टाक क्लियर केलें.\",\n",
        "    \"session_001_0016.txt\": \"कर्नाटकान असो अर्ज केला आनी केंद्रान तांकां परवानगी दिल्या जाल्यार दोगांनीय ती कोर्टाक एफिडॅव्हिटाचेर दाखोवची पडटली.\",\n",
        "    \"session_001_0017.txt\": \"रायबंदराचे एक्टिव्हिस्ट कॉंग्रेसीफाटल्यान.\",\n",
        "    \"session_001_0018.txt\": \"मुख्यमंत्री पर्रीकारान रायबंदरा कांयच कामां केल्लीं ना.\",\n",
        "    \"session_001_0019.txt\": \"रायबंदरा लोकांक लायटउदकाच्यो कटकटी.\",\n",
        "    \"session_001_0020.txt\": \"हिंदुक मसुंडी सारकी ना.\",\n",
        "    \"session_001_0021.txt\": \"आपल्या प्रचाराक लोकांचो बरो प्रदिसात मेळटा\",\n",
        "    \"session_001_0022.txt\": \"म्हूण काँग्रेसीचो उमेदवार गिरीश चोडणकार मिडियाकडेन उलयलो.\",\n",
        "    \"session_001_0024.txt\": \"काँग्रेस हावजांत मिडियामुखार पावलो.\",\n",
        "    \"session_001_0026.txt\": \"विश्वजीता फंड डिपॉजिटूय केलो.\",\n",
        "    \"session_001_0027.txt\": \"शाळांच्या मॅनेजमॅन्टांनी रिटायर स्टाफाच्या पॅन्शनचो बेगोबेग सॅटलमेंट करुंक जाय.\",\n",
        "    \"session_001_0028.txt\": \"ना जाल्यार काद्यान तांका दंड म्हूण शिक्षण संचालक गजाजन भट उलयला.\",\n",
        "    \"session_001_0029.txt\": \"एडमीशनावेळार भुरग्यांचे आर्धारकार्ड करुन घेयात.\",\n",
        "    \"session_001_0030.txt\": \"तशें सर्कुलरुय हे पयली खंय खात्यान काडलां.\",\n",
        "    \"session_001_0031.txt\": \"इलेक्शनाचोकोड सुरु जावचें पयली रिक्रुटमेंट जाला\",\n",
        "    \"session_001_0032.txt\": \"तांच्योच अपॉयंटमेंट ऑर्डरी खंय रिझल्टा उपरांत दिवपाच्यो.\",\n",
        "    \"session_001_0033.txt\": \"कोंबा रिंग रोडावेल्या 15 फामिलींक चवथी उपरांत हालोवचे\",\n",
        "    \"session_001_0034.txt\": \"तांचें हावसींग बोर्डांत पुनर्वसन जावंक जाय म्हूण टीसीपी मंत्री विजय उलयला.\",\n",
        "    \"session_001_0035.txt\": \"थंयचे फामीलींची फातोड्डेच्या आमदारासयत मडगांव कलॅक्टरेटांत बसका जाली.\",\n",
        "    \"session_001_0036.txt\": \"रस्त्याचे काम जावंक जाय म्हूण पंदरशीभीतर तांकां हालोवपाचो कोर्टान आदेश दिला.\",\n",
        "    \"session_001_0037.txt\": \"मात चवथ आनी फामिलींचो विचार करून कांय बदलांखातीर कोर्टाक विनवणी करपाचो विजयाचो विचार.\",\n",
        "    \"session_001_0039.txt\": \"आशिल्ले सात आठ जाणूच. करंजाळे निदर्शनां करता म्हूण सांगून म्हापालिका पोलीस कलॅक्टराचें परमिशन घेतलें,\",\n",
        "    \"session_001_0040.txt\": \"इलॅक्शन कमिशनाक सांगलेंच ना. निदर्शनां केलीं पणजे.\",\n",
        "    \"session_001_0041.txt\": \"बीजेपीवाल्यांचे कंप्लेनीचेर पुलिसांनी तांकां बिडकी कवळूंक लायली.\",\n",
        "    \"session_001_0042.txt\": \"गोंयात सायबर क्रायम वाडाटा. वेबसायटींचेर सॅक्स टुरिजमाचें प्रोमोशन चल्लां म्हूण गोवा वुमन्स फोरमान तिडक उक्तायली. शिंदळकेखातीर चलयां आनी बायलांची पुरवण करपी वॅबसाटीं चलतात.\",\n",
        "    \"session_001_0043.txt\": \"असले प्रकार बंद जावंक जाय म्हूण गोवा वूमन्स फोरम मागता.\",\n",
        "    \"session_001_0044.txt\": \"फोरमान मडगावां जागृती रॅली काडली.\",\n",
        "    \"session_001_0045.txt\": \"दाबोळे केशव स्मृतीच्या अकरावेच्या भुरग्यांक हायर सेकंडरीचे तीन मजली बिल्डिंगेचे तेर्रासाचेर चडोवन टायल्स नितळ करूंक लायिल्ले प्रकरणाची शिक्षण खात्यान दखल घेतल्या.\",\n",
        "    \"session_001_0046.txt\": \"शिक्षण संचालकान एडीईआय कडच्यान रिपोर्ट मागला.\",\n",
        "    \"session_001_0047.txt\": \"रिपोर्ट आयलो काय हायर सेकंडरीच्या प्रिन्सिपलाक नोटीस वतली\",\n",
        "    \"session_001_0048.txt\": \"म्हूण शिक्षण संचालक गजानन भट प्रुडंटाकडेन उलयलो.\",\n",
        "    \"session_001_0049.txt\": \"सिटिझन जर्नलिस्टाचे व्हिडिओवेल्यान प्रुडंटान हो प्रकार एक्स्पोज केल्लो.\",\n",
        "    \"session_001_0050.txt\": \"ऑल इंडिया फुटबॉल फॅडरेशन आतां फुटसाल डेव्हलाप करपाचेर लक्ष घालतलें.\",\n",
        "    \"session_001_0051.txt\": \"ह्या कामांत आतां तांकां एशीयन फुटबॉल कन्फॅडरेशन आदार करतलें.\",\n",
        "    \"session_001_0052.txt\": \"इन्फ्रास्ट्रक्चर कशें आसा कांय पळोवंक सद्या एएफसी आनी एआयएफएफाचें शिश्टमंडळ देशभर भोंवतले.\",\n",
        "    \"session_001_0053.txt\": \"गोंयात ब्रेस्तारा इन्स्पेक्शन जालें.\",\n",
        "    \"session_001_0055.txt\": \"मुरगांवचो बीजेपी आमदार मिलींद नायकान आंगवण पुराय केली\",\n",
        "    \"session_001_0056.txt\": \"वास्को ते बोरये 28 किलोमिटर तो साई बाबाचें दर्शनाक चलत गेलो.\",\n",
        "    \"session_001_0057.txt\": \"बरोबर धायेक कार्यकर्त्यांचीय वारी\",\n",
        "    \"session_001_0058.txt\": \"मिलींदान सगल्यांक बरोबर घेवन फांतोडेर पाचंक वास्कोसावन बोरयेची वाट धल्ली.\",\n",
        "    \"session_001_0059.txt\": \"इलॅक्शनाकडेन मिलिंद जिकल्यार बोरये वारीची साईबाबाक ताचे कार्य़कर्त्यान आंगवण केल्ली.\",\n",
        "    \"session_001_0060.txt\": \"मिलिंद 143 मतांनी जिकलो\",\n",
        "    \"session_001_0061.txt\": \"स्वतंत्रताय दीस तेंकलो आयकपाक येना उलोवंक कळना अश्या दिव्यांग लोकांखातीर आता सायन लॅग्वेजीन राष्ट्रगीत लाँच जालां.\",\n",
        "    \"session_001_0062.txt\": \"ब्रदरहुड न्यु दिल्ली आनी डिसॅबीलीटी रायट्स असोसिएशन ऑफ गोवा\",\n",
        "    \"session_001_0063.txt\": \"कला अकादमीक बरोबर घेवन सायन लॅग्वीजींतलो नॅशनल एन्थम व्हिडियो लाँच केलो.\",\n",
        "    \"session_001_0064.txt\": \"अमिताभ बच्चनाक घेवन हें खास राष्ट्रगीत केलां.\",\n",
        "    \"session_001_0065.txt\": \"हें बुलॅटिन हांगांच सोंपता\",\n",
        "    \"session_001_0066.txt\": \"ह्यो खबरो तुमकां प्रुडंट वॉट्सॅप ट्विटर फेसबूक आनी प्रुडंट व्हॅबसायटीचेर लायव्ह मेळटात\",\n",
        "}\n",
        "\n",
        "for file_name, text in segments.items():\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "print(f\"✅ Created {len(segments)} files in folder: {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm7LpNu0pgTF",
        "outputId": "c5a059c6-b192-49eb-d05a-35d0bbebaf47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created 61 files in folder: /content/drive/MyDrive/dataset/matched_segments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "raw = r'''\n",
        "file_name,matched_text\n",
        "--- session_001_0000.txt ---,\"नमस्कार पळोवया प्रुडंट खबरो\"\n",
        "--- session_001_0001.txt ---,\"टॅंकरवाल्यांक ना धरबांद गोंयभरच्या वॉटर टॅंकरांचें ट्रान्स्पोर्ट डिपोर्टमेन्ट करतलो सेफ्टीऑडिट उदका पुरवणे उपरांत लोकांकडच्यान सय घेवपाची सिस्टम प्रुडंटाचे ऑपा वॉट टॅंकर एक्स्पोजेचो इम्पॅक्ट.\"\n",
        "--- session_001_0002.txt ---,\"कळसाभंडुरा प्रकल्पाक केंद्राची परवानगी ना प्रॉजॅक्ट जाला म्हण्टा ती फट म्हादय बचाव अभियानान कर्नाटकाक सुप्रिम कोर्टांत केलें एक्स्पोज कोर्टान मागलें केंद्र कर्नाटकाकडच्यान एफिडॅव्हिट.\"\n",
        "--- session_001_0003.txt ---,\"एक्टिव्हिस्टांचो कॉंग्रेसीक तेंको रायबंदरा पर्रीकारान कांयच कामां केल्लीं ना लायटउदकाच्यो कटकटी गिरीशाचो रायबंदरा प्रचार पर्रीकार खंय सगळ्यात भ्रष्ट मुख्यमंत्री\"\n",
        "--- session_001_0004.txt ---,\"आयरीशाक जीतो मारता म्हूण दिल्ले धमकेचे केशींत विश्वजीतान हायकोर्टांत भल्लो देड लाख लीगल एड फंड\"\n",
        "--- session_001_0005.txt ---,\"चार्जशीट रद्द केस बंद 10 वर्सां उपरांत आयरिश म्हण्टा हरकत ना.\"\n",
        "--- session_001_0007.txt ---,\"आनी भारतांत फुटसालाक येतले बरे दिस ए.एफ.सी आनी ए.आय.एफ.एफ मेळून तयार करता फुटसाल डॅव्हलापमॅन्टाचो आराखडो.\"\n",
        "--- session_001_0008.txt ---,\"एखाद्या सुवातीर उदका पुरवण केली काय टॅंकरवाल्यान थंयचे लोकांची सय घेवपाची आसता\"\n",
        "--- session_001_0009.txt ---,\"आनी डिपार्टमेंटाक ट्रिपींचो रिपोर्ट दिवपाचो आसता.\"\n",
        "--- session_001_0010.txt ---,\"मात टॅंकरवाल्यांकधरबांद ना म्हूण\"\n",
        "--- session_001_0011.txt ---,\"प्रुडंटान ओपाची टॅंकर वॉटर सप्लाय एक्स्पोजे केल्लो.\"\n",
        "--- session_001_0013.txt ---,\"कळसाभंडुरा प्रकल्पाचे काम कर्नाटकान केंद्राची परवानगी नासतना सुरु केलां ही गजाल म्हादय बचाव अभियानान सुप्रिम कोर्टांत एक्पोज केल्या.\"\n",
        "--- session_001_0014.txt ---,\"कळसा बंडुरा प्रकल्पाचे काम जवळ जवळ पुराय जायत आयलां म्हूण कर्नाटकान कोर्टाक सांगलां.\"\n",
        "--- session_001_0015.txt ---,\"मात काम जाल्लें ना आनी तें नेटान चल्लां म्हूण अभियानान कोर्टाक क्लियर केलें.\"\n",
        "--- session_001_0016.txt ---,\"कर्नाटकान असो अर्ज केला आनी केंद्रान तांकां परवानगी दिल्या जाल्यार दोगांनीय ती कोर्टाक एफिडॅव्हिटाचेर दाखोवची पडटली.\"\n",
        "--- session_001_0017.txt ---,\"रायबंदराचे एक्टिव्हिस्ट कॉंग्रेसीफाटल्यान.\"\n",
        "--- session_001_0018.txt ---,\"मुख्यमंत्री पर्रीकारान रायबंदरा कांयच कामां केल्लीं ना.\"\n",
        "--- session_001_0019.txt ---,\"रायबंदरा लोकांक लायटउदकाच्यो कटकटी.\"\n",
        "--- session_001_0020.txt ---,\"हिंदुक मसुंडी सारकी ना.\"\n",
        "--- session_001_0021.txt ---,\"आपल्या प्रचाराक लोकांचो बरो प्रदिसात मेळटा\"\n",
        "--- session_001_0022.txt ---,\"म्हूण काँग्रेसीचो उमेदवार गिरीश चोडणकार मिडियाकडेन उलयलो.\"\n",
        "--- session_001_0024.txt ---,\"काँग्रेस हावजांत मिडियामुखार पावलो.\"\n",
        "--- session_001_0026.txt ---,\"विश्वजीता फंड डिपॉजिटूय केलो.\"\n",
        "--- session_001_0027.txt ---,\"शाळांच्या मॅनेजमॅन्टांनी रिटायर स्टाफाच्या पॅन्शनचो बेगोबेग सॅटलमेंट करुंक जाय.\"\n",
        "--- session_001_0028.txt ---,\"ना जाल्यार काद्यान तांका दंड म्हूण शिक्षण संचालक गजाजन भट उलयला.\"\n",
        "--- session_001_0029.txt ---,\"एडमीशनावेळार भुरग्यांचे आर्धारकार्ड करुन घेयात.\"\n",
        "--- session_001_0030.txt ---,\"तशें सर्कुलरुय हे पयली खंय खात्यान काडलां.\"\n",
        "--- session_001_0031.txt ---,\"इलेक्शनाचोकोड सुरु जावचें पयली रिक्रुटमेंट जाला\"\n",
        "--- session_001_0032.txt ---,\"तांच्योच अपॉयंटमेंट ऑर्डरी खंय रिझल्टा उपरांत दिवपाच्यो.\"\n",
        "--- session_001_0033.txt ---,\"कोंबा रिंग रोडावेल्या 15 फामिलींक चवथी उपरांत हालोवचे\"\n",
        "--- session_001_0034.txt ---,\"तांचें हावसींग बोर्डांत पुनर्वसन जावंक जाय म्हूण टीसीपी मंत्री विजय उलयला.\"\n",
        "--- session_001_0035.txt ---,\"थंयचे फामीलींची फातोड्डेच्या आमदारासयत मडगांव कलॅक्टरेटांत बसका जाली.\"\n",
        "--- session_001_0036.txt ---,\"रस्त्याचे काम जावंक जाय म्हूण पंदरशीभीतर तांकां हालोवपाचो कोर्टान आदेश दिला.\"\n",
        "--- session_001_0037.txt ---,\"मात चवथ आनी फामिलींचो विचार करून कांय बदलांखातीर कोर्टाक विनवणी करपाचो विजयाचो विचार.\"\n",
        "--- session_001_0039.txt ---,\"आशिल्ले सात आठ जाणूच. करंजाळे निदर्शनां करता म्हूण सांगून म्हापालिका पोलीस कलॅक्टराचें परमिशन घेतलें,\"\n",
        "--- session_001_0040.txt ---,\"इलॅक्शन कमिशनाक सांगलेंच ना. निदर्शनां केलीं पणजे.\"\n",
        "--- session_001_0041.txt ---,\"बीजेपीवाल्यांचे कंप्लेनीचेर पुलिसांनी तांकां बिडकी कवळूंक लायली.\"\n",
        "--- session_001_0042.txt ---,\"गोंयात सायबर क्रायम वाडाटा. वेबसायटींचेर सॅक्स टुरिजमाचें प्रोमोशन चल्लां म्हूण गोवा वुमन्स फोरमान तिडक उक्तायली. शिंदळकेखातीर चलयां आनी बायलांची पुरवण करपी वॅबसाटीं चलतात.\"\n",
        "--- session_001_0043.txt ---,\"असले प्रकार बंद जावंक जाय म्हूण गोवा वूमन्स फोरम मागता.\"\n",
        "--- session_001_0044.txt ---,\"फोरमान मडगावां जागृती रॅली काडली.\"\n",
        "--- session_001_0045.txt ---,\"दाबोळे केशव स्मृतीच्या अकरावेच्या भुरग्यांक हायर सेकंडरीचे तीन मजली बिल्डिंगेचे तेर्रासाचेर चडोवन टायल्स नितळ करूंक लायिल्ले प्रकरणाची शिक्षण खात्यान दखल घेतल्या.\"\n",
        "--- session_001_0046.txt ---,\"शिक्षण संचालकान एडीईआय कडच्यान रिपोर्ट मागला.\"\n",
        "--- session_001_0047.txt ---,\"रिपोर्ट आयलो काय हायर सेकंडरीच्या प्रिन्सिपलाक नोटीस वतली\"\n",
        "--- session_001_0048.txt ---,\"म्हूण शिक्षण संचालक गजानन भट प्रुडंटाकडेन उलयलो.\"\n",
        "--- session_001_0049.txt ---,\"सिटिझन जर्नलिस्टाचे व्हिडिओवेल्यान प्रुडंटान हो प्रकार एक्स्पोज केल्लो.\"\n",
        "--- session_001_0050.txt ---,\"ऑल इंडिया फुटबॉल फॅडरेशन आतां फुटसाल डेव्हलाप करपाचेर लक्ष घालतलें.\"\n",
        "--- session_001_0051.txt ---,\"ह्या कामांत आतां तांकां एशीयन फुटबॉल कन्फॅडरेशन आदार करतलें.\"\n",
        "--- session_001_0052.txt ---,\"इन्फ्रास्ट्रक्चर कशें आसा कांय पळोवंक सद्या एएफसी आनी एआयएफएफाचें शिश्टमंडळ देशभर भोंवतले.\"\n",
        "--- session_001_0053.txt ---,\"गोंयात ब्रेस्तारा इन्स्पेक्शन जालें.\"\n",
        "--- session_001_0055.txt ---,\"मुरगांवचो बीजेपी आमदार मिलींद नायकान आंगवण पुराय केली\"\n",
        "--- session_001_0056.txt ---,\"वास्को ते बोरये 28 किलोमिटर तो साई बाबाचें दर्शनाक चलत गेलो.\"\n",
        "--- session_001_0057.txt ---,\"बरोबर धायेक कार्यकर्त्यांचीय वारी\"\n",
        "--- session_001_0058.txt ---,\"मिलींदान सगल्यांक बरोबर घेवन फांतोडेर पाचंक वास्कोसावन बोरयेची वाट धल्ली.\"\n",
        "--- session_001_0059.txt ---,\"इलॅक्शनाकडेन मिलिंद जिकल्यार बोरये वारीची साईबाबाक ताचे कार्य़कर्त्यान आंगवण केल्ली.\"\n",
        "--- session_001_0060.txt ---,\"मिलिंद 143 मतांनी जिकलो\"\n",
        "--- session_001_0061.txt ---,\"स्वतंत्रताय दीस तेंकलो आयकपाक येना उलोवंक कळना अश्या दिव्यांग लोकांखातीर आता सायन लॅग्वेजीन राष्ट्रगीत लाँच जालां.\"\n",
        "--- session_001_0062.txt ---,\"ब्रदरहुड न्यु दिल्ली आनी डिसॅबीलीटी रायट्स असोसिएशन ऑफ गोवा\"\n",
        "--- session_001_0063.txt ---,\"कला अकादमीक बरोबर घेवन सायन लॅग्वीजींतलो नॅशनल एन्थम व्हिडियो लाँच केलो.\"\n",
        "--- session_001_0064.txt ---,\"अमिताभ बच्चनाक घेवन हें खास राष्ट्रगीत केलां.\"\n",
        "--- session_001_0065.txt ---,\"हें बुलॅटिन हांगांच सोंपता\"\n",
        "--- session_001_0066.txt ---,\"ह्यो खबरो तुमकां प्रुडंट वॉट्सॅप ट्विटर फेसबूक आनी प्रुडंट व्हॅबसायटीचेर लायव्ह मेळटात\"\n",
        "'''\n",
        "\n",
        "pattern = r\"---\\s*(session_[\\d_]+\\.txt)\\s*---,\\\"(.+?)\\\"\"\n",
        "matches = re.findall(pattern, raw, flags=re.DOTALL)\n",
        "\n",
        "print(\"{\")\n",
        "for file, text in matches:\n",
        "    text = text.replace('\"', '\\\\\"')  # escape quotes\n",
        "    print(f'    \"{file}\": \"{text}\",')\n",
        "print(\"}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQGnOal3p6hU",
        "outputId": "9df5e2ca-671b-4a1c-a0d7-e09ff662cec7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"session_001_0000.txt\": \"नमस्कार पळोवया प्रुडंट खबरो\",\n",
            "    \"session_001_0001.txt\": \"टॅंकरवाल्यांक ना धरबांद गोंयभरच्या वॉटर टॅंकरांचें ट्रान्स्पोर्ट डिपोर्टमेन्ट करतलो सेफ्टीऑडिट उदका पुरवणे उपरांत लोकांकडच्यान सय घेवपाची सिस्टम प्रुडंटाचे ऑपा वॉट टॅंकर एक्स्पोजेचो इम्पॅक्ट.\",\n",
            "    \"session_001_0002.txt\": \"कळसाभंडुरा प्रकल्पाक केंद्राची परवानगी ना प्रॉजॅक्ट जाला म्हण्टा ती फट म्हादय बचाव अभियानान कर्नाटकाक सुप्रिम कोर्टांत केलें एक्स्पोज कोर्टान मागलें केंद्र कर्नाटकाकडच्यान एफिडॅव्हिट.\",\n",
            "    \"session_001_0003.txt\": \"एक्टिव्हिस्टांचो कॉंग्रेसीक तेंको रायबंदरा पर्रीकारान कांयच कामां केल्लीं ना लायटउदकाच्यो कटकटी गिरीशाचो रायबंदरा प्रचार पर्रीकार खंय सगळ्यात भ्रष्ट मुख्यमंत्री\",\n",
            "    \"session_001_0004.txt\": \"आयरीशाक जीतो मारता म्हूण दिल्ले धमकेचे केशींत विश्वजीतान हायकोर्टांत भल्लो देड लाख लीगल एड फंड\",\n",
            "    \"session_001_0005.txt\": \"चार्जशीट रद्द केस बंद 10 वर्सां उपरांत आयरिश म्हण्टा हरकत ना.\",\n",
            "    \"session_001_0007.txt\": \"आनी भारतांत फुटसालाक येतले बरे दिस ए.एफ.सी आनी ए.आय.एफ.एफ मेळून तयार करता फुटसाल डॅव्हलापमॅन्टाचो आराखडो.\",\n",
            "    \"session_001_0008.txt\": \"एखाद्या सुवातीर उदका पुरवण केली काय टॅंकरवाल्यान थंयचे लोकांची सय घेवपाची आसता\",\n",
            "    \"session_001_0009.txt\": \"आनी डिपार्टमेंटाक ट्रिपींचो रिपोर्ट दिवपाचो आसता.\",\n",
            "    \"session_001_0010.txt\": \"मात टॅंकरवाल्यांकधरबांद ना म्हूण\",\n",
            "    \"session_001_0011.txt\": \"प्रुडंटान ओपाची टॅंकर वॉटर सप्लाय एक्स्पोजे केल्लो.\",\n",
            "    \"session_001_0013.txt\": \"कळसाभंडुरा प्रकल्पाचे काम कर्नाटकान केंद्राची परवानगी नासतना सुरु केलां ही गजाल म्हादय बचाव अभियानान सुप्रिम कोर्टांत एक्पोज केल्या.\",\n",
            "    \"session_001_0014.txt\": \"कळसा बंडुरा प्रकल्पाचे काम जवळ जवळ पुराय जायत आयलां म्हूण कर्नाटकान कोर्टाक सांगलां.\",\n",
            "    \"session_001_0015.txt\": \"मात काम जाल्लें ना आनी तें नेटान चल्लां म्हूण अभियानान कोर्टाक क्लियर केलें.\",\n",
            "    \"session_001_0016.txt\": \"कर्नाटकान असो अर्ज केला आनी केंद्रान तांकां परवानगी दिल्या जाल्यार दोगांनीय ती कोर्टाक एफिडॅव्हिटाचेर दाखोवची पडटली.\",\n",
            "    \"session_001_0017.txt\": \"रायबंदराचे एक्टिव्हिस्ट कॉंग्रेसीफाटल्यान.\",\n",
            "    \"session_001_0018.txt\": \"मुख्यमंत्री पर्रीकारान रायबंदरा कांयच कामां केल्लीं ना.\",\n",
            "    \"session_001_0019.txt\": \"रायबंदरा लोकांक लायटउदकाच्यो कटकटी.\",\n",
            "    \"session_001_0020.txt\": \"हिंदुक मसुंडी सारकी ना.\",\n",
            "    \"session_001_0021.txt\": \"आपल्या प्रचाराक लोकांचो बरो प्रदिसात मेळटा\",\n",
            "    \"session_001_0022.txt\": \"म्हूण काँग्रेसीचो उमेदवार गिरीश चोडणकार मिडियाकडेन उलयलो.\",\n",
            "    \"session_001_0024.txt\": \"काँग्रेस हावजांत मिडियामुखार पावलो.\",\n",
            "    \"session_001_0026.txt\": \"विश्वजीता फंड डिपॉजिटूय केलो.\",\n",
            "    \"session_001_0027.txt\": \"शाळांच्या मॅनेजमॅन्टांनी रिटायर स्टाफाच्या पॅन्शनचो बेगोबेग सॅटलमेंट करुंक जाय.\",\n",
            "    \"session_001_0028.txt\": \"ना जाल्यार काद्यान तांका दंड म्हूण शिक्षण संचालक गजाजन भट उलयला.\",\n",
            "    \"session_001_0029.txt\": \"एडमीशनावेळार भुरग्यांचे आर्धारकार्ड करुन घेयात.\",\n",
            "    \"session_001_0030.txt\": \"तशें सर्कुलरुय हे पयली खंय खात्यान काडलां.\",\n",
            "    \"session_001_0031.txt\": \"इलेक्शनाचोकोड सुरु जावचें पयली रिक्रुटमेंट जाला\",\n",
            "    \"session_001_0032.txt\": \"तांच्योच अपॉयंटमेंट ऑर्डरी खंय रिझल्टा उपरांत दिवपाच्यो.\",\n",
            "    \"session_001_0033.txt\": \"कोंबा रिंग रोडावेल्या 15 फामिलींक चवथी उपरांत हालोवचे\",\n",
            "    \"session_001_0034.txt\": \"तांचें हावसींग बोर्डांत पुनर्वसन जावंक जाय म्हूण टीसीपी मंत्री विजय उलयला.\",\n",
            "    \"session_001_0035.txt\": \"थंयचे फामीलींची फातोड्डेच्या आमदारासयत मडगांव कलॅक्टरेटांत बसका जाली.\",\n",
            "    \"session_001_0036.txt\": \"रस्त्याचे काम जावंक जाय म्हूण पंदरशीभीतर तांकां हालोवपाचो कोर्टान आदेश दिला.\",\n",
            "    \"session_001_0037.txt\": \"मात चवथ आनी फामिलींचो विचार करून कांय बदलांखातीर कोर्टाक विनवणी करपाचो विजयाचो विचार.\",\n",
            "    \"session_001_0039.txt\": \"आशिल्ले सात आठ जाणूच. करंजाळे निदर्शनां करता म्हूण सांगून म्हापालिका पोलीस कलॅक्टराचें परमिशन घेतलें,\",\n",
            "    \"session_001_0040.txt\": \"इलॅक्शन कमिशनाक सांगलेंच ना. निदर्शनां केलीं पणजे.\",\n",
            "    \"session_001_0041.txt\": \"बीजेपीवाल्यांचे कंप्लेनीचेर पुलिसांनी तांकां बिडकी कवळूंक लायली.\",\n",
            "    \"session_001_0042.txt\": \"गोंयात सायबर क्रायम वाडाटा. वेबसायटींचेर सॅक्स टुरिजमाचें प्रोमोशन चल्लां म्हूण गोवा वुमन्स फोरमान तिडक उक्तायली. शिंदळकेखातीर चलयां आनी बायलांची पुरवण करपी वॅबसाटीं चलतात.\",\n",
            "    \"session_001_0043.txt\": \"असले प्रकार बंद जावंक जाय म्हूण गोवा वूमन्स फोरम मागता.\",\n",
            "    \"session_001_0044.txt\": \"फोरमान मडगावां जागृती रॅली काडली.\",\n",
            "    \"session_001_0045.txt\": \"दाबोळे केशव स्मृतीच्या अकरावेच्या भुरग्यांक हायर सेकंडरीचे तीन मजली बिल्डिंगेचे तेर्रासाचेर चडोवन टायल्स नितळ करूंक लायिल्ले प्रकरणाची शिक्षण खात्यान दखल घेतल्या.\",\n",
            "    \"session_001_0046.txt\": \"शिक्षण संचालकान एडीईआय कडच्यान रिपोर्ट मागला.\",\n",
            "    \"session_001_0047.txt\": \"रिपोर्ट आयलो काय हायर सेकंडरीच्या प्रिन्सिपलाक नोटीस वतली\",\n",
            "    \"session_001_0048.txt\": \"म्हूण शिक्षण संचालक गजानन भट प्रुडंटाकडेन उलयलो.\",\n",
            "    \"session_001_0049.txt\": \"सिटिझन जर्नलिस्टाचे व्हिडिओवेल्यान प्रुडंटान हो प्रकार एक्स्पोज केल्लो.\",\n",
            "    \"session_001_0050.txt\": \"ऑल इंडिया फुटबॉल फॅडरेशन आतां फुटसाल डेव्हलाप करपाचेर लक्ष घालतलें.\",\n",
            "    \"session_001_0051.txt\": \"ह्या कामांत आतां तांकां एशीयन फुटबॉल कन्फॅडरेशन आदार करतलें.\",\n",
            "    \"session_001_0052.txt\": \"इन्फ्रास्ट्रक्चर कशें आसा कांय पळोवंक सद्या एएफसी आनी एआयएफएफाचें शिश्टमंडळ देशभर भोंवतले.\",\n",
            "    \"session_001_0053.txt\": \"गोंयात ब्रेस्तारा इन्स्पेक्शन जालें.\",\n",
            "    \"session_001_0055.txt\": \"मुरगांवचो बीजेपी आमदार मिलींद नायकान आंगवण पुराय केली\",\n",
            "    \"session_001_0056.txt\": \"वास्को ते बोरये 28 किलोमिटर तो साई बाबाचें दर्शनाक चलत गेलो.\",\n",
            "    \"session_001_0057.txt\": \"बरोबर धायेक कार्यकर्त्यांचीय वारी\",\n",
            "    \"session_001_0058.txt\": \"मिलींदान सगल्यांक बरोबर घेवन फांतोडेर पाचंक वास्कोसावन बोरयेची वाट धल्ली.\",\n",
            "    \"session_001_0059.txt\": \"इलॅक्शनाकडेन मिलिंद जिकल्यार बोरये वारीची साईबाबाक ताचे कार्य़कर्त्यान आंगवण केल्ली.\",\n",
            "    \"session_001_0060.txt\": \"मिलिंद 143 मतांनी जिकलो\",\n",
            "    \"session_001_0061.txt\": \"स्वतंत्रताय दीस तेंकलो आयकपाक येना उलोवंक कळना अश्या दिव्यांग लोकांखातीर आता सायन लॅग्वेजीन राष्ट्रगीत लाँच जालां.\",\n",
            "    \"session_001_0062.txt\": \"ब्रदरहुड न्यु दिल्ली आनी डिसॅबीलीटी रायट्स असोसिएशन ऑफ गोवा\",\n",
            "    \"session_001_0063.txt\": \"कला अकादमीक बरोबर घेवन सायन लॅग्वीजींतलो नॅशनल एन्थम व्हिडियो लाँच केलो.\",\n",
            "    \"session_001_0064.txt\": \"अमिताभ बच्चनाक घेवन हें खास राष्ट्रगीत केलां.\",\n",
            "    \"session_001_0065.txt\": \"हें बुलॅटिन हांगांच सोंपता\",\n",
            "    \"session_001_0066.txt\": \"ह्यो खबरो तुमकां प्रुडंट वॉट्सॅप ट्विटर फेसबूक आनी प्रुडंट व्हॅबसायटीचेर लायव्ह मेळटात\",\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1D-kGrRqDD9HTTa3oQloR_btKTVnGZ8l2",
      "authorship_tag": "ABX9TyOGYEmxFxSGzqJ2V1mS8j1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}