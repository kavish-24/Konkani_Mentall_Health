{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavish-24/Konkani_Mentall_Health/blob/main/DataPreparationPrudentMedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrCp5PbZ-SYz",
        "outputId": "d98b206f-21ce-45a1-e533-e543413055e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting odfpy\n",
            "  Downloading odfpy-1.4.1.tar.gz (717 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.0/717.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from odfpy) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: odfpy\n",
            "  Building wheel for odfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for odfpy: filename=odfpy-1.4.1-py2.py3-none-any.whl size=160673 sha256=3d68e8ce6e862b9e708ef6583da209a090c20ac1361939528cf005642fac446c\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/5d/63/8243a7ee78fff0f944d638fd0e66d7278888f5e2285d7346b6\n",
            "Successfully built odfpy\n",
            "Installing collected packages: odfpy, evaluate\n",
            "Successfully installed evaluate-0.4.6 odfpy-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate odfpy pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from odf import text, teletype\n",
        "from odf.opendocument import OpenDocumentText\n",
        "from odf.style import Style, TextProperties\n",
        "from odf.text import P\n",
        "import os\n",
        "import re\n",
        "\n",
        "def extract_text_from_odt(odt_path, skip_bold=True, debug=False):\n",
        "    \"\"\"\n",
        "    Extract text from ODT file, skipping all bold text (including letters, spaces, and punctuation)\n",
        "    and removing all noise (punctuation, extra spaces) from non-bold text.\n",
        "\n",
        "    Args:\n",
        "        odt_path: Path to input ODT file\n",
        "        skip_bold: If True, skip all bold text\n",
        "        debug: If True, print debug information\n",
        "\n",
        "    Returns:\n",
        "        List of cleaned non-bold text paragraphs\n",
        "    \"\"\"\n",
        "    from odf.opendocument import load\n",
        "\n",
        "    if not os.path.exists(odt_path):\n",
        "        raise FileNotFoundError(f\"ODT file not found: {odt_path}\")\n",
        "\n",
        "    try:\n",
        "        doc = load(odt_path)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load ODT file: {e}\")\n",
        "\n",
        "    bold_styles = set()\n",
        "\n",
        "    # Check automatic styles for bold\n",
        "    for style in doc.automaticstyles.getElementsByType(Style):\n",
        "        style_name = style.getAttribute('name')\n",
        "        for prop in style.getElementsByType(TextProperties):\n",
        "            font_weight = prop.getAttribute('fontweight')\n",
        "            if font_weight and 'bold' in str(font_weight).lower():\n",
        "                bold_styles.add(style_name)\n",
        "                if debug:\n",
        "                    print(f\"   Found bold style: {style_name}\")\n",
        "\n",
        "    # Check named styles for bold\n",
        "    for style in doc.styles.getElementsByType(Style):\n",
        "        style_name = style.getAttribute('name')\n",
        "        for prop in style.getElementsByType(TextProperties):\n",
        "            font_weight = prop.getAttribute('fontweight')\n",
        "            if font_weight and 'bold' in str(font_weight).lower():\n",
        "                bold_styles.add(style_name)\n",
        "                if debug:\n",
        "                    print(f\"   Found bold style: {style_name}\")\n",
        "\n",
        "    print(f\"\\n🔍 Detected {len(bold_styles)} bold styles: {bold_styles}\")\n",
        "\n",
        "    extracted_parts = []\n",
        "    skipped_count = 0\n",
        "    kept_count = 0\n",
        "    skipped_text_samples = []\n",
        "    kept_text_samples = []\n",
        "\n",
        "    def get_style_name(node):\n",
        "        \"\"\"Get style name from a node using multiple methods\"\"\"\n",
        "        # Try different attribute names\n",
        "        for attr_name in ['stylename', 'style-name']:\n",
        "            try:\n",
        "                style = node.getAttribute(attr_name)\n",
        "                if style:\n",
        "                    return style\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Try namespace-aware retrieval\n",
        "        try:\n",
        "            style = node.getAttrNS(\n",
        "                \"urn:oasis:names:tc:opendocument:xmlns:text:1.0\",\n",
        "                \"style-name\"\n",
        "            )\n",
        "            if style:\n",
        "                return style\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def is_node_bold(node):\n",
        "        \"\"\"Check if a node has bold styling\"\"\"\n",
        "        style_name = get_style_name(node)\n",
        "        if style_name and style_name in bold_styles:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def process_node(node, parent_is_bold=False):\n",
        "        \"\"\"Recursively process a node and its children\"\"\"\n",
        "        nonlocal skipped_count, kept_count, skipped_text_samples, kept_text_samples\n",
        "\n",
        "        result_text = \"\"\n",
        "\n",
        "        # Check if current node is bold\n",
        "        current_is_bold = parent_is_bold or is_node_bold(node)\n",
        "\n",
        "        if node.nodeType == node.TEXT_NODE:\n",
        "            node_text = node.data\n",
        "            if node_text.strip():\n",
        "                if skip_bold and current_is_bold:\n",
        "                    skipped_count += 1\n",
        "                    if len(skipped_text_samples) < 5:\n",
        "                        skipped_text_samples.append(node_text.strip()[:50])\n",
        "                    if debug:\n",
        "                        print(f\"   [SKIP] TEXT_NODE (bold): {repr(node_text.strip()[:50])}\")\n",
        "                else:\n",
        "                    result_text += node_text\n",
        "                    kept_count += 1\n",
        "                    if len(kept_text_samples) < 5:\n",
        "                        kept_text_samples.append(node_text.strip()[:50])\n",
        "                    if debug:\n",
        "                        print(f\"   [KEEP] TEXT_NODE: {repr(node_text.strip()[:50])}\")\n",
        "\n",
        "        elif node.nodeType == node.ELEMENT_NODE:\n",
        "            # For span elements, check if they're bold\n",
        "            if node.tagName == \"text:span\":\n",
        "                span_is_bold = current_is_bold or is_node_bold(node)\n",
        "\n",
        "                # Get all text from this span (including nested elements)\n",
        "                span_full_text = teletype.extractText(node)\n",
        "\n",
        "                if skip_bold and span_is_bold:\n",
        "                    skipped_count += 1\n",
        "                    if span_full_text.strip() and len(skipped_text_samples) < 5:\n",
        "                        skipped_text_samples.append(span_full_text.strip()[:50])\n",
        "                    if debug:\n",
        "                        style_name = get_style_name(node)\n",
        "                        print(f\"   [SKIP] SPAN (bold - {style_name}): {repr(span_full_text[:50])}\")\n",
        "                    # Don't process children if parent span is bold\n",
        "                    return \"\"\n",
        "                else:\n",
        "                    if debug and span_full_text.strip():\n",
        "                        style_name = get_style_name(node)\n",
        "                        print(f\"   [KEEP] SPAN ({style_name}): {repr(span_full_text[:50])}\")\n",
        "                    # Process children with current bold status\n",
        "                    for child in node.childNodes:\n",
        "                        result_text += process_node(child, span_is_bold)\n",
        "            else:\n",
        "                # For other elements, process children\n",
        "                for child in node.childNodes:\n",
        "                    result_text += process_node(child, current_is_bold)\n",
        "\n",
        "        return result_text\n",
        "\n",
        "    for paragraph in doc.getElementsByType(text.P):\n",
        "        # Check if paragraph itself is bold\n",
        "        para_is_bold = is_node_bold(paragraph)\n",
        "\n",
        "        if debug and para_is_bold:\n",
        "            print(f\"\\n⚠️  Paragraph itself is BOLD - will skip all content\")\n",
        "\n",
        "        para_text = \"\"\n",
        "        for node in paragraph.childNodes:\n",
        "            para_text += process_node(node, para_is_bold)\n",
        "\n",
        "        if para_text.strip():\n",
        "            # Clean text: remove all punctuation and normalize spaces\n",
        "            para_text = re.sub(r'\\.{2,}', ' ', para_text)  # Replace 2+ dots with single space\n",
        "            para_text = re.sub(r'[!?,;:\"\\'()\\[\\]{}\\-—*]+', '', para_text)  # Remove other punctuation\n",
        "\n",
        "            para_text = re.sub(r'\\s+', ' ', para_text)  # Normalize spaces\n",
        "            para_text = para_text.strip()\n",
        "            if para_text:\n",
        "                extracted_parts.append(para_text)\n",
        "\n",
        "    print(f\"\\n📊 Extraction summary:\")\n",
        "    print(f\"   Kept: {kept_count} elements\")\n",
        "    print(f\"   Skipped (bold): {skipped_count} elements\")\n",
        "    print(f\"   Extracted paragraphs: {len(extracted_parts)}\")\n",
        "\n",
        "    if skipped_count > 0:\n",
        "        print(f\"\\n❌ Sample SKIPPED bold text:\")\n",
        "        for sample in skipped_text_samples:\n",
        "            print(f\"   • {repr(sample)}...\")\n",
        "\n",
        "    if kept_count > 0:\n",
        "        print(f\"\\n✅ Sample KEPT non-bold text:\")\n",
        "        for sample in kept_text_samples:\n",
        "            print(f\"   • {repr(sample)}...\")\n",
        "\n",
        "    if extracted_parts:\n",
        "        print(f\"\\n🧼 Sample CLEANED paragraphs:\")\n",
        "        for i, para in enumerate(extracted_parts[:3]):\n",
        "            print(f\"   • [{i}] {repr(para)[:100]}...\")\n",
        "\n",
        "    if skipped_count == 0 and skip_bold:\n",
        "        print(f\"\\n⚠️  WARNING: No bold text was found to skip! Check ODT styles.\")\n",
        "\n",
        "    return extracted_parts\n",
        "\n",
        "def create_new_odt(output_path, paragraphs):\n",
        "    \"\"\"\n",
        "    Create a new ODT file with the given paragraphs.\n",
        "\n",
        "    Args:\n",
        "        output_path: Path to save the new ODT file\n",
        "        paragraphs: List of text paragraphs to include\n",
        "    \"\"\"\n",
        "    doc = OpenDocumentText()\n",
        "\n",
        "    for para_text in paragraphs:\n",
        "        p = P(text=para_text)\n",
        "        doc.text.addElement(p)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        doc.save(output_path)\n",
        "        print(f\"\\n💾 New ODT file created: {output_path}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to save new ODT file: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    INPUT_ODT_PATH = os.getenv(\"TRANSCRIPT_FILE_PATH\", \"/content/drive/MyDrive/Anju Project (1)/Audio Prudent media (1)/August 2017 (1)/dataset/10  AUG PRIME.odt\")\n",
        "    OUTPUT_ODT_PATH = os.getenv(\"OUTPUT_ODT_PATH\", \"/content/drive/MyDrive/Anju Project (1)/Audio Prudent media (1)/August 2017 (1)/dataset/10 AUG PRIME_non_bold.odt\")\n",
        "    DEBUG = True\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"EXTRACTING NON-BOLD TEXT AND REMOVING ALL NOISE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        print(\"\\n📄 Extracting text from ODT file...\")\n",
        "        paragraphs = extract_text_from_odt(INPUT_ODT_PATH, skip_bold=True, debug=DEBUG)\n",
        "\n",
        "        print(f\"\\n✓ Extracted {len(paragraphs)} paragraphs\")\n",
        "        if paragraphs:\n",
        "            print(f\"   Sample cleaned paragraph: {repr(paragraphs[0])[:150]}...\")\n",
        "\n",
        "        print(\"\\n📝 Creating new ODT file...\")\n",
        "        create_new_odt(OUTPUT_ODT_PATH, paragraphs)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"✅ PROCESS COMPLETE!\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Process failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CaN9XmAZa5_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simplified Audio Segmentation Using Whisper-Only Approach\n",
        "=========================================================\n",
        "\n",
        "This script uses Whisper's built-in capabilities for accurate audio-text alignment\n",
        "without complex manual matching. Perfect for Google Colab.\n",
        "\n",
        "Approach:\n",
        "1. Use Silero VAD to detect speech segments\n",
        "2. Let Whisper transcribe each segment with word timestamps\n",
        "3. Validate and create training manifest\n",
        "\n",
        "No complex alignment needed - Whisper handles it!\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALLATION (Run this first in Colab)\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "!pip install -q faster-whisper\n",
        "!pip install -q torch torchaudio\n",
        "!pip install -q librosa soundfile\n",
        "!pip install -q odfpy\n",
        "!pip install -q tqdm\n",
        "!apt-get install -y ffmpeg\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For ODT reading\n",
        "from odf.opendocument import load\n",
        "from odf import text, teletype\n",
        "\n",
        "# Whisper\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"Configuration for audio segmentation.\"\"\"\n",
        "\n",
        "    # Paths (modify these for your Colab setup)\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/dataset/whisper_segments\"\n",
        "\n",
        "    # Whisper settings\n",
        "    WHISPER_MODEL = \"small\"  # Options: tiny, base, small, medium, large\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    COMPUTE_TYPE = \"float16\" if torch.cuda.is_available() else \"int8\"\n",
        "\n",
        "    # VAD settings\n",
        "    VAD_THRESHOLD = 0.5\n",
        "    MIN_SPEECH_DURATION = 0.5  # seconds\n",
        "    MIN_SILENCE_DURATION = 0.3  # seconds\n",
        "\n",
        "    # Segment settings\n",
        "    MIN_SEGMENT_DURATION = 1.0\n",
        "    MAX_SEGMENT_DURATION = 30.0\n",
        "    TARGET_SEGMENT_DURATION = 10.0  # Ideal segment length\n",
        "\n",
        "    # Language\n",
        "    LANGUAGE = \"mr\"  # Marathi (closest to Konkani in Whisper)\n",
        "\n",
        "    # Quality thresholds\n",
        "    MIN_CONFIDENCE = 0.3  # Minimum word probability\n",
        "    MIN_WORDS_PER_SEGMENT = 3\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SILERO VAD (Voice Activity Detection)\n",
        "# ============================================================================\n",
        "class SileroVAD:\n",
        "    \"\"\"Silero VAD for detecting speech segments.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Loading Silero VAD model...\")\n",
        "        try:\n",
        "            self.model, utils = torch.hub.load(\n",
        "                repo_or_dir='snakers4/silero-vad',\n",
        "                model='silero_vad',\n",
        "                force_reload=False,\n",
        "                onnx=False\n",
        "            )\n",
        "            self.get_speech_timestamps = utils[0]\n",
        "            print(\"✓ Silero VAD loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Could not load Silero VAD: {e}\")\n",
        "            print(\"Falling back to energy-based VAD\")\n",
        "            self.model = None\n",
        "\n",
        "    def detect_speech(self, audio_path, threshold=0.5, min_speech_ms=250, min_silence_ms=100):\n",
        "        \"\"\"Detect speech segments in audio file.\"\"\"\n",
        "\n",
        "        # Load audio at 16kHz (required by Silero)\n",
        "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "        if self.model is not None:\n",
        "            # Use Silero VAD\n",
        "            audio_tensor = torch.from_numpy(audio)\n",
        "\n",
        "            speech_timestamps = self.get_speech_timestamps(\n",
        "                audio_tensor,\n",
        "                self.model,\n",
        "                threshold=threshold,\n",
        "                min_speech_duration_ms=min_speech_ms,\n",
        "                min_silence_duration_ms=min_silence_ms,\n",
        "                sampling_rate=16000\n",
        "            )\n",
        "\n",
        "            segments = [\n",
        "                {\n",
        "                    'start': ts['start'] / 16000,\n",
        "                    'end': ts['end'] / 16000\n",
        "                }\n",
        "                for ts in speech_timestamps\n",
        "            ]\n",
        "        else:\n",
        "            # Fallback: Energy-based VAD\n",
        "            segments = self._energy_based_vad(audio, sr)\n",
        "\n",
        "        return segments\n",
        "\n",
        "    def _energy_based_vad(self, audio, sr, frame_length=2048, hop_length=512):\n",
        "        \"\"\"Fallback energy-based VAD.\"\"\"\n",
        "        # Calculate RMS energy\n",
        "        rms = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Threshold\n",
        "        threshold = np.mean(rms) * 1.5\n",
        "\n",
        "        # Find speech frames\n",
        "        speech_frames = rms > threshold\n",
        "\n",
        "        # Convert to time segments\n",
        "        times = librosa.frames_to_time(np.arange(len(speech_frames)), sr=sr, hop_length=hop_length)\n",
        "\n",
        "        segments = []\n",
        "        in_speech = False\n",
        "        start_time = 0\n",
        "\n",
        "        for i, is_speech in enumerate(speech_frames):\n",
        "            if is_speech and not in_speech:\n",
        "                start_time = times[i]\n",
        "                in_speech = True\n",
        "            elif not is_speech and in_speech:\n",
        "                segments.append({'start': start_time, 'end': times[i]})\n",
        "                in_speech = False\n",
        "\n",
        "        if in_speech:\n",
        "            segments.append({'start': start_time, 'end': times[-1]})\n",
        "\n",
        "        return segments\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# WHISPER ALIGNER\n",
        "# ============================================================================\n",
        "class WhisperAligner:\n",
        "    \"\"\"Simplified aligner using only Whisper.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config = None):\n",
        "        self.config = config or Config()\n",
        "\n",
        "        # Create output directories\n",
        "        self.output_dir = Path(self.config.OUTPUT_DIR)\n",
        "        self.audio_dir = self.output_dir / \"audio\"\n",
        "        self.text_dir = self.output_dir / \"text\"\n",
        "        self.metadata_dir = self.output_dir / \"metadata\"\n",
        "\n",
        "        for dir_path in [self.audio_dir, self.text_dir, self.metadata_dir]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Initialize VAD\n",
        "        self.vad = SileroVAD()\n",
        "\n",
        "        # Initialize Whisper\n",
        "        print(f\"Loading Whisper model: {self.config.WHISPER_MODEL}...\")\n",
        "        self.whisper = WhisperModel(\n",
        "            self.config.WHISPER_MODEL,\n",
        "            device=self.config.DEVICE,\n",
        "            compute_type=self.config.COMPUTE_TYPE\n",
        "        )\n",
        "        print(f\"✓ Whisper loaded on {self.config.DEVICE}\")\n",
        "\n",
        "    def load_transcript_from_odt(self, odt_path: str) -> str:\n",
        "        \"\"\"Load transcript from ODT file.\"\"\"\n",
        "        print(f\"Loading transcript: {Path(odt_path).name}\")\n",
        "\n",
        "        doc = load(str(odt_path))\n",
        "        paragraphs = []\n",
        "\n",
        "        for paragraph in doc.getElementsByType(text.P):\n",
        "            para_text = teletype.extractText(paragraph)\n",
        "            if para_text.strip():\n",
        "                paragraphs.append(para_text.strip())\n",
        "\n",
        "        full_text = \" \".join(paragraphs)\n",
        "        print(f\"✓ Loaded {len(paragraphs)} paragraphs, {len(full_text)} characters\")\n",
        "        return full_text\n",
        "\n",
        "    def process_audio_file(\n",
        "        self,\n",
        "        audio_path: str,\n",
        "        odt_path: str = None,\n",
        "        session_id: str = None\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process audio file using Whisper-only approach.\n",
        "\n",
        "        Args:\n",
        "            audio_path: Path to audio file\n",
        "            odt_path: Optional path to reference transcript (for validation only)\n",
        "            session_id: Session identifier\n",
        "\n",
        "        Returns:\n",
        "            List of segment metadata\n",
        "        \"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        if session_id is None:\n",
        "            session_id = audio_path.stem\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PROCESSING: {audio_path.name}\")\n",
        "        print(f\"Session: {session_id}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Load reference transcript if provided\n",
        "        reference_transcript = None\n",
        "        if odt_path:\n",
        "            reference_transcript = self.load_transcript_from_odt(odt_path)\n",
        "\n",
        "        # Step 1: Detect speech segments with VAD\n",
        "        print(\"Step 1: Detecting speech segments...\")\n",
        "        vad_segments = self.vad.detect_speech(\n",
        "            str(audio_path),\n",
        "            threshold=self.config.VAD_THRESHOLD,\n",
        "            min_speech_ms=int(self.config.MIN_SPEECH_DURATION * 1000),\n",
        "            min_silence_ms=int(self.config.MIN_SILENCE_DURATION * 1000)\n",
        "        )\n",
        "        print(f\"✓ Found {len(vad_segments)} speech segments\")\n",
        "\n",
        "        # Step 2: Merge short segments\n",
        "        merged_segments = self._merge_short_segments(vad_segments)\n",
        "        print(f\"✓ Merged to {len(merged_segments)} segments\")\n",
        "\n",
        "        # Step 3: Transcribe each segment with Whisper\n",
        "        print(\"\\nStep 2: Transcribing with Whisper...\")\n",
        "        all_segments = []\n",
        "\n",
        "        for idx, vad_seg in enumerate(tqdm(merged_segments, desc=\"Transcribing\")):\n",
        "            # Extract audio segment\n",
        "            segment_audio, sr = librosa.load(\n",
        "                str(audio_path),\n",
        "                sr=16000,\n",
        "                offset=vad_seg['start'],\n",
        "                duration=vad_seg['end'] - vad_seg['start']\n",
        "            )\n",
        "\n",
        "            # Save temporary audio file for Whisper\n",
        "            temp_audio = self.output_dir / f\"temp_{idx}.wav\"\n",
        "            sf.write(temp_audio, segment_audio, sr)\n",
        "\n",
        "            try:\n",
        "                # Transcribe with Whisper\n",
        "                segments, info = self.whisper.transcribe(\n",
        "                    str(temp_audio),\n",
        "                    language=self.config.LANGUAGE,\n",
        "                    word_timestamps=True,\n",
        "                    beam_size=5,\n",
        "                    best_of=5,\n",
        "                    temperature=0.0,\n",
        "                    vad_filter=False  # We already did VAD\n",
        "                )\n",
        "\n",
        "                # Process segments\n",
        "                for seg in segments:\n",
        "                    if not seg.text.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Adjust timestamps to original audio\n",
        "                    adjusted_start = vad_seg['start'] + seg.start\n",
        "                    adjusted_end = vad_seg['start'] + seg.end\n",
        "\n",
        "                    # Extract word-level info\n",
        "                    words = []\n",
        "                    if hasattr(seg, 'words') and seg.words:\n",
        "                        words = [\n",
        "                            {\n",
        "                                'word': w.word.strip(),\n",
        "                                'start': vad_seg['start'] + w.start,\n",
        "                                'end': vad_seg['start'] + w.end,\n",
        "                                'probability': w.probability\n",
        "                            }\n",
        "                            for w in seg.words\n",
        "                        ]\n",
        "\n",
        "                    all_segments.append({\n",
        "                        'start': adjusted_start,\n",
        "                        'end': adjusted_end,\n",
        "                        'duration': adjusted_end - adjusted_start,\n",
        "                        'text': seg.text.strip(),\n",
        "                        'words': words,\n",
        "                        'avg_logprob': seg.avg_logprob if hasattr(seg, 'avg_logprob') else 0.0\n",
        "                    })\n",
        "\n",
        "            finally:\n",
        "                # Clean up temp file\n",
        "                if temp_audio.exists():\n",
        "                    temp_audio.unlink()\n",
        "\n",
        "        print(f\"\\n✓ Transcribed {len(all_segments)} segments\")\n",
        "\n",
        "        # Step 3: Validate and filter segments\n",
        "        print(\"\\nStep 3: Validating segments...\")\n",
        "        valid_segments = self._validate_segments(all_segments)\n",
        "        print(f\"✓ {len(valid_segments)}/{len(all_segments)} segments passed validation\")\n",
        "\n",
        "        # Step 4: Save segments\n",
        "        print(\"\\nStep 4: Saving segments...\")\n",
        "        saved_segments = self._save_segments(\n",
        "            valid_segments,\n",
        "            audio_path,\n",
        "            session_id,\n",
        "            reference_transcript\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"✓ COMPLETE! Created {len(saved_segments)} segments\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return saved_segments\n",
        "\n",
        "    def _merge_short_segments(self, segments: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Merge segments that are too short.\"\"\"\n",
        "        if not segments:\n",
        "            return []\n",
        "\n",
        "        merged = []\n",
        "        current = segments[0].copy()\n",
        "\n",
        "        for next_seg in segments[1:]:\n",
        "            current_duration = current['end'] - current['start']\n",
        "            gap = next_seg['start'] - current['end']\n",
        "\n",
        "            # Merge if current is too short and gap is small\n",
        "            if current_duration < self.config.TARGET_SEGMENT_DURATION and gap < 1.0:\n",
        "                current['end'] = next_seg['end']\n",
        "            else:\n",
        "                merged.append(current)\n",
        "                current = next_seg.copy()\n",
        "\n",
        "        merged.append(current)\n",
        "        return merged\n",
        "\n",
        "    def _validate_segments(self, segments: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Validate and filter segments based on quality criteria.\"\"\"\n",
        "        valid = []\n",
        "\n",
        "        for seg in segments:\n",
        "            # Check duration\n",
        "            if seg['duration'] < self.config.MIN_SEGMENT_DURATION:\n",
        "                continue\n",
        "            if seg['duration'] > self.config.MAX_SEGMENT_DURATION:\n",
        "                continue\n",
        "\n",
        "            # Check text quality\n",
        "            text = seg['text'].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            words = text.split()\n",
        "            if len(words) < self.config.MIN_WORDS_PER_SEGMENT:\n",
        "                continue\n",
        "\n",
        "            # Check word-level confidence\n",
        "            if seg.get('words'):\n",
        "                avg_prob = np.mean([w['probability'] for w in seg['words']])\n",
        "                if avg_prob < self.config.MIN_CONFIDENCE:\n",
        "                    continue\n",
        "\n",
        "            valid.append(seg)\n",
        "\n",
        "        return valid\n",
        "\n",
        "    def _save_segments(\n",
        "        self,\n",
        "        segments: List[Dict],\n",
        "        audio_path: Path,\n",
        "        session_id: str,\n",
        "        reference_transcript: str = None\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"Save segments to disk and create manifest.\"\"\"\n",
        "        saved_segments = []\n",
        "\n",
        "        # Load full audio once\n",
        "        print(\"Loading audio for extraction...\")\n",
        "        audio, sr = librosa.load(str(audio_path), sr=16000, mono=True)\n",
        "\n",
        "        for idx, seg in enumerate(tqdm(segments, desc=\"Saving segments\")):\n",
        "            segment_id = f\"{session_id}_{idx:04d}\"\n",
        "\n",
        "            # Paths\n",
        "            audio_file = self.audio_dir / f\"{segment_id}.wav\"\n",
        "            text_file = self.text_dir / f\"{segment_id}.txt\"\n",
        "\n",
        "            try:\n",
        "                # Extract audio segment\n",
        "                start_sample = int(seg['start'] * sr)\n",
        "                end_sample = int(seg['end'] * sr)\n",
        "                segment_audio = audio[start_sample:end_sample]\n",
        "\n",
        "                # Save audio\n",
        "                sf.write(audio_file, segment_audio, sr)\n",
        "\n",
        "                # Save text\n",
        "                clean_text = self._clean_text(seg['text'])\n",
        "                with open(text_file, 'w', encoding='utf-8') as f:\n",
        "                    f.write(clean_text)\n",
        "\n",
        "                # Create metadata entry\n",
        "                metadata = {\n",
        "                    'segment_id': segment_id,\n",
        "                    'audio_filepath': f\"audio/{segment_id}.wav\",\n",
        "                    'text_filepath': f\"text/{segment_id}.txt\",\n",
        "                    'text': clean_text,\n",
        "                    'start_time': float(seg['start']),\n",
        "                    'end_time': float(seg['end']),\n",
        "                    'duration': float(seg['duration']),\n",
        "                    'word_count': len(clean_text.split()),\n",
        "                    'language': self.config.LANGUAGE,\n",
        "                    'avg_confidence': float(np.mean([w['probability'] for w in seg.get('words', [])])) if seg.get('words') else 0.0\n",
        "                }\n",
        "\n",
        "                saved_segments.append(metadata)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n⚠ Error saving segment {segment_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save manifest\n",
        "        manifest = {\n",
        "            'session_id': session_id,\n",
        "            'audio_file': str(audio_path.name),\n",
        "            'total_segments': len(saved_segments),\n",
        "            'total_duration': sum(s['duration'] for s in saved_segments),\n",
        "            'language': self.config.LANGUAGE,\n",
        "            'segments': saved_segments\n",
        "        }\n",
        "\n",
        "        manifest_file = self.metadata_dir / f\"{session_id}_manifest.json\"\n",
        "        with open(manifest_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Also save JSONL format for Whisper fine-tuning\n",
        "        jsonl_file = self.metadata_dir / f\"{session_id}_train.jsonl\"\n",
        "        with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
        "            for seg in saved_segments:\n",
        "                entry = {\n",
        "                    'audio': seg['audio_filepath'],\n",
        "                    'text': seg['text'],\n",
        "                    'duration': seg['duration']\n",
        "                }\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"\\n✓ Saved manifest to: {manifest_file}\")\n",
        "        print(f\"✓ Saved JSONL to: {jsonl_file}\")\n",
        "\n",
        "        return saved_segments\n",
        "\n",
        "    @staticmethod\n",
        "    def _clean_text(text: str) -> str:\n",
        "        \"\"\"Clean and normalize text.\"\"\"\n",
        "        # Normalize Unicode\n",
        "        text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Strip\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    config = Config()\n",
        "\n",
        "    # Initialize aligner\n",
        "    aligner = WhisperAligner(config)\n",
        "\n",
        "    # File list - MODIFY THESE PATHS FOR YOUR SETUP\n",
        "    files_to_process = [\n",
        "        {\n",
        "            'audio_path': '/content/drive/MyDrive/dataset/Konkani Prime News_100817.wav',  # Your audio file\n",
        "            'odt_path': '/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt',  # Optional: reference transcript\n",
        "            'session_id': 'session_001'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Process files\n",
        "    all_segments = []\n",
        "\n",
        "    for file_info in files_to_process:\n",
        "        try:\n",
        "            # Check if files exist\n",
        "            if not os.path.exists(file_info['audio_path']):\n",
        "                print(f\"❌ Audio file not found: {file_info['audio_path']}\")\n",
        "                continue\n",
        "\n",
        "            # Process\n",
        "            segments = aligner.process_audio_file(\n",
        "                audio_path=file_info['audio_path'],\n",
        "                odt_path=file_info.get('odt_path'),\n",
        "                session_id=file_info.get('session_id')\n",
        "            )\n",
        "\n",
        "            all_segments.extend(segments)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file_info['audio_path']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINAL SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total segments created: {len(all_segments)}\")\n",
        "    print(f\"Total duration: {sum(s['duration'] for s in all_segments):.2f}s\")\n",
        "    print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RUN\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5h5wS1yCo4O",
        "outputId": "cf8e4d87-aad0-4d32-8564-68cfb2367b10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Silero VAD model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Silero VAD loaded\n",
            "Loading Whisper model: small...\n",
            "✓ Whisper loaded on cuda\n",
            "\n",
            "================================================================================\n",
            "PROCESSING: Konkani Prime News_100817.wav\n",
            "Session: session_001\n",
            "================================================================================\n",
            "\n",
            "Loading transcript: 10 AUG PRIME_non_bold (1).odt\n",
            "✓ Loaded 27 paragraphs, 6076 characters\n",
            "Step 1: Detecting speech segments...\n",
            "✓ Found 71 speech segments\n",
            "✓ Merged to 37 segments\n",
            "\n",
            "Step 2: Transcribing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 37/37 [01:00<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Transcribed 75 segments\n",
            "\n",
            "Step 3: Validating segments...\n",
            "✓ 68/75 segments passed validation\n",
            "\n",
            "Step 4: Saving segments...\n",
            "Loading audio for extraction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving segments: 100%|██████████| 68/68 [00:01<00:00, 38.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Saved manifest to: /content/drive/MyDrive/dataset/whisper_segments/metadata/session_001_manifest.json\n",
            "✓ Saved JSONL to: /content/drive/MyDrive/dataset/whisper_segments/metadata/session_001_train.jsonl\n",
            "\n",
            "================================================================================\n",
            "✓ COMPLETE! Created 68 segments\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY\n",
            "================================================================================\n",
            "Total segments created: 68\n",
            "Total duration: 336.08s\n",
            "Output directory: /content/drive/MyDrive/dataset/whisper_segments\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy librosa faster-whisper tqdm odfpy soundfile python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkzcJZzJDh9c",
        "outputId": "fa15c1a7-b452-4bae-ada0-3cb87fbf6e1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: faster-whisper in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: odfpy in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.35.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (1.23.2)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (16.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from odfpy) (0.7.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.10)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2025.10.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from odf import text, teletype\n",
        "from odf.opendocument import load\n",
        "\n",
        "class PhoneticTranscriptMatcher:\n",
        "    def __init__(self, whisper_dir, odt_path, output_dir, noise_threshold=10):\n",
        "        self.whisper_dir = Path(whisper_dir)\n",
        "        self.odt_path = Path(odt_path)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.noise_threshold = noise_threshold  # Minimum word count\n",
        "        self.reference_paragraphs = []\n",
        "        self.used_paragraphs = set()  # Track which paragraphs have been used\n",
        "\n",
        "    def read_odt(self):\n",
        "        \"\"\"Extract text from ODT file\"\"\"\n",
        "        print(\"Reading ODT reference document...\")\n",
        "        try:\n",
        "            doc = load(self.odt_path)\n",
        "            all_paragraphs = doc.getElementsByType(text.P)\n",
        "\n",
        "            for para in all_paragraphs:\n",
        "                para_text = teletype.extractText(para)\n",
        "                if para_text.strip():\n",
        "                    self.reference_paragraphs.append(para_text.strip())\n",
        "\n",
        "            print(f\"Loaded {len(self.reference_paragraphs)} paragraphs from ODT\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading ODT: {e}\")\n",
        "            return False\n",
        "\n",
        "    def normalize_marathi_phonetic(self, char):\n",
        "        \"\"\"Normalize Marathi/Konkani characters to their phonetic equivalents\"\"\"\n",
        "        # Remove vowel marks (matras)\n",
        "        vowel_marks = ['ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', '़', 'ृ', 'ॅ']\n",
        "        if char in vowel_marks:\n",
        "            return ''\n",
        "\n",
        "        # Phonetic normalization mapping\n",
        "        phonetic_map = {\n",
        "            # थ / ट / ठ -> ट\n",
        "            'थ': 'ट', 'ठ': 'ट',\n",
        "            # ध / ड / ढ -> ड\n",
        "            'ध': 'ड', 'ढ': 'ड',\n",
        "            # फ / प -> प\n",
        "            'फ': 'प',\n",
        "            # भ / ब -> ब\n",
        "            'भ': 'ब',\n",
        "            # छ / च -> च\n",
        "            'छ': 'च',\n",
        "            # झ / ज -> ज\n",
        "            'झ': 'ज',\n",
        "            # ख / क / घ / ग -> क\n",
        "            'ख': 'क', 'घ': 'क', 'ग': 'क',\n",
        "            # ण / न -> न\n",
        "            'ण': 'न',\n",
        "            # ष / श / स -> स\n",
        "            'ष': 'स', 'श': 'स',\n",
        "            # ळ / ल -> ल\n",
        "            'ळ': 'ल',\n",
        "        }\n",
        "\n",
        "        return phonetic_map.get(char, char.lower())\n",
        "\n",
        "    def get_first_letters(self, text):\n",
        "        \"\"\"Extract first letters of each word for phonetic matching\"\"\"\n",
        "        words = re.findall(r'\\S+', text)\n",
        "        first_letters = []\n",
        "        for word in words:\n",
        "            # Remove punctuation from start\n",
        "            clean_word = re.sub(r'^[^\\w]+', '', word)\n",
        "            if clean_word:\n",
        "                first_char = clean_word[0]\n",
        "                normalized = self.normalize_marathi_phonetic(first_char)\n",
        "                if normalized:  # Only add if not empty (vowel marks return '')\n",
        "                    first_letters.append(normalized)\n",
        "        return first_letters\n",
        "\n",
        "    def is_noisy_transcript(self, content):\n",
        "        \"\"\"Detect if transcript is likely noise/useless\"\"\"\n",
        "        if not content:\n",
        "            return True\n",
        "\n",
        "        words = content.split()\n",
        "        if len(words) < self.noise_threshold:\n",
        "            return True\n",
        "\n",
        "        # Check for excessive repetition\n",
        "        unique_words = set(words)\n",
        "        repetition_ratio = len(unique_words) / len(words)\n",
        "        if repetition_ratio < 0.2:  # Too repetitive\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def match_by_first_letters(self, transcript_text):\n",
        "        \"\"\"Match transcript to reference paragraph based on first letter sounds\"\"\"\n",
        "        transcript_letters = self.get_first_letters(transcript_text)\n",
        "\n",
        "        if len(transcript_letters) < 3:\n",
        "            return None, 0, -1\n",
        "\n",
        "        # Compare first 5-10 letters\n",
        "        compare_length = min(10, len(transcript_letters))\n",
        "        transcript_signature = transcript_letters[:compare_length]\n",
        "\n",
        "        best_match = None\n",
        "        best_score = 0\n",
        "        best_idx = -1\n",
        "\n",
        "        for idx, ref_para in enumerate(self.reference_paragraphs):\n",
        "            # Skip if this paragraph has already been used\n",
        "            if idx in self.used_paragraphs:\n",
        "                continue\n",
        "\n",
        "            ref_letters = self.get_first_letters(ref_para)\n",
        "\n",
        "            if len(ref_letters) < 3:\n",
        "                continue\n",
        "\n",
        "            ref_signature = ref_letters[:compare_length]\n",
        "\n",
        "            # Calculate matching score\n",
        "            matches = sum(1 for i, letter in enumerate(transcript_signature)\n",
        "                         if i < len(ref_signature) and letter == ref_signature[i])\n",
        "\n",
        "            score = matches / compare_length\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = ref_para\n",
        "                best_idx = idx\n",
        "\n",
        "        return best_match, best_score, best_idx\n",
        "\n",
        "    def process_transcripts(self):\n",
        "        \"\"\"Process all transcript files\"\"\"\n",
        "        if not self.read_odt():\n",
        "            print(\"Failed to read ODT file. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Create output directory\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Check if directory exists\n",
        "        if not self.whisper_dir.exists():\n",
        "            print(f\"ERROR: Directory not found: {self.whisper_dir}\")\n",
        "            return\n",
        "\n",
        "        # Get all text files (try different patterns)\n",
        "        transcript_files = list(self.whisper_dir.glob('*.txt'))\n",
        "        if not transcript_files:\n",
        "            transcript_files = list(self.whisper_dir.glob('**/*.txt'))  # Search subdirectories\n",
        "\n",
        "        if not transcript_files:\n",
        "            print(f\"\\nNo .txt files found in: {self.whisper_dir}\")\n",
        "            print(\"Contents of directory:\")\n",
        "            try:\n",
        "                for item in self.whisper_dir.iterdir():\n",
        "                    print(f\"  - {item.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Cannot read directory: {e}\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nFound {len(transcript_files)} transcript files\")\n",
        "\n",
        "        processed = 0\n",
        "        skipped_noise = 0\n",
        "        no_match = 0\n",
        "\n",
        "        for txt_file in sorted(transcript_files):\n",
        "            print(f\"\\nProcessing: {txt_file.name}\")\n",
        "\n",
        "            # Read transcript\n",
        "            try:\n",
        "                with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "                    transcript_content = f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️  Error reading file: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Match based on first letter sounds (process all files, no noise filtering)\n",
        "            matched_text, score, idx = self.match_by_first_letters(transcript_content)\n",
        "\n",
        "            # Get first letters for display\n",
        "            trans_letters = ''.join(self.get_first_letters(transcript_content)[:15])\n",
        "\n",
        "            if score < 0.1:  # Low confidence match\n",
        "                print(f\"  ⚠️  No good match (score: {score:.2f}, letters: {trans_letters})\")\n",
        "                no_match += 1\n",
        "                # Skip this file - no good match found\n",
        "                continue\n",
        "            else:\n",
        "                ref_letters = ''.join(self.get_first_letters(matched_text)[:15]) if matched_text else ''\n",
        "                print(f\"  ✓ Match found (score: {score:.2f}, para: {idx})\")\n",
        "                print(f\"    Transcript letters: {trans_letters}\")\n",
        "                print(f\"    Reference letters:  {ref_letters}\")\n",
        "\n",
        "                # Mark this paragraph as used\n",
        "                self.used_paragraphs.add(idx)\n",
        "\n",
        "                # Output only the reference text from ODT\n",
        "                output_content = matched_text\n",
        "\n",
        "            # Save to output\n",
        "            output_file = self.output_dir / txt_file.name\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(output_content)\n",
        "\n",
        "            processed += 1\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PROCESSING SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total files found: {len(transcript_files)}\")\n",
        "        print(f\"Successfully processed: {processed}\")\n",
        "        print(f\"Skipped (noise): {skipped_noise}\")\n",
        "        print(f\"No good match: {no_match}\")\n",
        "        print(f\"Unique paragraphs used: {len(self.used_paragraphs)}/{len(self.reference_paragraphs)}\")\n",
        "        print(f\"\\nOutput saved to: {self.output_dir}\")\n",
        "\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure paths\n",
        "    WHISPER_DIR = \"/content/drive/MyDrive/dataset/whisper_segments/text\"\n",
        "    ODT_FILE = \"/content/drive/MyDrive/dataset/10 AUG PRIME_non_bold (1).odt\"\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/dataset/final_text\"\n",
        "\n",
        "    # Initialize and run\n",
        "    matcher = PhoneticTranscriptMatcher(\n",
        "        whisper_dir=WHISPER_DIR,\n",
        "        odt_path=ODT_FILE,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        noise_threshold=10  # Minimum words to not be considered noise\n",
        "    )\n",
        "\n",
        "    matcher.process_transcripts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiJxHNg1JoK_",
        "outputId": "433a2176-59ed-4cb2-c548-2b01df36854e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading ODT reference document...\n",
            "Loaded 27 paragraphs from ODT\n",
            "\n",
            "Found 68 transcript files\n",
            "\n",
            "Processing: session_001_0000.txt\n",
            "  ✓ Match found (score: 0.50, para: 0)\n",
            "    Transcript letters: नबपअ\n",
            "    Reference letters:  नपपक\n",
            "\n",
            "Processing: session_001_0001.txt\n",
            "  ✓ Match found (score: 0.10, para: 1)\n",
            "    Transcript letters: तवदकबवतआटडकसअउप\n",
            "    Reference letters:  टनडकवटटडकसउपउलस\n",
            "\n",
            "Processing: session_001_0002.txt\n",
            "  ✓ Match found (score: 0.20, para: 8)\n",
            "    Transcript letters: कसबआबकपवपजततपमब\n",
            "    Reference letters:  कवटआटडसऑकपउसआकन\n",
            "\n",
            "Processing: session_001_0003.txt\n",
            "  ✓ Match found (score: 0.30, para: 9)\n",
            "    Transcript letters: असकसकरबपककककललउ\n",
            "    Reference letters:  कपकककपनसकहकमबअस\n",
            "\n",
            "Processing: session_001_0004.txt\n",
            "  ✓ Match found (score: 0.30, para: 4)\n",
            "    Transcript letters: आजतममदलदचसवतहकत\n",
            "    Reference letters:  आजममदडकवहबदललएप\n",
            "\n",
            "Processing: session_001_0005.txt\n",
            "  ✓ Match found (score: 0.20, para: 7)\n",
            "    Transcript letters: चकबदवसउआमतहन\n",
            "    Reference letters:  बपयबदएआएमतकपडआ\n",
            "\n",
            "Processing: session_001_0006.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: ॐॐ)\n",
            "\n",
            "Processing: session_001_0007.txt\n",
            "  ✓ Match found (score: 0.10, para: 14)\n",
            "    Transcript letters: अबपकतबदआसअआआमतक\n",
            "    Reference letters:  समरसपबसकजनजकतदम\n",
            "\n",
            "Processing: session_001_0008.txt\n",
            "  ✓ Match found (score: 0.20, para: 10)\n",
            "    Transcript letters: अदसदरपवककतवटअसक\n",
            "    Reference letters:  रएकमपरकककनरललकह\n",
            "\n",
            "Processing: session_001_0009.txt\n",
            "  ✓ Match found (score: 0.50, para: 13)\n",
            "    Transcript letters: अदरद\n",
            "    Reference letters:  कआरदवनआसजसकमवतत\n",
            "\n",
            "Processing: session_001_0010.txt\n",
            "  ✓ Match found (score: 0.40, para: 17)\n",
            "    Transcript letters: मतवदन\n",
            "    Reference letters:  मवकदआआउपकववरनकक\n",
            "\n",
            "Processing: session_001_0011.txt\n",
            "  ✓ Match found (score: 0.25, para: 19)\n",
            "    Transcript letters: मपअतवसअल\n",
            "    Reference letters:  बपककमससपयनकमदवज\n",
            "\n",
            "Processing: session_001_0012.txt\n",
            "  ✓ Match found (score: 0.20, para: 5)\n",
            "    Transcript letters: आआआआआआआआआआआआआआआ\n",
            "    Reference letters:  मआमवबचसदवआमककप7\n",
            "\n",
            "Processing: session_001_0013.txt\n",
            "  ✓ Match found (score: 0.40, para: 23)\n",
            "    Transcript letters: कबपककपनसलकमबअसक\n",
            "    Reference letters:  ऑइपपआपडकलकहकआतए\n",
            "\n",
            "Processing: session_001_0014.txt\n",
            "  ✓ Match found (score: 0.20, para: 16)\n",
            "    Transcript letters: कबपजपजततलमककस\n",
            "    Reference letters:  करर1पचउहतहबपजजम\n",
            "\n",
            "Processing: session_001_0015.txt\n",
            "  ✓ Match found (score: 0.20, para: 21)\n",
            "    Transcript letters: मकजलअतनचमअकककल\n",
            "    Reference letters:  दकसअबहसतमबतचटनक\n",
            "\n",
            "Processing: session_001_0016.txt\n",
            "  ✓ Match found (score: 0.20, para: 2)\n",
            "    Transcript letters: कअवकलअकरकपदजयदत\n",
            "    Reference letters:  कपकपनपजमतपमबअकस\n",
            "\n",
            "Processing: session_001_0017.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रजअकब)\n",
            "\n",
            "Processing: session_001_0018.txt\n",
            "  ✓ Match found (score: 0.25, para: 3)\n",
            "    Transcript letters: मपकरकसकल\n",
            "    Reference letters:  एकतरपकककनलककरपप\n",
            "\n",
            "Processing: session_001_0019.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रललउक)\n",
            "\n",
            "Processing: session_001_0020.txt\n",
            "  ✓ Match found (score: 0.33, para: 12)\n",
            "    Transcript letters: हमस\n",
            "    Reference letters:  बमवरएआरजमडदमतकस\n",
            "\n",
            "Processing: session_001_0021.txt\n",
            "  ✓ Match found (score: 0.14, para: 26)\n",
            "    Transcript letters: अपलसबपम\n",
            "    Reference letters:  हबहसहकतपवटपआपवल\n",
            "\n",
            "Processing: session_001_0022.txt\n",
            "  ✓ Match found (score: 0.20, para: 24)\n",
            "    Transcript letters: मकउकसकमकनल\n",
            "    Reference letters:  मबआमनआपकवतब2कतस\n",
            "\n",
            "Processing: session_001_0023.txt\n",
            "  ✓ Match found (score: 0.10, para: 20)\n",
            "    Transcript letters: अआकअरतदलकप\n",
            "    Reference letters:  कसकववसटपचमकवपतउ\n",
            "\n",
            "Processing: session_001_0024.txt\n",
            "  ✓ Match found (score: 0.20, para: 11)\n",
            "    Transcript letters: कअममप\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0025.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बमवरअअरसजमददमकस)\n",
            "\n",
            "Processing: session_001_0026.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: वपदहकल)\n",
            "\n",
            "Processing: session_001_0027.txt\n",
            "  ✓ Match found (score: 0.10, para: 25)\n",
            "    Transcript letters: समरसपसबबसकरज\n",
            "    Reference letters:  सदतआयउकअदलआसलरल\n",
            "\n",
            "Processing: session_001_0028.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: नकदतदससकबल)\n",
            "\n",
            "Processing: session_001_0029.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: अअवआककरक)\n",
            "\n",
            "Processing: session_001_0030.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तसहपककद)\n",
            "\n",
            "Processing: session_001_0031.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इजकसजहलमज)\n",
            "\n",
            "Processing: session_001_0032.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तअमअकरउदप)\n",
            "\n",
            "Processing: session_001_0033.txt\n",
            "  ✓ Match found (score: 0.12, para: 15)\n",
            "    Transcript letters: कररपपचउह\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0034.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तहपजमतमबल)\n",
            "\n",
            "Processing: session_001_0035.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तइपपआसमकत)\n",
            "\n",
            "Processing: session_001_0036.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रकजमपबतरकद)\n",
            "\n",
            "Processing: session_001_0037.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मतपककबककवकबव)\n",
            "\n",
            "Processing: session_001_0038.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बपचककमससतपननकमद)\n",
            "\n",
            "Processing: session_001_0039.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: आसजकनकमसमपकजपज)\n",
            "\n",
            "Processing: session_001_0040.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इकसननकप)\n",
            "\n",
            "Processing: session_001_0041.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बबवजकपतबकलल)\n",
            "\n",
            "Processing: session_001_0042.txt\n",
            "  ✓ Match found (score: 0.10, para: 18)\n",
            "    Transcript letters: कसकववसतमपजआकवपत\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0043.txt\n",
            "  ✓ Match found (score: 0.11, para: 22)\n",
            "    Transcript letters: आपबजमकमपम\n",
            "    Reference letters:  कएब\n",
            "\n",
            "Processing: session_001_0044.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: पमजरलकल)\n",
            "\n",
            "Processing: session_001_0045.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: तबकससहसकचअबहसकच)\n",
            "\n",
            "Processing: session_001_0046.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: ससएकचरम)\n",
            "\n",
            "Processing: session_001_0047.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: रआकहसइपनस)\n",
            "\n",
            "Processing: session_001_0048.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मससकबरतकनल)\n",
            "\n",
            "Processing: session_001_0049.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: cjavvapapek)\n",
            "\n",
            "Processing: session_001_0050.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: aiffअपडकलक)\n",
            "\n",
            "Processing: session_001_0051.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: यमआतदआपकआक)\n",
            "\n",
            "Processing: session_001_0052.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इकआपसaअaचअदवव)\n",
            "\n",
            "Processing: session_001_0053.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: कबइजल)\n",
            "\n",
            "Processing: session_001_0054.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: कबइजल)\n",
            "\n",
            "Processing: session_001_0055.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मसबसआमनआपकल)\n",
            "\n",
            "Processing: session_001_0056.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: वतपयअकतसबदचक)\n",
            "\n",
            "Processing: session_001_0057.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: वदककजव)\n",
            "\n",
            "Processing: session_001_0058.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मआसबकपपवसबवड)\n",
            "\n",
            "Processing: session_001_0059.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इअदमजबवसतकदअक)\n",
            "\n",
            "Processing: session_001_0060.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: मएतइज)\n",
            "\n",
            "Processing: session_001_0061.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: सददतआपउकअदलकरसल)\n",
            "\n",
            "Processing: session_001_0062.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: बहनदअदरअअक)\n",
            "\n",
            "Processing: session_001_0063.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: कअबकसललनअवल)\n",
            "\n",
            "Processing: session_001_0064.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: आबकहकरककल)\n",
            "\n",
            "Processing: session_001_0065.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: इनमदनस)\n",
            "\n",
            "Processing: session_001_0066.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: यकपवपपनपवटलम)\n",
            "\n",
            "Processing: session_001_0067.txt\n",
            "  ⚠️  No good match (score: 0.00, letters: पपतपपपतपपपतपपपत)\n",
            "\n",
            "============================================================\n",
            "PROCESSING SUMMARY\n",
            "============================================================\n",
            "Total files found: 68\n",
            "Successfully processed: 26\n",
            "Skipped (noise): 0\n",
            "No good match: 42\n",
            "Unique paragraphs used: 26/27\n",
            "\n",
            "Output saved to: /content/drive/MyDrive/dataset/final_text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiAZhHyMSzBs",
        "outputId": "4adab74d-cd02-4c25-fd98-47567ede77e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: odfpy in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from odfpy) (0.7.1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1D-kGrRqDD9HTTa3oQloR_btKTVnGZ8l2",
      "authorship_tag": "ABX9TyM7AAD5C3kPEzW6YnJzKFSq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}